{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归(练习)\n",
    "---\n",
    "这一部分将会利用逻辑回归对MNIST数据集中的图片数据是否为0进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "首先对数据进行预处理，读入图片，将2维的图片数据（28， 28）转化为向量（784, ）, 并且生成标签，0 对应标签为 1， 其余图片对应标签为 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg # mpimg 用于读取图片\n",
    "\n",
    "data_path = '../data'\n",
    "\n",
    "# 图片的路径，1中包括了数字1-9，0中全部为0\n",
    "path_1 = os.listdir(os.path.join(data_path, '1'))\n",
    "np.random.shuffle(path_1)\n",
    "path_1 = list(map(lambda x: os.path.join(data_path, '1', x), path_1))\n",
    "path_0 = os.listdir(os.path.join(data_path, '0'))\n",
    "path_0 = list(map(lambda x: os.path.join(data_path, '0', x), path_0))\n",
    "\n",
    "\n",
    "def parse_image(image_path):\n",
    "    \"\"\"对所给的图像进行处理，变成一维向量, 并且归一化\n",
    "    Args:\n",
    "        image_path: 图像的路径\n",
    "    Returns：\n",
    "        img: 处理好的图像\n",
    "    \"\"\"\n",
    "    t = mpimg.imread(image_path)\n",
    "    return np.reshape(t, (28 * 28)) / 255.\n",
    "\n",
    "\n",
    "def get_label(paths, labels):\n",
    "    \"\"\"根据给的路径对图像进行处理，打上标签\n",
    "    Args:\n",
    "        paths: 图片路径\n",
    "        labels: 图片标签\n",
    "    Returns:\n",
    "        x: 处理好的图片\n",
    "        y: 对应长度的标签\n",
    "    \"\"\"\n",
    "    x = list(map(parse_image, paths))\n",
    "#     print(x[:200])\n",
    "    if labels == 1:\n",
    "        y = [[1,0] for _ in range(len(paths))]\n",
    "    else:\n",
    "        y = [[0,1] for _ in range(len(paths))]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "data_0 = get_label(path_0, 1)\n",
    "print(data_0[1][:10])\n",
    "\n",
    "\n",
    "data_1 = get_label(path_1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：\n",
    "将数据集分割为两部分，一部分用于训练，另一部分用于验证模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01176471 0.         0.         0.01176471 0.02745098 0.01176471\n",
      " 0.         0.01176471 0.         0.04313725 0.         0.\n",
      " 0.01176471 0.         0.         0.01176471 0.03137255 0.\n",
      " 0.         0.01176471 0.         0.         0.         0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.01960784 0.         0.04705882 0.         0.0627451  0.\n",
      " 0.         0.01568627 0.         0.00784314 0.03137255 0.01176471\n",
      " 0.         0.01568627 0.03137255 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.00784314 0.\n",
      " 0.         0.         0.00392157 0.00784314 0.00392157 0.04705882\n",
      " 0.         0.03137255 0.         0.         0.02352941 0.\n",
      " 0.04313725 0.         0.         0.02352941 0.02745098 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00392157 0.01176471 0.         0.         0.00784314\n",
      " 0.01176471 0.         0.         0.         0.04705882 0.\n",
      " 0.         0.09019608 0.         0.         0.         0.\n",
      " 0.04313725 0.01176471 0.         0.         0.01568627 0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.00392157 0.         0.         0.00784314 0.         0.\n",
      " 0.02352941 0.         0.09803922 0.10588235 0.53333333 0.52941176\n",
      " 0.7372549  0.34901961 0.32941176 0.09803922 0.         0.\n",
      " 0.01176471 0.00392157 0.         0.         0.         0.\n",
      " 0.         0.         0.01568627 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01176471 0.34509804\n",
      " 0.96862745 0.9254902  1.         0.97647059 0.98039216 0.89019608\n",
      " 0.94117647 0.53333333 0.14509804 0.00392157 0.         0.00784314\n",
      " 0.00784314 0.         0.         0.         0.         0.\n",
      " 0.00784314 0.         0.         0.01176471 0.         0.\n",
      " 0.01568627 0.10588235 0.75686275 0.98431373 0.99215686 1.\n",
      " 1.         1.         1.         0.94117647 0.99607843 1.\n",
      " 0.83529412 0.34901961 0.         0.         0.05490196 0.00392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.02352941 0.         0.         0.07058824 0.21960784\n",
      " 0.96470588 1.         0.99215686 0.95294118 0.98431373 1.\n",
      " 0.96078431 1.         1.         0.99607843 1.         0.90588235\n",
      " 0.46666667 0.02745098 0.         0.01960784 0.         0.\n",
      " 0.         0.         0.01568627 0.         0.         0.04705882\n",
      " 0.05098039 0.         0.25490196 0.74509804 0.96470588 1.\n",
      " 1.         0.98431373 1.         0.42745098 0.34509804 0.78039216\n",
      " 1.         0.96862745 0.98039216 1.         0.91764706 0.36078431\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.03921569 0.00392157 0.         0.         0.07058824\n",
      " 0.63921569 0.97254902 1.         0.92156863 0.84705882 0.58823529\n",
      " 0.50196078 0.17647059 0.02352941 0.03137255 0.08627451 0.83137255\n",
      " 1.         1.         0.98823529 0.6745098  0.         0.05882353\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.01568627 0.01960784 0.         0.         0.73333333 1.\n",
      " 0.99607843 0.36862745 0.22352941 0.02745098 0.00392157 0.\n",
      " 0.02352941 0.         0.         0.54509804 0.94901961 1.\n",
      " 1.         0.85490196 0.24313725 0.         0.         0.\n",
      " 0.         0.         0.01960784 0.00784314 0.         0.\n",
      " 0.04313725 0.21960784 0.98823529 0.92156863 0.99215686 0.07843137\n",
      " 0.01960784 0.00784314 0.01960784 0.00392157 0.         0.00392157\n",
      " 0.00784314 0.         0.38039216 0.97647059 0.97254902 0.97647059\n",
      " 0.65098039 0.03137255 0.         0.         0.         0.\n",
      " 0.         0.         0.00784314 0.         0.         0.2745098\n",
      " 1.         1.         0.96078431 0.09803922 0.03921569 0.\n",
      " 0.         0.00392157 0.         0.01568627 0.03921569 0.\n",
      " 0.03921569 1.         0.96470588 0.98039216 0.60784314 0.\n",
      " 0.         0.         0.         0.         0.00784314 0.\n",
      " 0.02745098 0.04705882 0.         0.34117647 0.88627451 1.\n",
      " 0.72156863 0.         0.01176471 0.         0.03921569 0.01960784\n",
      " 0.         0.         0.         0.03529412 0.         0.71764706\n",
      " 0.98431373 1.         0.87058824 0.05882353 0.         0.\n",
      " 0.         0.         0.         0.01960784 0.00392157 0.\n",
      " 0.0745098  0.90196078 1.         0.95294118 1.         0.1372549\n",
      " 0.00784314 0.         0.         0.         0.         0.03529412\n",
      " 0.03137255 0.         0.         0.2745098  0.96078431 0.94901961\n",
      " 1.         0.05490196 0.         0.         0.         0.\n",
      " 0.         0.01568627 0.01176471 0.         0.0745098  0.98431373\n",
      " 0.9372549  1.         0.96862745 0.11764706 0.00392157 0.\n",
      " 0.01568627 0.01568627 0.05490196 0.         0.         0.00784314\n",
      " 0.         0.18431373 1.         1.         0.96862745 0.08235294\n",
      " 0.         0.         0.         0.         0.02352941 0.\n",
      " 0.00784314 0.00784314 0.         0.67843137 0.96862745 0.98823529\n",
      " 0.98039216 0.10980392 0.03921569 0.         0.         0.03137255\n",
      " 0.         0.         0.         0.03137255 0.         0.2627451\n",
      " 0.97647059 1.         1.         0.04705882 0.         0.\n",
      " 0.         0.         0.         0.         0.02352941 0.01176471\n",
      " 0.         0.34509804 1.         0.98431373 1.         0.7372549\n",
      " 0.08235294 0.         0.05882353 0.         0.03137255 0.00784314\n",
      " 0.0627451  0.         0.1372549  0.78431373 0.96862745 0.98431373\n",
      " 0.5254902  0.01568627 0.         0.         0.         0.\n",
      " 0.         0.01176471 0.01176471 0.00392157 0.         0.04313725\n",
      " 0.82745098 0.96862745 0.97647059 1.         0.74117647 0.29803922\n",
      " 0.         0.         0.01568627 0.         0.00784314 0.\n",
      " 0.6627451  1.         1.         0.96862745 0.18431373 0.\n",
      " 0.         0.         0.         0.         0.         0.02352941\n",
      " 0.         0.         0.00784314 0.         0.23137255 0.80392157\n",
      " 1.         0.94117647 1.         0.71372549 0.16078431 0.21960784\n",
      " 0.10980392 0.12941176 0.16470588 0.9372549  0.96470588 0.98431373\n",
      " 0.93333333 0.61568627 0.         0.00392157 0.         0.\n",
      " 0.         0.         0.00784314 0.00392157 0.         0.\n",
      " 0.00784314 0.03921569 0.         0.40784314 0.9372549  1.\n",
      " 0.94117647 1.         0.99215686 0.96862745 0.92941176 1.\n",
      " 1.         0.98039216 1.         0.9372549  1.         0.39215686\n",
      " 0.         0.00392157 0.         0.         0.         0.\n",
      " 0.00392157 0.         0.01176471 0.         0.         0.02745098\n",
      " 0.         0.01568627 0.44705882 1.         1.         1.\n",
      " 1.         0.96862745 0.97647059 0.99215686 0.98431373 0.99607843\n",
      " 0.92941176 0.98431373 0.34901961 0.         0.         0.00392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.03529412 0.         0.         0.00392157 0.05098039 0.\n",
      " 0.05490196 0.65490196 1.         0.96470588 0.99215686 1.\n",
      " 1.         0.99607843 0.94901961 1.         0.95686275 0.23921569\n",
      " 0.         0.0745098  0.         0.00392157 0.         0.\n",
      " 0.         0.         0.00784314 0.00392157 0.02745098 0.\n",
      " 0.         0.01568627 0.         0.05490196 0.         0.10588235\n",
      " 0.23921569 0.56078431 1.         1.         0.98823529 1.\n",
      " 0.58431373 0.08235294 0.02352941 0.0627451  0.         0.\n",
      " 0.02745098 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         1.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# 输入图片\n",
    "data_X_train = np.concatenate((data_0[0][:-500], data_1[0][:-500]))\n",
    "# 输入标签数据\n",
    "data_Y_train = np.concatenate((data_0[1][:-500], data_1[1][:-500]))\n",
    "training_set = np.concatenate([data_X_train, data_Y_train], axis=1)\n",
    "print(training_set[0])\n",
    "# 验证集\n",
    "data_X_test = np.concatenate((data_0[0][-500:], data_1[0][-500:]))\n",
    "data_Y_test = np.concatenate((data_0[1][-500:], data_1[1][-500:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：\n",
    "定义产生batch的生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(dataset, batchsize):\n",
    "    \"\"\"根据设定的batchsize大小产生mini batch\n",
    "    Args:\n",
    "        dataset: 数据集\n",
    "        batchsize: batchsize\n",
    "    Generates:\n",
    "        x: 输入\n",
    "        y：输出\n",
    "    \"\"\"\n",
    "    for i in range(np.shape(dataset)[0] // batchsize):\n",
    "        pos = i * batchsize\n",
    "        x = dataset[pos:pos + batchsize, 0:-2]\n",
    "        y = dataset[pos:pos + batchsize, -2:]\n",
    "        yield x, y\n",
    "    remain = np.shape(dataset)[0] % batchsize\n",
    "    if remain != 0:\n",
    "        x, y = dataset[-remain:, 0:-2], dataset[-remain:, -2:]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义计算图\n",
    "在这一步中，我们主要完成了以下几点：\n",
    "* 定义 placeholder 用于后面输入数据\n",
    "* 定义权重变量 W\n",
    "* 定义损失函数\n",
    "* 定义优化算法，这里使用的时梯度下降\n",
    "* 计算错误率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "lr = 0.01\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：\n",
    "定义计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义计算图\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # 定义placeholder\n",
    "    X = tf.placeholder(shape=(None, 28*28), dtype=tf.float32, name=\"X\")\n",
    "    Y = tf.placeholder(shape=(None, 2), dtype=tf.float32, name=\"Y\")\n",
    "\n",
    "    # 定义weight matrix\n",
    "    W = tf.Variable(tf.truncated_normal(shape=[784, 2]), name=\"WeightMatrix\")\n",
    "    lgt = tf.matmul(X, W)\n",
    "    output = tf.nn.softmax(lgt, name=\"Apply_Sigmoid\")\n",
    "    # 定义loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=lgt), name=\"calculate_loss\")\n",
    "    \n",
    "    with tf.name_scope(\"SGD\"):\n",
    "        # 使用梯度下降进行优化\n",
    "        opt = tf.train.GradientDescentOptimizer(lr).minimize(loss, var_list=[W])\n",
    "    \n",
    "    # 计算错误率\n",
    "    with tf.name_scope(\"calculate_error_rate\"):\n",
    "        # 概率大于 0.5 预测结果为0， 否则为 0\n",
    "        prediction_result = tf.cast(tf.equal(tf.argmax(output,axis=1),tf.argmax(Y,axis=1)), dtype=tf.float32)\n",
    "        \n",
    "\n",
    "        error_rate = 1 - tf.reduce_mean(prediction_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：\n",
    "### 定义进程并进行运算\n",
    "这一步将准备好的数据输入给运算图，对模型中的变量赋予了初值，进行计算和优化，总共对模型进行了500次训练，并且训练完成以后利用测试数据集对模型进行了评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    0, Loss: 4.4679, Error Rate: 57.8125%\n",
      "Step:   50, Loss: 0.2033, Error Rate: 6.2500%\n",
      "Step:  100, Loss: 0.9343, Error Rate: 21.8750%\n",
      "Step:  150, Loss: 0.4969, Error Rate: 18.7500%\n",
      "Step:  200, Loss: 1.2791, Error Rate: 18.7500%\n",
      "Step:  250, Loss: 0.9955, Error Rate: 28.1250%\n",
      "Step:  300, Loss: 1.9551, Error Rate: 35.9375%\n",
      "Step:  350, Loss: 0.2824, Error Rate: 6.2500%\n",
      "Step:  400, Loss: 0.4004, Error Rate: 9.3750%\n",
      "Step:  450, Loss: 0.3505, Error Rate: 7.8125%\n",
      "Step:  500, Loss: 0.4162, Error Rate: 9.3750%\n",
      "Step:  550, Loss: 0.9963, Error Rate: 14.0625%\n",
      "Step:  600, Loss: 0.9964, Error Rate: 20.3125%\n",
      "Step:  650, Loss: 0.2347, Error Rate: 4.6875%\n",
      "Step:  700, Loss: 0.3939, Error Rate: 7.8125%\n",
      "Step:  750, Loss: 0.1811, Error Rate: 6.2500%\n",
      "Step:  800, Loss: 0.6759, Error Rate: 14.0625%\n",
      "Step:  850, Loss: 0.3129, Error Rate: 7.8125%\n",
      "Step:  900, Loss: 0.8652, Error Rate: 21.8750%\n",
      "Step:  950, Loss: 0.3943, Error Rate: 9.3750%\n",
      "Step: 1000, Loss: 0.2777, Error Rate: 4.6875%\n",
      "Step: 1050, Loss: 0.1969, Error Rate: 6.2500%\n",
      "Step: 1100, Loss: 0.1642, Error Rate: 6.2500%\n",
      "Step: 1150, Loss: 0.3554, Error Rate: 12.5000%\n",
      "Step: 1200, Loss: 0.4514, Error Rate: 7.8125%\n",
      "Step: 1250, Loss: 0.6109, Error Rate: 9.3750%\n",
      "Step: 1300, Loss: 0.2919, Error Rate: 9.3750%\n",
      "Step: 1350, Loss: 0.2602, Error Rate: 6.2500%\n",
      "Step: 1400, Loss: 0.5198, Error Rate: 9.3750%\n",
      "Step: 1450, Loss: 0.4340, Error Rate: 15.6250%\n",
      "Step: 1500, Loss: 0.4159, Error Rate: 10.9375%\n",
      "Step: 1550, Loss: 0.6013, Error Rate: 14.0625%\n",
      "Step: 1600, Loss: 0.4247, Error Rate: 6.2500%\n",
      "Step: 1650, Loss: 0.2429, Error Rate: 7.8125%\n",
      "Step: 1700, Loss: 0.1546, Error Rate: 4.6875%\n",
      "Step: 1750, Loss: 0.0521, Error Rate: 4.6875%\n",
      "Step: 1800, Loss: 0.0477, Error Rate: 0.0000%\n",
      "Step: 1850, Loss: 0.1957, Error Rate: 4.6875%\n",
      "Step: 1900, Loss: 0.0802, Error Rate: 3.1250%\n",
      "Step: 1950, Loss: 0.2262, Error Rate: 1.5625%\n",
      "Step: 2000, Loss: 0.3422, Error Rate: 7.8125%\n",
      "Step: 2050, Loss: 0.2894, Error Rate: 7.8125%\n",
      "Step: 2100, Loss: 0.5026, Error Rate: 10.9375%\n",
      "Step: 2150, Loss: 0.4352, Error Rate: 12.5000%\n",
      "Step: 2200, Loss: 0.1373, Error Rate: 4.6875%\n",
      "Step: 2250, Loss: 0.0443, Error Rate: 1.5625%\n",
      "Step: 2300, Loss: 0.1721, Error Rate: 6.2500%\n",
      "Step: 2350, Loss: 0.3496, Error Rate: 6.2500%\n",
      "Step: 2400, Loss: 0.0338, Error Rate: 1.5625%\n",
      "Step: 2450, Loss: 0.2439, Error Rate: 4.6875%\n",
      "Step: 2500, Loss: 0.1762, Error Rate: 7.8125%\n",
      "Step: 2550, Loss: 0.0682, Error Rate: 4.6875%\n",
      "Step: 2600, Loss: 0.2218, Error Rate: 3.1250%\n",
      "Step: 2650, Loss: 0.0623, Error Rate: 1.5625%\n",
      "Step: 2700, Loss: 0.5845, Error Rate: 12.5000%\n",
      "Step: 2750, Loss: 0.0527, Error Rate: 1.5625%\n",
      "Step: 2800, Loss: 0.2132, Error Rate: 6.2500%\n",
      "Step: 2850, Loss: 0.0802, Error Rate: 6.2500%\n",
      "Step: 2900, Loss: 0.0297, Error Rate: 1.5625%\n",
      "Step: 2950, Loss: 0.1822, Error Rate: 4.6875%\n",
      "Step: 3000, Loss: 0.4007, Error Rate: 7.8125%\n",
      "Step: 3050, Loss: 0.2754, Error Rate: 4.6875%\n",
      "Step: 3100, Loss: 0.2119, Error Rate: 4.6875%\n",
      "Step: 3150, Loss: 0.1434, Error Rate: 1.5625%\n",
      "Step: 3200, Loss: 0.1261, Error Rate: 4.6875%\n",
      "Step: 3250, Loss: 0.1573, Error Rate: 3.1250%\n",
      "Step: 3300, Loss: 0.1153, Error Rate: 3.1250%\n",
      "Step: 3350, Loss: 0.1500, Error Rate: 4.6875%\n",
      "Step: 3400, Loss: 0.5405, Error Rate: 7.8125%\n",
      "Step: 3450, Loss: 0.2895, Error Rate: 9.0909%\n",
      "Step: 3500, Loss: 0.6409, Error Rate: 9.3750%\n",
      "Step: 3550, Loss: 0.0824, Error Rate: 4.6875%\n",
      "Step: 3600, Loss: 0.0405, Error Rate: 1.5625%\n",
      "Step: 3650, Loss: 0.1442, Error Rate: 4.6875%\n",
      "Step: 3700, Loss: 0.1380, Error Rate: 6.2500%\n",
      "Step: 3750, Loss: 0.3799, Error Rate: 7.8125%\n",
      "Step: 3800, Loss: 0.0895, Error Rate: 3.1250%\n",
      "Step: 3850, Loss: 0.0822, Error Rate: 7.8125%\n",
      "Step: 3900, Loss: 0.0917, Error Rate: 3.1250%\n",
      "Step: 3950, Loss: 0.1988, Error Rate: 3.1250%\n",
      "Step: 4000, Loss: 0.2381, Error Rate: 4.6875%\n",
      "Step: 4050, Loss: 0.3318, Error Rate: 6.2500%\n",
      "Step: 4100, Loss: 0.0616, Error Rate: 3.1250%\n",
      "Step: 4150, Loss: 0.0876, Error Rate: 1.5625%\n",
      "Step: 4200, Loss: 0.4225, Error Rate: 10.9375%\n",
      "Step: 4250, Loss: 0.0443, Error Rate: 3.1250%\n",
      "Step: 4300, Loss: 0.1536, Error Rate: 3.1250%\n",
      "Step: 4350, Loss: 0.0691, Error Rate: 1.5625%\n",
      "Step: 4400, Loss: 0.2588, Error Rate: 4.6875%\n",
      "Step: 4450, Loss: 0.0912, Error Rate: 3.1250%\n",
      "Step: 4500, Loss: 0.2537, Error Rate: 7.8125%\n",
      "Step: 4550, Loss: 0.0341, Error Rate: 1.5625%\n",
      "Step: 4600, Loss: 0.1541, Error Rate: 6.2500%\n",
      "Step: 4650, Loss: 0.1953, Error Rate: 3.1250%\n",
      "Step: 4700, Loss: 0.3106, Error Rate: 14.0625%\n",
      "Step: 4750, Loss: 0.0052, Error Rate: 0.0000%\n",
      "Step: 4800, Loss: 0.0081, Error Rate: 0.0000%\n",
      "Step: 4850, Loss: 0.2426, Error Rate: 6.2500%\n",
      "Step: 4900, Loss: 0.3318, Error Rate: 4.6875%\n",
      "Step: 4950, Loss: 0.1564, Error Rate: 4.6875%\n",
      "Step: 5000, Loss: 0.0760, Error Rate: 3.1250%\n",
      "Step: 5050, Loss: 0.1576, Error Rate: 4.6875%\n",
      "Step: 5100, Loss: 0.0382, Error Rate: 3.1250%\n",
      "Step: 5150, Loss: 0.0748, Error Rate: 4.6875%\n",
      "Step: 5200, Loss: 0.0213, Error Rate: 1.5625%\n",
      "Step: 5250, Loss: 0.1916, Error Rate: 6.2500%\n",
      "Step: 5300, Loss: 0.1203, Error Rate: 4.6875%\n",
      "Step: 5350, Loss: 0.2387, Error Rate: 4.6875%\n",
      "Step: 5400, Loss: 0.4127, Error Rate: 4.6875%\n",
      "Step: 5450, Loss: 0.1557, Error Rate: 6.2500%\n",
      "Step: 5500, Loss: 0.0248, Error Rate: 1.5625%\n",
      "Step: 5550, Loss: 0.1020, Error Rate: 4.6875%\n",
      "Step: 5600, Loss: 0.2093, Error Rate: 4.6875%\n",
      "Step: 5650, Loss: 0.0579, Error Rate: 3.1250%\n",
      "Step: 5700, Loss: 0.1658, Error Rate: 4.6875%\n",
      "Step: 5750, Loss: 0.1559, Error Rate: 1.5625%\n",
      "Step: 5800, Loss: 0.0127, Error Rate: 0.0000%\n",
      "Step: 5850, Loss: 0.0796, Error Rate: 4.6875%\n",
      "Step: 5900, Loss: 0.2121, Error Rate: 4.6875%\n",
      "Step: 5950, Loss: 0.0961, Error Rate: 3.1250%\n",
      "Step: 6000, Loss: 0.1779, Error Rate: 1.5625%\n",
      "Step: 6050, Loss: 0.0314, Error Rate: 1.5625%\n",
      "Step: 6100, Loss: 0.0644, Error Rate: 3.1250%\n",
      "Step: 6150, Loss: 0.1967, Error Rate: 4.6875%\n",
      "Step: 6200, Loss: 0.0235, Error Rate: 1.5625%\n",
      "Step: 6250, Loss: 0.1424, Error Rate: 3.1250%\n",
      "Step: 6300, Loss: 0.1219, Error Rate: 3.1250%\n",
      "Step: 6350, Loss: 0.1394, Error Rate: 1.5625%\n",
      "Step: 6400, Loss: 0.0661, Error Rate: 1.5625%\n",
      "Step: 6450, Loss: 0.1269, Error Rate: 3.1250%\n",
      "Step: 6500, Loss: 0.2770, Error Rate: 9.3750%\n",
      "Step: 6550, Loss: 0.1637, Error Rate: 3.1250%\n",
      "Step: 6600, Loss: 0.1008, Error Rate: 3.1250%\n",
      "Step: 6650, Loss: 0.1688, Error Rate: 4.6875%\n",
      "Step: 6700, Loss: 0.0445, Error Rate: 3.1250%\n",
      "Step: 6750, Loss: 0.2995, Error Rate: 3.1250%\n",
      "Step: 6800, Loss: 0.1276, Error Rate: 3.1250%\n",
      "Step: 6850, Loss: 0.0972, Error Rate: 3.1250%\n",
      "Step: 6900, Loss: 0.2596, Error Rate: 4.6875%\n",
      "Step: 6950, Loss: 0.0416, Error Rate: 1.5625%\n",
      "Step: 7000, Loss: 0.1318, Error Rate: 3.1250%\n",
      "Step: 7050, Loss: 0.1543, Error Rate: 3.1250%\n",
      "Step: 7100, Loss: 0.1430, Error Rate: 4.6875%\n",
      "Step: 7150, Loss: 0.1228, Error Rate: 4.6875%\n",
      "Step: 7200, Loss: 0.2526, Error Rate: 3.1250%\n",
      "Step: 7250, Loss: 0.1674, Error Rate: 3.1250%\n",
      "Step: 7300, Loss: 0.1425, Error Rate: 6.2500%\n",
      "Step: 7350, Loss: 0.4979, Error Rate: 6.2500%\n",
      "Step: 7400, Loss: 0.1856, Error Rate: 6.2500%\n",
      "Step: 7450, Loss: 0.1246, Error Rate: 4.6875%\n",
      "Step: 7500, Loss: 0.2425, Error Rate: 6.2500%\n",
      "Step: 7550, Loss: 0.2821, Error Rate: 4.6875%\n",
      "Step: 7600, Loss: 0.1411, Error Rate: 6.2500%\n",
      "Step: 7650, Loss: 0.1474, Error Rate: 1.5625%\n",
      "Step: 7700, Loss: 0.0107, Error Rate: 0.0000%\n",
      "Step: 7750, Loss: 0.0336, Error Rate: 1.5625%\n",
      "Step: 7800, Loss: 0.0369, Error Rate: 1.5625%\n",
      "Step: 7850, Loss: 0.0558, Error Rate: 1.5625%\n",
      "Step: 7900, Loss: 0.2419, Error Rate: 1.5625%\n",
      "Step: 7950, Loss: 0.1810, Error Rate: 6.2500%\n",
      "Step: 8000, Loss: 0.2676, Error Rate: 4.6875%\n",
      "Step: 8050, Loss: 0.1160, Error Rate: 7.8125%\n",
      "Step: 8100, Loss: 0.0954, Error Rate: 4.6875%\n",
      "Step: 8150, Loss: 0.0508, Error Rate: 1.5625%\n",
      "Step: 8200, Loss: 0.0575, Error Rate: 1.5625%\n",
      "Step: 8250, Loss: 0.2190, Error Rate: 4.6875%\n",
      "Step: 8300, Loss: 0.1897, Error Rate: 4.6875%\n",
      "Step: 8350, Loss: 0.0037, Error Rate: 0.0000%\n",
      "Step: 8400, Loss: 0.1679, Error Rate: 1.5625%\n",
      "Step: 8450, Loss: 0.0443, Error Rate: 1.5625%\n",
      "Step: 8500, Loss: 0.0168, Error Rate: 0.0000%\n",
      "Step: 8550, Loss: 0.1586, Error Rate: 1.5625%\n",
      "Step: 8600, Loss: 0.0450, Error Rate: 1.5625%\n",
      "Step: 8650, Loss: 0.3140, Error Rate: 9.3750%\n",
      "Step: 8700, Loss: 0.0122, Error Rate: 0.0000%\n",
      "Step: 8750, Loss: 0.1569, Error Rate: 3.1250%\n",
      "Step: 8800, Loss: 0.0347, Error Rate: 1.5625%\n",
      "Step: 8850, Loss: 0.0124, Error Rate: 0.0000%\n",
      "Step: 8900, Loss: 0.1185, Error Rate: 1.5625%\n",
      "Step: 8950, Loss: 0.2611, Error Rate: 4.6875%\n",
      "Step: 9000, Loss: 0.2389, Error Rate: 4.6875%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 9050, Loss: 0.1658, Error Rate: 3.1250%\n",
      "Step: 9100, Loss: 0.0907, Error Rate: 1.5625%\n",
      "Step: 9150, Loss: 0.0665, Error Rate: 3.1250%\n",
      "Step: 9200, Loss: 0.1702, Error Rate: 1.5625%\n",
      "Step: 9250, Loss: 0.0731, Error Rate: 3.1250%\n",
      "Step: 9300, Loss: 0.0520, Error Rate: 1.5625%\n",
      "Step: 9350, Loss: 0.4551, Error Rate: 6.2500%\n",
      "Step: 9400, Loss: 0.1428, Error Rate: 6.8182%\n",
      "Step: 9450, Loss: 0.5842, Error Rate: 9.3750%\n",
      "Step: 9500, Loss: 0.0361, Error Rate: 1.5625%\n",
      "Step: 9550, Loss: 0.0056, Error Rate: 0.0000%\n",
      "Step: 9600, Loss: 0.0649, Error Rate: 4.6875%\n",
      "Step: 9650, Loss: 0.0639, Error Rate: 6.2500%\n",
      "Step: 9700, Loss: 0.2775, Error Rate: 7.8125%\n",
      "Step: 9750, Loss: 0.0641, Error Rate: 1.5625%\n",
      "Step: 9800, Loss: 0.0408, Error Rate: 3.1250%\n",
      "Step: 9850, Loss: 0.0407, Error Rate: 3.1250%\n",
      "Step: 9900, Loss: 0.1611, Error Rate: 3.1250%\n",
      "Step: 9950, Loss: 0.2061, Error Rate: 3.1250%\n",
      "Step: 10000, Loss: 0.2646, Error Rate: 6.2500%\n",
      "Step: 10050, Loss: 0.0415, Error Rate: 1.5625%\n",
      "Step: 10100, Loss: 0.0381, Error Rate: 1.5625%\n",
      "Step: 10150, Loss: 0.2759, Error Rate: 7.8125%\n",
      "Step: 10200, Loss: 0.0236, Error Rate: 0.0000%\n",
      "Step: 10250, Loss: 0.1425, Error Rate: 4.6875%\n",
      "Step: 10300, Loss: 0.0526, Error Rate: 1.5625%\n",
      "Step: 10350, Loss: 0.2346, Error Rate: 3.1250%\n",
      "Step: 10400, Loss: 0.0767, Error Rate: 3.1250%\n",
      "Step: 10450, Loss: 0.2191, Error Rate: 6.2500%\n",
      "Step: 10500, Loss: 0.0302, Error Rate: 1.5625%\n",
      "Step: 10550, Loss: 0.1182, Error Rate: 3.1250%\n",
      "Step: 10600, Loss: 0.1349, Error Rate: 3.1250%\n",
      "Step: 10650, Loss: 0.1867, Error Rate: 6.2500%\n",
      "Step: 10700, Loss: 0.0011, Error Rate: 0.0000%\n",
      "Step: 10750, Loss: 0.0030, Error Rate: 0.0000%\n",
      "Step: 10800, Loss: 0.2320, Error Rate: 4.6875%\n",
      "Step: 10850, Loss: 0.3137, Error Rate: 4.6875%\n",
      "Step: 10900, Loss: 0.1280, Error Rate: 3.1250%\n",
      "Step: 10950, Loss: 0.0535, Error Rate: 1.5625%\n",
      "Step: 11000, Loss: 0.0746, Error Rate: 4.6875%\n",
      "Step: 11050, Loss: 0.0263, Error Rate: 1.5625%\n",
      "Step: 11100, Loss: 0.0280, Error Rate: 0.0000%\n",
      "Step: 11150, Loss: 0.0154, Error Rate: 0.0000%\n",
      "Step: 11200, Loss: 0.1134, Error Rate: 3.1250%\n",
      "Step: 11250, Loss: 0.1141, Error Rate: 3.1250%\n",
      "Step: 11300, Loss: 0.1966, Error Rate: 4.6875%\n",
      "Step: 11350, Loss: 0.4097, Error Rate: 4.6875%\n",
      "Step: 11400, Loss: 0.1181, Error Rate: 4.6875%\n",
      "Step: 11450, Loss: 0.0328, Error Rate: 1.5625%\n",
      "Step: 11500, Loss: 0.0591, Error Rate: 3.1250%\n",
      "Step: 11550, Loss: 0.1330, Error Rate: 4.6875%\n",
      "Step: 11600, Loss: 0.0386, Error Rate: 1.5625%\n",
      "Step: 11650, Loss: 0.1167, Error Rate: 3.1250%\n",
      "Step: 11700, Loss: 0.1564, Error Rate: 1.5625%\n",
      "Step: 11750, Loss: 0.0054, Error Rate: 0.0000%\n",
      "Step: 11800, Loss: 0.0541, Error Rate: 1.5625%\n",
      "Step: 11850, Loss: 0.1612, Error Rate: 6.2500%\n",
      "Training finished.\n",
      "Testing Loss: 0.1698, Testing Error Rate: 3.7000%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    # 初始化变量\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    step = 0\n",
    "    for epc in range(epoch):\n",
    "        for x, y in gen_batch(training_set, batchsize):\n",
    "            l, error, _ = sess.run([loss, error_rate, opt], feed_dict={X: np.reshape(x, (-1, 784)), Y: np.reshape(y, (-1, 2))})\n",
    "            if step % 50 == 0:\n",
    "                print(\"Step: {:>4}, Loss: {:.4f}, Error Rate: {:.4%}\".format(step, l, error))\n",
    "            step += 1\n",
    "    print(\"Training finished.\")\n",
    "    l, error, weight_matrix = sess.run([loss, error_rate, W],\n",
    "                                       {X: data_X_test, Y: data_Y_test})\n",
    "    print(\"Testing Loss: {:.4f}, Testing Error Rate: {:.4%}\".format(l, error))\n",
    "    W_value = sess.run(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
