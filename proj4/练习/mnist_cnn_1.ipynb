{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可以看到以上的两种结构达到的都是大约92%的正确率，这并不是一个非常好的结果，我们可以通过改进网络的结构达到99%的正确率。\n",
    "# 利用CNN搭建手写数字识别系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 载入MNIST数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuyanfang/anaconda3/envs/tensorflow1.4/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../proj2/第二天/练习/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../proj2/第二天/练习/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../proj2/第二天/练习/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../proj2/第二天/练习/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../../proj2/第二天/练习/MNIST_data',one_hot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## placeholder\n",
    "### 2 我们从为输入的图像和输出的分类创建节点开始创建计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此处x,y_ 都不是实际的值，当我们要求TensorFlow进行计算的时候我们会对对应的值进行输入。 此处x是一个浮点类型的2d tensor，我们定义时分配的shape 为[None, 784]，其中784是将28*28的MNIST图片变成一个向量，None表示第一维可以为任何大小，对应batch size。y_是每张图片对应的数字，也是一个2d tensor，其中第一维仍然可以任意大小，与batch size相等即可，每一行是一个one-hot 十维向量，每行只有一个值为1，图片中的数字的标签，其余9个位置都是0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 权重初始化 \n",
    "建立模型的过程中我们会需要定义很多权重和偏置，初始化权重的时候应当增添随机的噪声。由于选用ReLU作为非线性激活函数，一般选取一个正值作为偏置的初值防止'dead neurons'的问题。下面定义两个函数完成上述功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4卷积和池化\n",
    "TensorFlow提供了灵活的卷积和池化操作，在这个模型中我们将选择最简单的版本，卷积stride选择为1，用0填充，这样输出和输入的维度是相同的，池化操作是在2\\*2的范围内选取最大值(max pooling)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一个卷积层\n",
    "我们现在可以实现我们的第一层了，其中包括了卷积层和池化层，卷积层将对每一个5\\*5的patch计算32个feature，因此它的权重的维度为[5, 5, 1, 32]，其中前两维表示了patch 的大小，第三个数字表示输入的图片通道数，32表示输出的通道数。对每一个通道的输出我们还会有偏置部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数据输入模型之前，我们需要先对输入数据的维度进行更改，变为一个4维张量，第2，3，4个维度分别表示图像的宽，高和输入图像的颜色通道数，第一维的-1表示这一个维度是通过计算从而保证整个数据大小不发生改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当模型对输入图像进行卷积的时候，我们先使用权重与图像进行卷积，然后加上偏置向量，最后经过非线性激活函数ReLU，最后再经过池化，经过池化以后图像的大小将会变成14*14。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二层\n",
    "与第一层类似，输出为64个特征。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层\n",
    "经过两层卷积和池化以后，图像的大小已经变成了7\\*7，我们现在增加一个含有1024个神经元的全连接层来对模型进行处理。首先将图像从二维转化为一维，然后与权重相乘，加上偏置后通过激活函数。\n",
    "$h_{fc1} = ReLU(h W_{fc1} + b_{fc1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "Dropout可以避免模型过拟合，我们在模型的输出之前使用dropout，创建一个placeholder表明神经元保留的概率，这可以使我们在训练的时候使用dropout，在测试的时候不适用dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出层\n",
    "\n",
    "输出层为全连接层，与softmax回归模型类似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练和评估模型\n",
    "基本步骤与前面相同，主要的区别在于：\n",
    "* 使用ADAM优化算法替代梯度下降\n",
    "* 增加了keep_prob参数来控制dropout的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.06\n",
      "step 100, training accuracy 0.82\n",
      "step 200, training accuracy 0.86\n",
      "step 300, training accuracy 0.92\n",
      "step 400, training accuracy 0.92\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 0.9\n",
      "step 700, training accuracy 0.88\n",
      "step 800, training accuracy 0.92\n",
      "step 900, training accuracy 0.96\n",
      "step 1000, training accuracy 0.9\n",
      "step 1100, training accuracy 0.94\n",
      "step 1200, training accuracy 0.92\n",
      "step 1300, training accuracy 0.98\n",
      "step 1400, training accuracy 0.94\n",
      "step 1500, training accuracy 1\n",
      "step 1600, training accuracy 0.94\n",
      "step 1700, training accuracy 1\n",
      "step 1800, training accuracy 1\n",
      "step 1900, training accuracy 0.98\n",
      "test accuracy 0.9732\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i in range(2000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 100 == 0:\n",
    "      train_accuracy = accuracy.eval(feed_dict={\n",
    "          x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "      print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "  print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "      x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
