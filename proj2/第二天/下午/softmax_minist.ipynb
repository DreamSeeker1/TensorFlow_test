{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numerical stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuyanfang/anaconda3/envs/tensorflow1.4/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/liuyanfang/anaconda3/envs/tensorflow1.4/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1000) / np.exp(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999980838\n"
     ]
    }
   ],
   "source": [
    "# 等价于计算e5 * le-5\n",
    "test = 0\n",
    "for i in range(int(1e5)):\n",
    "\n",
    "    test += 1e-5\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'hello'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello = tf.constant('hello')\n",
    "sess.run(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'hello'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1., 2., 3., 4.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0320586 , 0.08714432, 0.23688282, 0.64391426])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(y) / np.sum(np.exp(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0320586  0.08714432 0.2368828  0.6439142 ]\n"
     ]
    }
   ],
   "source": [
    "softmax = tf.nn.softmax(y).eval()\n",
    "print(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(softmax, axis=0).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'softmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-afe62df96d4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# y_pred = 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# real label 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'softmax' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = tf.argmax(softmax, axis=0).eval()  # y_pred = 3\n",
    "real = [1., 0., 0., 0.] # real label 0\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=real, logits=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4401898"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4401898008764813"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(0.0320586)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用TensorFlow内建函数载入MNIST!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuyanfang/anaconda3/envs/tensorflow1.4/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1d1a1a424906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "mnist.train.labels[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.12156864, 0.5176471 ,\n",
       "       0.9960785 , 0.9921569 , 0.9960785 , 0.8352942 , 0.32156864,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.08235294,\n",
       "       0.5568628 , 0.91372555, 0.98823535, 0.9921569 , 0.98823535,\n",
       "       0.9921569 , 0.98823535, 0.8745099 , 0.07843138, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.48235297, 0.9960785 , 0.9921569 , 0.9960785 ,\n",
       "       0.9921569 , 0.87843144, 0.7960785 , 0.7960785 , 0.8745099 ,\n",
       "       1.        , 0.8352942 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.7960785 ,\n",
       "       0.9921569 , 0.98823535, 0.9921569 , 0.8313726 , 0.07843138,\n",
       "       0.        , 0.        , 0.2392157 , 0.9921569 , 0.98823535,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.16078432, 0.95294124, 0.87843144, 0.7960785 ,\n",
       "       0.7176471 , 0.16078432, 0.59607846, 0.11764707, 0.        ,\n",
       "       0.        , 1.        , 0.9921569 , 0.40000004, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.15686275, 0.07843138, 0.        , 0.        , 0.40000004,\n",
       "       0.9921569 , 0.19607845, 0.        , 0.32156864, 0.9921569 ,\n",
       "       0.98823535, 0.07843138, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32156864, 0.83921576, 0.12156864,\n",
       "       0.4431373 , 0.91372555, 0.9960785 , 0.91372555, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.24313727, 0.40000004, 0.32156864,\n",
       "       0.16078432, 0.9921569 , 0.909804  , 0.9921569 , 0.98823535,\n",
       "       0.91372555, 0.19607845, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.59607846, 0.9921569 , 0.9960785 , 0.9921569 , 0.9960785 ,\n",
       "       0.9921569 , 0.9960785 , 0.91372555, 0.48235297, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.59607846, 0.98823535,\n",
       "       0.9921569 , 0.98823535, 0.9921569 , 0.98823535, 0.75294125,\n",
       "       0.19607845, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.24313727, 0.7176471 , 0.7960785 , 0.95294124,\n",
       "       0.9960785 , 0.9921569 , 0.24313727, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.15686275, 0.6745098 , 0.98823535,\n",
       "       0.7960785 , 0.07843138, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.08235294, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.7176471 , 0.9960785 , 0.43921572,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.24313727,\n",
       "       0.7960785 , 0.6392157 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.2392157 , 0.9921569 , 0.5921569 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.08235294, 0.83921576, 0.75294125, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313726, 0.8352942 , 0.9960785 ,\n",
       "       0.5921569 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.40000004,\n",
       "       0.9921569 , 0.5921569 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.16078432,\n",
       "       0.8352942 , 0.98823535, 0.9921569 , 0.43529415, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.16078432, 1.        , 0.8352942 ,\n",
       "       0.36078432, 0.20000002, 0.        , 0.        , 0.12156864,\n",
       "       0.36078432, 0.6784314 , 0.9921569 , 0.9960785 , 0.9921569 ,\n",
       "       0.5568628 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.6745098 , 0.98823535, 0.9921569 , 0.98823535,\n",
       "       0.7960785 , 0.7960785 , 0.91372555, 0.98823535, 0.9921569 ,\n",
       "       0.98823535, 0.9921569 , 0.50980395, 0.07843138, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.08235294,\n",
       "       0.7960785 , 1.        , 0.9921569 , 0.9960785 , 0.9921569 ,\n",
       "       0.9960785 , 0.9921569 , 0.9568628 , 0.7960785 , 0.32156864,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.07843138, 0.5921569 ,\n",
       "       0.5921569 , 0.9921569 , 0.67058825, 0.5921569 , 0.5921569 ,\n",
       "       0.15686275, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.next_batch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax 建模作为练习（20分钟）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 1（2分钟）：\n",
    "\n",
    "线性变换\n",
    "$y = xW + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义训练步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 2 （5分钟）：\n",
    "计算正确率，变量命名为 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_result = tf.nn.softmax(y)\n",
    "pred = tf.argmax(softmax_result, axis=1)\n",
    "true = tf.argmax(y_, axis=1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, true), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.next_batch(128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 3  训练模型（5 分钟）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    0, Loss: 2.3026, Accuracy:  7.81%\n",
      "Step:  100, Loss: 0.5722, Accuracy: 87.50%\n",
      "Step:  200, Loss: 0.4351, Accuracy: 86.72%\n",
      "Step:  300, Loss: 0.2649, Accuracy: 91.41%\n",
      "Step:  400, Loss: 0.2804, Accuracy: 92.97%\n",
      "Step:  500, Loss: 0.2859, Accuracy: 92.97%\n",
      "Step:  600, Loss: 0.3123, Accuracy: 91.41%\n",
      "Step:  700, Loss: 0.3307, Accuracy: 89.84%\n",
      "Step:  800, Loss: 0.3506, Accuracy: 89.06%\n",
      "Step:  900, Loss: 0.2090, Accuracy: 93.75%\n",
      "Step: 1000, Loss: 0.2656, Accuracy: 92.97%\n",
      "Step: 1100, Loss: 0.1960, Accuracy: 92.19%\n",
      "Step: 1200, Loss: 0.3611, Accuracy: 92.19%\n",
      "Step: 1300, Loss: 0.1996, Accuracy: 94.53%\n",
      "Step: 1400, Loss: 0.4188, Accuracy: 88.28%\n",
      "Step: 1500, Loss: 0.4101, Accuracy: 90.62%\n",
      "Step: 1600, Loss: 0.2885, Accuracy: 92.19%\n",
      "Step: 1700, Loss: 0.3597, Accuracy: 88.28%\n",
      "Step: 1800, Loss: 0.2644, Accuracy: 92.19%\n",
      "Step: 1900, Loss: 0.3930, Accuracy: 89.84%\n",
      "Step: 2000, Loss: 0.2107, Accuracy: 93.75%\n",
      "Step: 2100, Loss: 0.3641, Accuracy: 90.62%\n",
      "Step: 2200, Loss: 0.2710, Accuracy: 91.41%\n",
      "Step: 2300, Loss: 0.2176, Accuracy: 95.31%\n",
      "Step: 2400, Loss: 0.2385, Accuracy: 93.75%\n",
      "Step: 2500, Loss: 0.1775, Accuracy: 94.53%\n",
      "Step: 2600, Loss: 0.2675, Accuracy: 91.41%\n",
      "Step: 2700, Loss: 0.0963, Accuracy: 97.66%\n",
      "Step: 2800, Loss: 0.2884, Accuracy: 89.06%\n",
      "Step: 2900, Loss: 0.4414, Accuracy: 90.62%\n",
      "Step: 3000, Loss: 0.1929, Accuracy: 95.31%\n",
      "Step: 3100, Loss: 0.4400, Accuracy: 89.84%\n",
      "Step: 3200, Loss: 0.2039, Accuracy: 92.19%\n",
      "Step: 3300, Loss: 0.2805, Accuracy: 89.84%\n",
      "Step: 3400, Loss: 0.1510, Accuracy: 96.88%\n",
      "Step: 3500, Loss: 0.1809, Accuracy: 93.75%\n",
      "Step: 3600, Loss: 0.2023, Accuracy: 94.53%\n",
      "Step: 3700, Loss: 0.3294, Accuracy: 92.19%\n",
      "Step: 3800, Loss: 0.2970, Accuracy: 90.62%\n",
      "Step: 3900, Loss: 0.3628, Accuracy: 89.84%\n",
      "Training Finished, Loss: 0.2721, Accuracy: 92.20%\n"
     ]
    }
   ],
   "source": [
    "for i in range(4000):\n",
    "    batch = mnist.train.next_batch(128)\n",
    "    _, loss_, acu = sess.run([train_step, loss, accuracy], {x: batch[0], y_: batch[1]})\n",
    "    if i % 100 == 0:\n",
    "        print(\"Step:{:>5}, Loss:{:>7.4f}, Accuracy:{:>7.2%}\".format(i, loss_, acu))\n",
    "loss_, acu = sess.run([loss, accuracy], {x: mnist.test.images, \n",
    "                                                        y_: mnist.test.labels})\n",
    "print(\"Training Finished, Loss:{:>7.4f}, Accuracy:{:>7.2%}\".format(loss_, acu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习：DNN 建模\n",
    "* 单隐层\n",
    "* 激活函数可以使用：\n",
    "    * tf.nn.relu\n",
    "    * tf.nn.sigmoid\n",
    "    * tf.nn.tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../MNIST_data/',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,shape=[None,784])\n",
    "y_=tf.placeholder(tf.float32,shape=[None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=tf.Variable(tf.truncated_normal([784,512],mean=0,stddev=0.1))\n",
    "b=tf.Variable(tf.zeros([512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_model=tf.matmul(x,w)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_relu=tf.nn.relu(line_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_=tf.Variable(tf.truncated_normal([512,10],mean=0,stddev=0.1))\n",
    "b_=tf.Variable(tf.zeros([10]))\n",
    "y=tf.matmul(line_relu,w_)+b_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "softmax_y=tf.nn.softmax(y)\n",
    "pre=tf.argmax(softmax_y,axis=1)\n",
    "true=tf.argmax(y_,axis=1)\n",
    "acc=tf.reduce_mean(tf.cast(tf.equal(pre,true),tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    1, Loss: 2.5811, Accuracy: 17.19%\n",
      "Step:    2, Loss: 2.5852, Accuracy: 10.94%\n",
      "Step:    3, Loss: 2.5577, Accuracy: 14.06%\n",
      "Step:    4, Loss: 2.5246, Accuracy: 11.72%\n",
      "Step:    5, Loss: 2.6376, Accuracy: 12.50%\n",
      "Step:    6, Loss: 2.5260, Accuracy: 11.72%\n",
      "Step:    7, Loss: 2.5320, Accuracy: 10.94%\n",
      "Step:    8, Loss: 2.5368, Accuracy: 15.62%\n",
      "Step:    9, Loss: 2.4926, Accuracy: 17.97%\n",
      "Step:   10, Loss: 2.5432, Accuracy: 12.50%\n",
      "Step:   11, Loss: 2.4136, Accuracy: 12.50%\n",
      "Step:   12, Loss: 2.4337, Accuracy: 14.84%\n",
      "Step:   13, Loss: 2.4765, Accuracy: 12.50%\n",
      "Step:   14, Loss: 2.3049, Accuracy: 21.88%\n",
      "Step:   15, Loss: 2.4675, Accuracy: 15.62%\n",
      "Step:   16, Loss: 2.5824, Accuracy: 11.72%\n",
      "Step:   17, Loss: 2.5132, Accuracy:  9.38%\n",
      "Step:   18, Loss: 2.3980, Accuracy: 14.06%\n",
      "Step:   19, Loss: 2.5379, Accuracy: 13.28%\n",
      "Step:   20, Loss: 2.3038, Accuracy: 14.06%\n",
      "Step:   21, Loss: 2.5312, Accuracy: 10.94%\n",
      "Step:   22, Loss: 2.3740, Accuracy: 13.28%\n",
      "Step:   23, Loss: 2.3682, Accuracy: 14.84%\n",
      "Step:   24, Loss: 2.3366, Accuracy: 16.41%\n",
      "Step:   25, Loss: 2.3844, Accuracy: 14.06%\n",
      "Step:   26, Loss: 2.3252, Accuracy: 17.19%\n",
      "Step:   27, Loss: 2.2932, Accuracy: 21.88%\n",
      "Step:   28, Loss: 2.4475, Accuracy: 17.97%\n",
      "Step:   29, Loss: 2.2810, Accuracy: 14.06%\n",
      "Step:   30, Loss: 2.2636, Accuracy: 17.97%\n",
      "Step:   31, Loss: 2.3904, Accuracy: 19.53%\n",
      "Step:   32, Loss: 2.2604, Accuracy: 23.44%\n",
      "Step:   33, Loss: 2.3084, Accuracy: 16.41%\n",
      "Step:   34, Loss: 2.3151, Accuracy: 20.31%\n",
      "Step:   35, Loss: 2.1639, Accuracy: 21.88%\n",
      "Step:   36, Loss: 2.3518, Accuracy: 14.06%\n",
      "Step:   37, Loss: 2.2887, Accuracy: 12.50%\n",
      "Step:   38, Loss: 2.3249, Accuracy: 10.16%\n",
      "Step:   39, Loss: 2.3221, Accuracy: 16.41%\n",
      "Step:   40, Loss: 2.3571, Accuracy: 14.84%\n",
      "Step:   41, Loss: 2.4215, Accuracy: 15.62%\n",
      "Step:   42, Loss: 2.3228, Accuracy: 16.41%\n",
      "Step:   43, Loss: 2.3419, Accuracy: 21.09%\n",
      "Step:   44, Loss: 2.2647, Accuracy: 18.75%\n",
      "Step:   45, Loss: 2.3568, Accuracy: 15.62%\n",
      "Step:   46, Loss: 2.1150, Accuracy: 28.12%\n",
      "Step:   47, Loss: 2.2443, Accuracy: 15.62%\n",
      "Step:   48, Loss: 2.2582, Accuracy: 17.97%\n",
      "Step:   49, Loss: 2.2827, Accuracy: 20.31%\n",
      "Step:   50, Loss: 2.2320, Accuracy: 23.44%\n",
      "Step:   51, Loss: 2.3476, Accuracy: 12.50%\n",
      "Step:   52, Loss: 2.1753, Accuracy: 17.97%\n",
      "Step:   53, Loss: 2.2838, Accuracy: 16.41%\n",
      "Step:   54, Loss: 2.1945, Accuracy: 25.00%\n",
      "Step:   55, Loss: 2.2846, Accuracy: 16.41%\n",
      "Step:   56, Loss: 2.1803, Accuracy: 17.19%\n",
      "Step:   57, Loss: 2.1959, Accuracy: 19.53%\n",
      "Step:   58, Loss: 2.1806, Accuracy: 17.97%\n",
      "Step:   59, Loss: 2.2490, Accuracy: 17.97%\n",
      "Step:   60, Loss: 2.3048, Accuracy: 14.84%\n",
      "Step:   61, Loss: 2.2262, Accuracy: 21.09%\n",
      "Step:   62, Loss: 2.1760, Accuracy: 15.62%\n",
      "Step:   63, Loss: 2.1601, Accuracy: 21.88%\n",
      "Step:   64, Loss: 2.2207, Accuracy: 21.88%\n",
      "Step:   65, Loss: 2.1996, Accuracy: 19.53%\n",
      "Step:   66, Loss: 2.1381, Accuracy: 18.75%\n",
      "Step:   67, Loss: 2.1855, Accuracy: 21.88%\n",
      "Step:   68, Loss: 2.2580, Accuracy: 19.53%\n",
      "Step:   69, Loss: 2.1378, Accuracy: 27.34%\n",
      "Step:   70, Loss: 2.0989, Accuracy: 25.78%\n",
      "Step:   71, Loss: 2.0962, Accuracy: 25.00%\n",
      "Step:   72, Loss: 2.2198, Accuracy: 17.97%\n",
      "Step:   73, Loss: 2.1343, Accuracy: 27.34%\n",
      "Step:   74, Loss: 2.0906, Accuracy: 22.66%\n",
      "Step:   75, Loss: 2.2679, Accuracy: 17.97%\n",
      "Step:   76, Loss: 2.1291, Accuracy: 25.00%\n",
      "Step:   77, Loss: 2.2027, Accuracy: 22.66%\n",
      "Step:   78, Loss: 2.1680, Accuracy: 23.44%\n",
      "Step:   79, Loss: 2.0765, Accuracy: 28.91%\n",
      "Step:   80, Loss: 2.1841, Accuracy: 21.88%\n",
      "Step:   81, Loss: 2.1324, Accuracy: 25.00%\n",
      "Step:   82, Loss: 2.1952, Accuracy: 24.22%\n",
      "Step:   83, Loss: 2.2039, Accuracy: 20.31%\n",
      "Step:   84, Loss: 2.1481, Accuracy: 24.22%\n",
      "Step:   85, Loss: 2.0619, Accuracy: 26.56%\n",
      "Step:   86, Loss: 2.2404, Accuracy: 19.53%\n",
      "Step:   87, Loss: 2.0740, Accuracy: 22.66%\n",
      "Step:   88, Loss: 2.1176, Accuracy: 22.66%\n",
      "Step:   89, Loss: 2.1251, Accuracy: 26.56%\n",
      "Step:   90, Loss: 2.0820, Accuracy: 21.09%\n",
      "Step:   91, Loss: 2.1416, Accuracy: 17.19%\n",
      "Step:   92, Loss: 2.0981, Accuracy: 25.00%\n",
      "Step:   93, Loss: 2.1402, Accuracy: 25.78%\n",
      "Step:   94, Loss: 2.0673, Accuracy: 34.38%\n",
      "Step:   95, Loss: 2.0443, Accuracy: 26.56%\n",
      "Step:   96, Loss: 2.0908, Accuracy: 25.78%\n",
      "Step:   97, Loss: 2.0989, Accuracy: 25.00%\n",
      "Step:   98, Loss: 2.0498, Accuracy: 25.78%\n",
      "Step:   99, Loss: 2.0043, Accuracy: 30.47%\n",
      "Step:  101, Loss: 1.9853, Accuracy: 27.34%\n",
      "Step:  102, Loss: 2.0558, Accuracy: 25.00%\n",
      "Step:  103, Loss: 2.0548, Accuracy: 27.34%\n",
      "Step:  104, Loss: 2.0346, Accuracy: 30.47%\n",
      "Step:  105, Loss: 2.0953, Accuracy: 28.12%\n",
      "Step:  106, Loss: 2.1490, Accuracy: 22.66%\n",
      "Step:  107, Loss: 2.0411, Accuracy: 23.44%\n",
      "Step:  108, Loss: 2.0107, Accuracy: 30.47%\n",
      "Step:  109, Loss: 2.0754, Accuracy: 29.69%\n",
      "Step:  110, Loss: 2.0020, Accuracy: 27.34%\n",
      "Step:  111, Loss: 2.0661, Accuracy: 30.47%\n",
      "Step:  112, Loss: 2.0530, Accuracy: 27.34%\n",
      "Step:  113, Loss: 2.0476, Accuracy: 25.78%\n",
      "Step:  114, Loss: 2.0551, Accuracy: 25.78%\n",
      "Step:  115, Loss: 2.0438, Accuracy: 26.56%\n",
      "Step:  116, Loss: 1.9971, Accuracy: 34.38%\n",
      "Step:  117, Loss: 2.0346, Accuracy: 26.56%\n",
      "Step:  118, Loss: 1.8828, Accuracy: 38.28%\n",
      "Step:  119, Loss: 2.0892, Accuracy: 22.66%\n",
      "Step:  120, Loss: 2.0447, Accuracy: 21.09%\n",
      "Step:  121, Loss: 2.0305, Accuracy: 26.56%\n",
      "Step:  122, Loss: 2.0662, Accuracy: 21.88%\n",
      "Step:  123, Loss: 2.0681, Accuracy: 28.12%\n",
      "Step:  124, Loss: 1.8784, Accuracy: 39.06%\n",
      "Step:  125, Loss: 2.0742, Accuracy: 27.34%\n",
      "Step:  126, Loss: 2.0190, Accuracy: 28.12%\n",
      "Step:  127, Loss: 1.9634, Accuracy: 33.59%\n",
      "Step:  128, Loss: 2.0388, Accuracy: 29.69%\n",
      "Step:  129, Loss: 1.9688, Accuracy: 32.03%\n",
      "Step:  130, Loss: 1.9776, Accuracy: 35.16%\n",
      "Step:  131, Loss: 1.9766, Accuracy: 32.81%\n",
      "Step:  132, Loss: 1.9415, Accuracy: 33.59%\n",
      "Step:  133, Loss: 1.9429, Accuracy: 35.94%\n",
      "Step:  134, Loss: 2.0150, Accuracy: 30.47%\n",
      "Step:  135, Loss: 1.9886, Accuracy: 32.03%\n",
      "Step:  136, Loss: 1.9427, Accuracy: 32.81%\n",
      "Step:  137, Loss: 1.9627, Accuracy: 26.56%\n",
      "Step:  138, Loss: 2.0408, Accuracy: 28.12%\n",
      "Step:  139, Loss: 1.9901, Accuracy: 31.25%\n",
      "Step:  140, Loss: 1.9141, Accuracy: 35.16%\n",
      "Step:  141, Loss: 1.9408, Accuracy: 25.00%\n",
      "Step:  142, Loss: 1.9866, Accuracy: 32.81%\n",
      "Step:  143, Loss: 1.9431, Accuracy: 33.59%\n",
      "Step:  144, Loss: 1.9697, Accuracy: 25.78%\n",
      "Step:  145, Loss: 1.8923, Accuracy: 35.16%\n",
      "Step:  146, Loss: 2.0317, Accuracy: 35.94%\n",
      "Step:  147, Loss: 1.8490, Accuracy: 39.84%\n",
      "Step:  148, Loss: 1.9039, Accuracy: 41.41%\n",
      "Step:  149, Loss: 1.9409, Accuracy: 36.72%\n",
      "Step:  150, Loss: 1.9830, Accuracy: 28.91%\n",
      "Step:  151, Loss: 1.9357, Accuracy: 36.72%\n",
      "Step:  152, Loss: 2.0137, Accuracy: 33.59%\n",
      "Step:  153, Loss: 1.9916, Accuracy: 27.34%\n",
      "Step:  154, Loss: 1.9370, Accuracy: 29.69%\n",
      "Step:  155, Loss: 1.9837, Accuracy: 28.91%\n",
      "Step:  156, Loss: 1.8691, Accuracy: 38.28%\n",
      "Step:  157, Loss: 1.8377, Accuracy: 33.59%\n",
      "Step:  158, Loss: 1.9464, Accuracy: 32.81%\n",
      "Step:  159, Loss: 1.8390, Accuracy: 37.50%\n",
      "Step:  160, Loss: 1.9640, Accuracy: 29.69%\n",
      "Step:  161, Loss: 2.0163, Accuracy: 26.56%\n",
      "Step:  162, Loss: 1.8979, Accuracy: 39.84%\n",
      "Step:  163, Loss: 1.8375, Accuracy: 42.19%\n",
      "Step:  164, Loss: 2.0468, Accuracy: 30.47%\n",
      "Step:  165, Loss: 1.9618, Accuracy: 28.12%\n",
      "Step:  166, Loss: 1.8997, Accuracy: 39.06%\n",
      "Step:  167, Loss: 2.0212, Accuracy: 29.69%\n",
      "Step:  168, Loss: 1.8777, Accuracy: 36.72%\n",
      "Step:  169, Loss: 1.9151, Accuracy: 32.03%\n",
      "Step:  170, Loss: 1.9215, Accuracy: 33.59%\n",
      "Step:  171, Loss: 1.8935, Accuracy: 37.50%\n",
      "Step:  172, Loss: 2.0625, Accuracy: 25.00%\n",
      "Step:  173, Loss: 1.8482, Accuracy: 38.28%\n",
      "Step:  174, Loss: 1.9034, Accuracy: 31.25%\n",
      "Step:  175, Loss: 1.9565, Accuracy: 30.47%\n",
      "Step:  176, Loss: 1.9523, Accuracy: 32.03%\n",
      "Step:  177, Loss: 1.9666, Accuracy: 29.69%\n",
      "Step:  178, Loss: 1.9072, Accuracy: 39.06%\n",
      "Step:  179, Loss: 1.8727, Accuracy: 39.84%\n",
      "Step:  180, Loss: 1.9797, Accuracy: 32.03%\n",
      "Step:  181, Loss: 1.9012, Accuracy: 35.16%\n",
      "Step:  182, Loss: 1.7651, Accuracy: 37.50%\n",
      "Step:  183, Loss: 1.7989, Accuracy: 41.41%\n",
      "Step:  184, Loss: 1.8764, Accuracy: 38.28%\n",
      "Step:  185, Loss: 1.8744, Accuracy: 35.94%\n",
      "Step:  186, Loss: 1.8844, Accuracy: 32.81%\n",
      "Step:  187, Loss: 1.8113, Accuracy: 37.50%\n",
      "Step:  188, Loss: 1.8171, Accuracy: 39.84%\n",
      "Step:  189, Loss: 1.7892, Accuracy: 39.84%\n",
      "Step:  190, Loss: 1.9397, Accuracy: 33.59%\n",
      "Step:  191, Loss: 1.8033, Accuracy: 39.84%\n",
      "Step:  192, Loss: 1.7649, Accuracy: 47.66%\n",
      "Step:  193, Loss: 1.8587, Accuracy: 39.06%\n",
      "Step:  194, Loss: 1.8563, Accuracy: 39.84%\n",
      "Step:  195, Loss: 1.7373, Accuracy: 48.44%\n",
      "Step:  196, Loss: 1.7516, Accuracy: 41.41%\n",
      "Step:  197, Loss: 1.8211, Accuracy: 39.84%\n",
      "Step:  198, Loss: 1.8577, Accuracy: 35.16%\n",
      "Step:  199, Loss: 1.8902, Accuracy: 35.16%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  201, Loss: 1.8721, Accuracy: 36.72%\n",
      "Step:  202, Loss: 1.8533, Accuracy: 43.75%\n",
      "Step:  203, Loss: 1.8351, Accuracy: 39.06%\n",
      "Step:  204, Loss: 1.8678, Accuracy: 36.72%\n",
      "Step:  205, Loss: 1.7888, Accuracy: 45.31%\n",
      "Step:  206, Loss: 1.8117, Accuracy: 39.84%\n",
      "Step:  207, Loss: 1.7467, Accuracy: 40.62%\n",
      "Step:  208, Loss: 1.8182, Accuracy: 44.53%\n",
      "Step:  209, Loss: 1.8109, Accuracy: 43.75%\n",
      "Step:  210, Loss: 1.7329, Accuracy: 46.88%\n",
      "Step:  211, Loss: 1.8656, Accuracy: 35.16%\n",
      "Step:  212, Loss: 1.7924, Accuracy: 42.19%\n",
      "Step:  213, Loss: 1.7581, Accuracy: 41.41%\n",
      "Step:  214, Loss: 1.8582, Accuracy: 42.97%\n",
      "Step:  215, Loss: 1.8917, Accuracy: 35.94%\n",
      "Step:  216, Loss: 1.9615, Accuracy: 32.03%\n",
      "Step:  217, Loss: 1.8659, Accuracy: 39.84%\n",
      "Step:  218, Loss: 1.7526, Accuracy: 45.31%\n",
      "Step:  219, Loss: 1.8698, Accuracy: 39.06%\n",
      "Step:  220, Loss: 1.6711, Accuracy: 49.22%\n",
      "Step:  221, Loss: 1.8197, Accuracy: 44.53%\n",
      "Step:  222, Loss: 1.7443, Accuracy: 42.19%\n",
      "Step:  223, Loss: 1.8194, Accuracy: 36.72%\n",
      "Step:  224, Loss: 1.7934, Accuracy: 39.84%\n",
      "Step:  225, Loss: 1.8633, Accuracy: 38.28%\n",
      "Step:  226, Loss: 1.7742, Accuracy: 44.53%\n",
      "Step:  227, Loss: 1.8220, Accuracy: 36.72%\n",
      "Step:  228, Loss: 1.8345, Accuracy: 34.38%\n",
      "Step:  229, Loss: 1.7928, Accuracy: 42.97%\n",
      "Step:  230, Loss: 1.7877, Accuracy: 41.41%\n",
      "Step:  231, Loss: 1.8149, Accuracy: 37.50%\n",
      "Step:  232, Loss: 1.8181, Accuracy: 42.97%\n",
      "Step:  233, Loss: 1.7346, Accuracy: 46.09%\n",
      "Step:  234, Loss: 1.8049, Accuracy: 43.75%\n",
      "Step:  235, Loss: 1.7426, Accuracy: 42.97%\n",
      "Step:  236, Loss: 1.7862, Accuracy: 38.28%\n",
      "Step:  237, Loss: 1.6760, Accuracy: 46.09%\n",
      "Step:  238, Loss: 1.8143, Accuracy: 41.41%\n",
      "Step:  239, Loss: 1.7580, Accuracy: 38.28%\n",
      "Step:  240, Loss: 1.7241, Accuracy: 42.97%\n",
      "Step:  241, Loss: 1.6940, Accuracy: 45.31%\n",
      "Step:  242, Loss: 1.8272, Accuracy: 36.72%\n",
      "Step:  243, Loss: 1.7692, Accuracy: 46.09%\n",
      "Step:  244, Loss: 1.7420, Accuracy: 45.31%\n",
      "Step:  245, Loss: 1.8361, Accuracy: 40.62%\n",
      "Step:  246, Loss: 1.6780, Accuracy: 49.22%\n",
      "Step:  247, Loss: 1.7220, Accuracy: 44.53%\n",
      "Step:  248, Loss: 1.7044, Accuracy: 48.44%\n",
      "Step:  249, Loss: 1.6933, Accuracy: 47.66%\n",
      "Step:  250, Loss: 1.7851, Accuracy: 42.97%\n",
      "Step:  251, Loss: 1.6175, Accuracy: 53.91%\n",
      "Step:  252, Loss: 1.8244, Accuracy: 37.50%\n",
      "Step:  253, Loss: 1.8227, Accuracy: 37.50%\n",
      "Step:  254, Loss: 1.8249, Accuracy: 41.41%\n",
      "Step:  255, Loss: 1.7708, Accuracy: 42.19%\n",
      "Step:  256, Loss: 1.7118, Accuracy: 45.31%\n",
      "Step:  257, Loss: 1.7465, Accuracy: 45.31%\n",
      "Step:  258, Loss: 1.8180, Accuracy: 35.94%\n",
      "Step:  259, Loss: 1.7279, Accuracy: 46.88%\n",
      "Step:  260, Loss: 1.7636, Accuracy: 44.53%\n",
      "Step:  261, Loss: 1.7633, Accuracy: 35.94%\n",
      "Step:  262, Loss: 1.6866, Accuracy: 46.88%\n",
      "Step:  263, Loss: 1.7098, Accuracy: 41.41%\n",
      "Step:  264, Loss: 1.6615, Accuracy: 45.31%\n",
      "Step:  265, Loss: 1.6653, Accuracy: 50.00%\n",
      "Step:  266, Loss: 1.7726, Accuracy: 43.75%\n",
      "Step:  267, Loss: 1.7094, Accuracy: 42.97%\n",
      "Step:  268, Loss: 1.7098, Accuracy: 50.00%\n",
      "Step:  269, Loss: 1.6303, Accuracy: 54.69%\n",
      "Step:  270, Loss: 1.6151, Accuracy: 51.56%\n",
      "Step:  271, Loss: 1.6358, Accuracy: 51.56%\n",
      "Step:  272, Loss: 1.7579, Accuracy: 44.53%\n",
      "Step:  273, Loss: 1.7378, Accuracy: 40.62%\n",
      "Step:  274, Loss: 1.7471, Accuracy: 42.19%\n",
      "Step:  275, Loss: 1.8295, Accuracy: 41.41%\n",
      "Step:  276, Loss: 1.6511, Accuracy: 46.88%\n",
      "Step:  277, Loss: 1.7273, Accuracy: 47.66%\n",
      "Step:  278, Loss: 1.7622, Accuracy: 40.62%\n",
      "Step:  279, Loss: 1.6294, Accuracy: 50.78%\n",
      "Step:  280, Loss: 1.7109, Accuracy: 42.97%\n",
      "Step:  281, Loss: 1.7101, Accuracy: 52.34%\n",
      "Step:  282, Loss: 1.6587, Accuracy: 48.44%\n",
      "Step:  283, Loss: 1.6614, Accuracy: 53.12%\n",
      "Step:  284, Loss: 1.6430, Accuracy: 45.31%\n",
      "Step:  285, Loss: 1.7792, Accuracy: 46.88%\n",
      "Step:  286, Loss: 1.7259, Accuracy: 43.75%\n",
      "Step:  287, Loss: 1.6822, Accuracy: 46.09%\n",
      "Step:  288, Loss: 1.5993, Accuracy: 50.78%\n",
      "Step:  289, Loss: 1.5370, Accuracy: 56.25%\n",
      "Step:  290, Loss: 1.7151, Accuracy: 50.78%\n",
      "Step:  291, Loss: 1.6601, Accuracy: 52.34%\n",
      "Step:  292, Loss: 1.7904, Accuracy: 39.06%\n",
      "Step:  293, Loss: 1.6809, Accuracy: 50.00%\n",
      "Step:  294, Loss: 1.7306, Accuracy: 50.78%\n",
      "Step:  295, Loss: 1.6192, Accuracy: 49.22%\n",
      "Step:  296, Loss: 1.6897, Accuracy: 46.09%\n",
      "Step:  297, Loss: 1.6225, Accuracy: 49.22%\n",
      "Step:  298, Loss: 1.7423, Accuracy: 46.88%\n",
      "Step:  299, Loss: 1.6783, Accuracy: 51.56%\n",
      "Step:  301, Loss: 1.5885, Accuracy: 50.78%\n",
      "Step:  302, Loss: 1.6928, Accuracy: 46.88%\n",
      "Step:  303, Loss: 1.8231, Accuracy: 38.28%\n",
      "Step:  304, Loss: 1.7498, Accuracy: 42.97%\n",
      "Step:  305, Loss: 1.6551, Accuracy: 51.56%\n",
      "Step:  306, Loss: 1.6105, Accuracy: 48.44%\n",
      "Step:  307, Loss: 1.5808, Accuracy: 45.31%\n",
      "Step:  308, Loss: 1.5551, Accuracy: 56.25%\n",
      "Step:  309, Loss: 1.6895, Accuracy: 51.56%\n",
      "Step:  310, Loss: 1.5399, Accuracy: 54.69%\n",
      "Step:  311, Loss: 1.5927, Accuracy: 52.34%\n",
      "Step:  312, Loss: 1.5863, Accuracy: 48.44%\n",
      "Step:  313, Loss: 1.6836, Accuracy: 50.00%\n",
      "Step:  314, Loss: 1.6722, Accuracy: 48.44%\n",
      "Step:  315, Loss: 1.6523, Accuracy: 47.66%\n",
      "Step:  316, Loss: 1.6334, Accuracy: 50.78%\n",
      "Step:  317, Loss: 1.5834, Accuracy: 49.22%\n",
      "Step:  318, Loss: 1.5230, Accuracy: 56.25%\n",
      "Step:  319, Loss: 1.5903, Accuracy: 50.00%\n",
      "Step:  320, Loss: 1.7051, Accuracy: 50.78%\n",
      "Step:  321, Loss: 1.5564, Accuracy: 55.47%\n",
      "Step:  322, Loss: 1.5545, Accuracy: 53.91%\n",
      "Step:  323, Loss: 1.6299, Accuracy: 49.22%\n",
      "Step:  324, Loss: 1.7048, Accuracy: 47.66%\n",
      "Step:  325, Loss: 1.7119, Accuracy: 46.88%\n",
      "Step:  326, Loss: 1.6287, Accuracy: 48.44%\n",
      "Step:  327, Loss: 1.5582, Accuracy: 58.59%\n",
      "Step:  328, Loss: 1.6862, Accuracy: 49.22%\n",
      "Step:  329, Loss: 1.5240, Accuracy: 54.69%\n",
      "Step:  330, Loss: 1.7052, Accuracy: 48.44%\n",
      "Step:  331, Loss: 1.4946, Accuracy: 59.38%\n",
      "Step:  332, Loss: 1.6643, Accuracy: 50.78%\n",
      "Step:  333, Loss: 1.5678, Accuracy: 53.91%\n",
      "Step:  334, Loss: 1.6160, Accuracy: 51.56%\n",
      "Step:  335, Loss: 1.5565, Accuracy: 53.91%\n",
      "Step:  336, Loss: 1.5954, Accuracy: 54.69%\n",
      "Step:  337, Loss: 1.6246, Accuracy: 50.00%\n",
      "Step:  338, Loss: 1.5873, Accuracy: 50.78%\n",
      "Step:  339, Loss: 1.7155, Accuracy: 44.53%\n",
      "Step:  340, Loss: 1.6803, Accuracy: 42.19%\n",
      "Step:  341, Loss: 1.5348, Accuracy: 57.81%\n",
      "Step:  342, Loss: 1.6457, Accuracy: 45.31%\n",
      "Step:  343, Loss: 1.5979, Accuracy: 48.44%\n",
      "Step:  344, Loss: 1.5560, Accuracy: 55.47%\n",
      "Step:  345, Loss: 1.7277, Accuracy: 45.31%\n",
      "Step:  346, Loss: 1.5512, Accuracy: 56.25%\n",
      "Step:  347, Loss: 1.6207, Accuracy: 53.12%\n",
      "Step:  348, Loss: 1.6119, Accuracy: 50.00%\n",
      "Step:  349, Loss: 1.5468, Accuracy: 55.47%\n",
      "Step:  350, Loss: 1.5751, Accuracy: 50.78%\n",
      "Step:  351, Loss: 1.5658, Accuracy: 54.69%\n",
      "Step:  352, Loss: 1.5610, Accuracy: 53.12%\n",
      "Step:  353, Loss: 1.5871, Accuracy: 51.56%\n",
      "Step:  354, Loss: 1.5972, Accuracy: 51.56%\n",
      "Step:  355, Loss: 1.5499, Accuracy: 53.91%\n",
      "Step:  356, Loss: 1.5888, Accuracy: 55.47%\n",
      "Step:  357, Loss: 1.4205, Accuracy: 61.72%\n",
      "Step:  358, Loss: 1.6439, Accuracy: 47.66%\n",
      "Step:  359, Loss: 1.6875, Accuracy: 43.75%\n",
      "Step:  360, Loss: 1.5527, Accuracy: 57.81%\n",
      "Step:  361, Loss: 1.5853, Accuracy: 54.69%\n",
      "Step:  362, Loss: 1.6007, Accuracy: 45.31%\n",
      "Step:  363, Loss: 1.5717, Accuracy: 51.56%\n",
      "Step:  364, Loss: 1.4979, Accuracy: 56.25%\n",
      "Step:  365, Loss: 1.6113, Accuracy: 48.44%\n",
      "Step:  366, Loss: 1.5281, Accuracy: 54.69%\n",
      "Step:  367, Loss: 1.6452, Accuracy: 49.22%\n",
      "Step:  368, Loss: 1.5039, Accuracy: 61.72%\n",
      "Step:  369, Loss: 1.5441, Accuracy: 57.03%\n",
      "Step:  370, Loss: 1.5705, Accuracy: 53.12%\n",
      "Step:  371, Loss: 1.5216, Accuracy: 53.91%\n",
      "Step:  372, Loss: 1.5425, Accuracy: 48.44%\n",
      "Step:  373, Loss: 1.5557, Accuracy: 50.78%\n",
      "Step:  374, Loss: 1.5644, Accuracy: 53.12%\n",
      "Step:  375, Loss: 1.5385, Accuracy: 50.78%\n",
      "Step:  376, Loss: 1.5324, Accuracy: 56.25%\n",
      "Step:  377, Loss: 1.6356, Accuracy: 50.00%\n",
      "Step:  378, Loss: 1.5407, Accuracy: 53.91%\n",
      "Step:  379, Loss: 1.4796, Accuracy: 55.47%\n",
      "Step:  380, Loss: 1.5846, Accuracy: 57.03%\n",
      "Step:  381, Loss: 1.5018, Accuracy: 58.59%\n",
      "Step:  382, Loss: 1.4852, Accuracy: 54.69%\n",
      "Step:  383, Loss: 1.4842, Accuracy: 61.72%\n",
      "Step:  384, Loss: 1.5373, Accuracy: 51.56%\n",
      "Step:  385, Loss: 1.4612, Accuracy: 59.38%\n",
      "Step:  386, Loss: 1.5354, Accuracy: 51.56%\n",
      "Step:  387, Loss: 1.5321, Accuracy: 53.91%\n",
      "Step:  388, Loss: 1.4139, Accuracy: 62.50%\n",
      "Step:  389, Loss: 1.4466, Accuracy: 54.69%\n",
      "Step:  390, Loss: 1.5146, Accuracy: 55.47%\n",
      "Step:  391, Loss: 1.6277, Accuracy: 49.22%\n",
      "Step:  392, Loss: 1.6254, Accuracy: 50.00%\n",
      "Step:  393, Loss: 1.5452, Accuracy: 52.34%\n",
      "Step:  394, Loss: 1.7158, Accuracy: 44.53%\n",
      "Step:  395, Loss: 1.6687, Accuracy: 46.09%\n",
      "Step:  396, Loss: 1.4104, Accuracy: 61.72%\n",
      "Step:  397, Loss: 1.4984, Accuracy: 57.03%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  398, Loss: 1.4132, Accuracy: 58.59%\n",
      "Step:  399, Loss: 1.5849, Accuracy: 51.56%\n",
      "Step:  401, Loss: 1.3772, Accuracy: 69.53%\n",
      "Step:  402, Loss: 1.5949, Accuracy: 53.12%\n",
      "Step:  403, Loss: 1.5020, Accuracy: 58.59%\n",
      "Step:  404, Loss: 1.5104, Accuracy: 53.91%\n",
      "Step:  405, Loss: 1.4509, Accuracy: 55.47%\n",
      "Step:  406, Loss: 1.4461, Accuracy: 56.25%\n",
      "Step:  407, Loss: 1.5038, Accuracy: 57.03%\n",
      "Step:  408, Loss: 1.5216, Accuracy: 50.78%\n",
      "Step:  409, Loss: 1.5263, Accuracy: 57.03%\n",
      "Step:  410, Loss: 1.5585, Accuracy: 49.22%\n",
      "Step:  411, Loss: 1.4514, Accuracy: 60.16%\n",
      "Step:  412, Loss: 1.5024, Accuracy: 54.69%\n",
      "Step:  413, Loss: 1.6931, Accuracy: 44.53%\n",
      "Step:  414, Loss: 1.5152, Accuracy: 53.12%\n",
      "Step:  415, Loss: 1.6230, Accuracy: 51.56%\n",
      "Step:  416, Loss: 1.3895, Accuracy: 59.38%\n",
      "Step:  417, Loss: 1.4106, Accuracy: 62.50%\n",
      "Step:  418, Loss: 1.4974, Accuracy: 54.69%\n",
      "Step:  419, Loss: 1.5454, Accuracy: 52.34%\n",
      "Step:  420, Loss: 1.5145, Accuracy: 55.47%\n",
      "Step:  421, Loss: 1.4490, Accuracy: 60.94%\n",
      "Step:  422, Loss: 1.5216, Accuracy: 56.25%\n",
      "Step:  423, Loss: 1.5174, Accuracy: 58.59%\n",
      "Step:  424, Loss: 1.4868, Accuracy: 60.16%\n",
      "Step:  425, Loss: 1.5378, Accuracy: 57.03%\n",
      "Step:  426, Loss: 1.5151, Accuracy: 54.69%\n",
      "Step:  427, Loss: 1.3906, Accuracy: 59.38%\n",
      "Step:  428, Loss: 1.4767, Accuracy: 53.91%\n",
      "Step:  429, Loss: 1.5445, Accuracy: 53.12%\n",
      "Step:  430, Loss: 1.4844, Accuracy: 54.69%\n",
      "Step:  431, Loss: 1.4310, Accuracy: 64.06%\n",
      "Step:  432, Loss: 1.5264, Accuracy: 51.56%\n",
      "Step:  433, Loss: 1.4078, Accuracy: 63.28%\n",
      "Step:  434, Loss: 1.5451, Accuracy: 56.25%\n",
      "Step:  435, Loss: 1.3403, Accuracy: 64.84%\n",
      "Step:  436, Loss: 1.4275, Accuracy: 60.94%\n",
      "Step:  437, Loss: 1.4767, Accuracy: 61.72%\n",
      "Step:  438, Loss: 1.4704, Accuracy: 60.94%\n",
      "Step:  439, Loss: 1.4157, Accuracy: 56.25%\n",
      "Step:  440, Loss: 1.4976, Accuracy: 60.94%\n",
      "Step:  441, Loss: 1.4210, Accuracy: 58.59%\n",
      "Step:  442, Loss: 1.3970, Accuracy: 60.16%\n",
      "Step:  443, Loss: 1.5241, Accuracy: 52.34%\n",
      "Step:  444, Loss: 1.4725, Accuracy: 59.38%\n",
      "Step:  445, Loss: 1.4026, Accuracy: 57.81%\n",
      "Step:  446, Loss: 1.4212, Accuracy: 60.94%\n",
      "Step:  447, Loss: 1.4039, Accuracy: 60.94%\n",
      "Step:  448, Loss: 1.4650, Accuracy: 60.16%\n",
      "Step:  449, Loss: 1.5068, Accuracy: 57.81%\n",
      "Step:  450, Loss: 1.4554, Accuracy: 59.38%\n",
      "Step:  451, Loss: 1.4391, Accuracy: 53.91%\n",
      "Step:  452, Loss: 1.4446, Accuracy: 60.16%\n",
      "Step:  453, Loss: 1.4215, Accuracy: 60.94%\n",
      "Step:  454, Loss: 1.3799, Accuracy: 60.94%\n",
      "Step:  455, Loss: 1.4220, Accuracy: 58.59%\n",
      "Step:  456, Loss: 1.4463, Accuracy: 56.25%\n",
      "Step:  457, Loss: 1.4980, Accuracy: 58.59%\n",
      "Step:  458, Loss: 1.5478, Accuracy: 57.81%\n",
      "Step:  459, Loss: 1.4541, Accuracy: 54.69%\n",
      "Step:  460, Loss: 1.4194, Accuracy: 61.72%\n",
      "Step:  461, Loss: 1.4081, Accuracy: 64.06%\n",
      "Step:  462, Loss: 1.5359, Accuracy: 48.44%\n",
      "Step:  463, Loss: 1.3856, Accuracy: 56.25%\n",
      "Step:  464, Loss: 1.3682, Accuracy: 63.28%\n",
      "Step:  465, Loss: 1.4300, Accuracy: 64.06%\n",
      "Step:  466, Loss: 1.3951, Accuracy: 63.28%\n",
      "Step:  467, Loss: 1.5003, Accuracy: 54.69%\n",
      "Step:  468, Loss: 1.4617, Accuracy: 61.72%\n",
      "Step:  469, Loss: 1.4102, Accuracy: 60.94%\n",
      "Step:  470, Loss: 1.4099, Accuracy: 57.81%\n",
      "Step:  471, Loss: 1.4186, Accuracy: 55.47%\n",
      "Step:  472, Loss: 1.4940, Accuracy: 55.47%\n",
      "Step:  473, Loss: 1.4214, Accuracy: 60.16%\n",
      "Step:  474, Loss: 1.3780, Accuracy: 64.06%\n",
      "Step:  475, Loss: 1.4072, Accuracy: 63.28%\n",
      "Step:  476, Loss: 1.3694, Accuracy: 64.84%\n",
      "Step:  477, Loss: 1.4935, Accuracy: 55.47%\n",
      "Step:  478, Loss: 1.4784, Accuracy: 60.94%\n",
      "Step:  479, Loss: 1.3735, Accuracy: 63.28%\n",
      "Step:  480, Loss: 1.3779, Accuracy: 60.94%\n",
      "Step:  481, Loss: 1.3805, Accuracy: 60.16%\n",
      "Step:  482, Loss: 1.4670, Accuracy: 55.47%\n",
      "Step:  483, Loss: 1.3664, Accuracy: 62.50%\n",
      "Step:  484, Loss: 1.4010, Accuracy: 59.38%\n",
      "Step:  485, Loss: 1.3396, Accuracy: 67.19%\n",
      "Step:  486, Loss: 1.3659, Accuracy: 64.06%\n",
      "Step:  487, Loss: 1.3303, Accuracy: 68.75%\n",
      "Step:  488, Loss: 1.4547, Accuracy: 57.81%\n",
      "Step:  489, Loss: 1.3556, Accuracy: 67.19%\n",
      "Step:  490, Loss: 1.3877, Accuracy: 62.50%\n",
      "Step:  491, Loss: 1.3556, Accuracy: 63.28%\n",
      "Step:  492, Loss: 1.3980, Accuracy: 60.16%\n",
      "Step:  493, Loss: 1.3993, Accuracy: 57.03%\n",
      "Step:  494, Loss: 1.3906, Accuracy: 62.50%\n",
      "Step:  495, Loss: 1.4121, Accuracy: 61.72%\n",
      "Step:  496, Loss: 1.4177, Accuracy: 59.38%\n",
      "Step:  497, Loss: 1.4581, Accuracy: 57.81%\n",
      "Step:  498, Loss: 1.3285, Accuracy: 65.62%\n",
      "Step:  499, Loss: 1.4030, Accuracy: 60.16%\n",
      "Step:  501, Loss: 1.3925, Accuracy: 57.81%\n",
      "Step:  502, Loss: 1.3855, Accuracy: 60.94%\n",
      "Step:  503, Loss: 1.3677, Accuracy: 60.94%\n",
      "Step:  504, Loss: 1.3848, Accuracy: 62.50%\n",
      "Step:  505, Loss: 1.2981, Accuracy: 62.50%\n",
      "Step:  506, Loss: 1.4489, Accuracy: 58.59%\n",
      "Step:  507, Loss: 1.3079, Accuracy: 66.41%\n",
      "Step:  508, Loss: 1.5321, Accuracy: 53.12%\n",
      "Step:  509, Loss: 1.3934, Accuracy: 56.25%\n",
      "Step:  510, Loss: 1.3746, Accuracy: 63.28%\n",
      "Step:  511, Loss: 1.4947, Accuracy: 57.03%\n",
      "Step:  512, Loss: 1.3502, Accuracy: 64.06%\n",
      "Step:  513, Loss: 1.4573, Accuracy: 59.38%\n",
      "Step:  514, Loss: 1.3214, Accuracy: 63.28%\n",
      "Step:  515, Loss: 1.4210, Accuracy: 60.94%\n",
      "Step:  516, Loss: 1.3181, Accuracy: 64.84%\n",
      "Step:  517, Loss: 1.5078, Accuracy: 57.81%\n",
      "Step:  518, Loss: 1.4537, Accuracy: 60.94%\n",
      "Step:  519, Loss: 1.3990, Accuracy: 55.47%\n",
      "Step:  520, Loss: 1.3629, Accuracy: 58.59%\n",
      "Step:  521, Loss: 1.3280, Accuracy: 59.38%\n",
      "Step:  522, Loss: 1.4841, Accuracy: 58.59%\n",
      "Step:  523, Loss: 1.3110, Accuracy: 61.72%\n",
      "Step:  524, Loss: 1.3937, Accuracy: 60.16%\n",
      "Step:  525, Loss: 1.3689, Accuracy: 61.72%\n",
      "Step:  526, Loss: 1.3781, Accuracy: 57.81%\n",
      "Step:  527, Loss: 1.3604, Accuracy: 59.38%\n",
      "Step:  528, Loss: 1.2818, Accuracy: 61.72%\n",
      "Step:  529, Loss: 1.5209, Accuracy: 56.25%\n",
      "Step:  530, Loss: 1.3202, Accuracy: 60.94%\n",
      "Step:  531, Loss: 1.3869, Accuracy: 59.38%\n",
      "Step:  532, Loss: 1.3028, Accuracy: 62.50%\n",
      "Step:  533, Loss: 1.3210, Accuracy: 60.16%\n",
      "Step:  534, Loss: 1.4420, Accuracy: 57.81%\n",
      "Step:  535, Loss: 1.4022, Accuracy: 59.38%\n",
      "Step:  536, Loss: 1.5040, Accuracy: 60.16%\n",
      "Step:  537, Loss: 1.3481, Accuracy: 65.62%\n",
      "Step:  538, Loss: 1.3297, Accuracy: 63.28%\n",
      "Step:  539, Loss: 1.4127, Accuracy: 57.03%\n",
      "Step:  540, Loss: 1.4106, Accuracy: 57.81%\n",
      "Step:  541, Loss: 1.4251, Accuracy: 59.38%\n",
      "Step:  542, Loss: 1.4277, Accuracy: 53.12%\n",
      "Step:  543, Loss: 1.3685, Accuracy: 61.72%\n",
      "Step:  544, Loss: 1.2848, Accuracy: 66.41%\n",
      "Step:  545, Loss: 1.2379, Accuracy: 64.84%\n",
      "Step:  546, Loss: 1.3320, Accuracy: 59.38%\n",
      "Step:  547, Loss: 1.3091, Accuracy: 60.16%\n",
      "Step:  548, Loss: 1.3121, Accuracy: 67.19%\n",
      "Step:  549, Loss: 1.4137, Accuracy: 58.59%\n",
      "Step:  550, Loss: 1.3564, Accuracy: 63.28%\n",
      "Step:  551, Loss: 1.3368, Accuracy: 63.28%\n",
      "Step:  552, Loss: 1.2952, Accuracy: 60.94%\n",
      "Step:  553, Loss: 1.2640, Accuracy: 67.19%\n",
      "Step:  554, Loss: 1.4289, Accuracy: 59.38%\n",
      "Step:  555, Loss: 1.3067, Accuracy: 64.84%\n",
      "Step:  556, Loss: 1.3121, Accuracy: 62.50%\n",
      "Step:  557, Loss: 1.3345, Accuracy: 62.50%\n",
      "Step:  558, Loss: 1.4707, Accuracy: 58.59%\n",
      "Step:  559, Loss: 1.3494, Accuracy: 61.72%\n",
      "Step:  560, Loss: 1.3839, Accuracy: 61.72%\n",
      "Step:  561, Loss: 1.3385, Accuracy: 61.72%\n",
      "Step:  562, Loss: 1.2550, Accuracy: 71.09%\n",
      "Step:  563, Loss: 1.3699, Accuracy: 62.50%\n",
      "Step:  564, Loss: 1.3196, Accuracy: 64.06%\n",
      "Step:  565, Loss: 1.2612, Accuracy: 65.62%\n",
      "Step:  566, Loss: 1.2489, Accuracy: 69.53%\n",
      "Step:  567, Loss: 1.2634, Accuracy: 67.97%\n",
      "Step:  568, Loss: 1.2486, Accuracy: 67.97%\n",
      "Step:  569, Loss: 1.3433, Accuracy: 60.94%\n",
      "Step:  570, Loss: 1.3197, Accuracy: 66.41%\n",
      "Step:  571, Loss: 1.3982, Accuracy: 59.38%\n",
      "Step:  572, Loss: 1.3890, Accuracy: 62.50%\n",
      "Step:  573, Loss: 1.2640, Accuracy: 67.19%\n",
      "Step:  574, Loss: 1.3383, Accuracy: 63.28%\n",
      "Step:  575, Loss: 1.3542, Accuracy: 64.06%\n",
      "Step:  576, Loss: 1.3483, Accuracy: 65.62%\n",
      "Step:  577, Loss: 1.3604, Accuracy: 60.94%\n",
      "Step:  578, Loss: 1.4167, Accuracy: 61.72%\n",
      "Step:  579, Loss: 1.3002, Accuracy: 68.75%\n",
      "Step:  580, Loss: 1.4321, Accuracy: 58.59%\n",
      "Step:  581, Loss: 1.3197, Accuracy: 64.06%\n",
      "Step:  582, Loss: 1.2900, Accuracy: 61.72%\n",
      "Step:  583, Loss: 1.2918, Accuracy: 65.62%\n",
      "Step:  584, Loss: 1.2948, Accuracy: 68.75%\n",
      "Step:  585, Loss: 1.2289, Accuracy: 67.97%\n",
      "Step:  586, Loss: 1.3051, Accuracy: 60.94%\n",
      "Step:  587, Loss: 1.3137, Accuracy: 63.28%\n",
      "Step:  588, Loss: 1.3760, Accuracy: 61.72%\n",
      "Step:  589, Loss: 1.3543, Accuracy: 62.50%\n",
      "Step:  590, Loss: 1.2948, Accuracy: 63.28%\n",
      "Step:  591, Loss: 1.3462, Accuracy: 63.28%\n",
      "Step:  592, Loss: 1.3841, Accuracy: 58.59%\n",
      "Step:  593, Loss: 1.3668, Accuracy: 61.72%\n",
      "Step:  594, Loss: 1.2826, Accuracy: 67.19%\n",
      "Step:  595, Loss: 1.2534, Accuracy: 67.19%\n",
      "Step:  596, Loss: 1.2532, Accuracy: 65.62%\n",
      "Step:  597, Loss: 1.2049, Accuracy: 72.66%\n",
      "Step:  598, Loss: 1.3433, Accuracy: 60.16%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  599, Loss: 1.2551, Accuracy: 71.88%\n",
      "Step:  601, Loss: 1.4379, Accuracy: 54.69%\n",
      "Step:  602, Loss: 1.3084, Accuracy: 61.72%\n",
      "Step:  603, Loss: 1.3367, Accuracy: 63.28%\n",
      "Step:  604, Loss: 1.3594, Accuracy: 64.06%\n",
      "Step:  605, Loss: 1.2706, Accuracy: 66.41%\n",
      "Step:  606, Loss: 1.2464, Accuracy: 67.97%\n",
      "Step:  607, Loss: 1.1981, Accuracy: 64.84%\n",
      "Step:  608, Loss: 1.2469, Accuracy: 67.97%\n",
      "Step:  609, Loss: 1.2827, Accuracy: 64.84%\n",
      "Step:  610, Loss: 1.4307, Accuracy: 57.81%\n",
      "Step:  611, Loss: 1.2674, Accuracy: 64.06%\n",
      "Step:  612, Loss: 1.1961, Accuracy: 67.19%\n",
      "Step:  613, Loss: 1.2901, Accuracy: 66.41%\n",
      "Step:  614, Loss: 1.2847, Accuracy: 64.06%\n",
      "Step:  615, Loss: 1.2434, Accuracy: 66.41%\n",
      "Step:  616, Loss: 1.3235, Accuracy: 64.06%\n",
      "Step:  617, Loss: 1.2667, Accuracy: 71.09%\n",
      "Step:  618, Loss: 1.2871, Accuracy: 67.97%\n",
      "Step:  619, Loss: 1.1903, Accuracy: 69.53%\n",
      "Step:  620, Loss: 1.1676, Accuracy: 69.53%\n",
      "Step:  621, Loss: 1.2947, Accuracy: 64.06%\n",
      "Step:  622, Loss: 1.2540, Accuracy: 60.94%\n",
      "Step:  623, Loss: 1.2846, Accuracy: 60.16%\n",
      "Step:  624, Loss: 1.1837, Accuracy: 67.97%\n",
      "Step:  625, Loss: 1.2655, Accuracy: 65.62%\n",
      "Step:  626, Loss: 1.3065, Accuracy: 60.94%\n",
      "Step:  627, Loss: 1.2368, Accuracy: 66.41%\n",
      "Step:  628, Loss: 1.3507, Accuracy: 60.16%\n",
      "Step:  629, Loss: 1.2422, Accuracy: 64.06%\n",
      "Step:  630, Loss: 1.3245, Accuracy: 62.50%\n",
      "Step:  631, Loss: 1.3369, Accuracy: 65.62%\n",
      "Step:  632, Loss: 1.2430, Accuracy: 60.16%\n",
      "Step:  633, Loss: 1.1707, Accuracy: 70.31%\n",
      "Step:  634, Loss: 1.2387, Accuracy: 69.53%\n",
      "Step:  635, Loss: 1.2711, Accuracy: 60.16%\n",
      "Step:  636, Loss: 1.3135, Accuracy: 64.84%\n",
      "Step:  637, Loss: 1.2742, Accuracy: 67.19%\n",
      "Step:  638, Loss: 1.1737, Accuracy: 66.41%\n",
      "Step:  639, Loss: 1.2253, Accuracy: 66.41%\n",
      "Step:  640, Loss: 1.2798, Accuracy: 62.50%\n",
      "Step:  641, Loss: 1.2711, Accuracy: 67.97%\n",
      "Step:  642, Loss: 1.2410, Accuracy: 66.41%\n",
      "Step:  643, Loss: 1.2735, Accuracy: 67.97%\n",
      "Step:  644, Loss: 1.2823, Accuracy: 64.84%\n",
      "Step:  645, Loss: 1.1860, Accuracy: 71.09%\n",
      "Step:  646, Loss: 1.2791, Accuracy: 63.28%\n",
      "Step:  647, Loss: 1.2466, Accuracy: 71.88%\n",
      "Step:  648, Loss: 1.1796, Accuracy: 68.75%\n",
      "Step:  649, Loss: 1.2274, Accuracy: 73.44%\n",
      "Step:  650, Loss: 1.2475, Accuracy: 64.84%\n",
      "Step:  651, Loss: 1.3142, Accuracy: 64.06%\n",
      "Step:  652, Loss: 1.3383, Accuracy: 67.19%\n",
      "Step:  653, Loss: 1.1366, Accuracy: 67.97%\n",
      "Step:  654, Loss: 1.2556, Accuracy: 64.06%\n",
      "Step:  655, Loss: 1.3088, Accuracy: 67.19%\n",
      "Step:  656, Loss: 1.1856, Accuracy: 69.53%\n",
      "Step:  657, Loss: 1.3292, Accuracy: 64.84%\n",
      "Step:  658, Loss: 1.3273, Accuracy: 60.94%\n",
      "Step:  659, Loss: 1.2728, Accuracy: 67.19%\n",
      "Step:  660, Loss: 1.3098, Accuracy: 60.94%\n",
      "Step:  661, Loss: 1.1642, Accuracy: 71.88%\n",
      "Step:  662, Loss: 1.3248, Accuracy: 60.16%\n",
      "Step:  663, Loss: 1.2874, Accuracy: 62.50%\n",
      "Step:  664, Loss: 1.2941, Accuracy: 64.06%\n",
      "Step:  665, Loss: 1.2408, Accuracy: 70.31%\n",
      "Step:  666, Loss: 1.2281, Accuracy: 70.31%\n",
      "Step:  667, Loss: 1.2896, Accuracy: 65.62%\n",
      "Step:  668, Loss: 1.1726, Accuracy: 70.31%\n",
      "Step:  669, Loss: 1.1956, Accuracy: 64.84%\n",
      "Step:  670, Loss: 1.1516, Accuracy: 73.44%\n",
      "Step:  671, Loss: 1.1556, Accuracy: 69.53%\n",
      "Step:  672, Loss: 1.2151, Accuracy: 64.06%\n",
      "Step:  673, Loss: 1.2615, Accuracy: 60.16%\n",
      "Step:  674, Loss: 1.2057, Accuracy: 66.41%\n",
      "Step:  675, Loss: 1.1332, Accuracy: 75.78%\n",
      "Step:  676, Loss: 1.2525, Accuracy: 68.75%\n",
      "Step:  677, Loss: 1.1613, Accuracy: 70.31%\n",
      "Step:  678, Loss: 1.2768, Accuracy: 64.06%\n",
      "Step:  679, Loss: 1.2698, Accuracy: 63.28%\n",
      "Step:  680, Loss: 1.1948, Accuracy: 71.09%\n",
      "Step:  681, Loss: 1.3102, Accuracy: 62.50%\n",
      "Step:  682, Loss: 1.3241, Accuracy: 66.41%\n",
      "Step:  683, Loss: 1.1737, Accuracy: 69.53%\n",
      "Step:  684, Loss: 1.2844, Accuracy: 64.06%\n",
      "Step:  685, Loss: 1.0817, Accuracy: 75.78%\n",
      "Step:  686, Loss: 1.2185, Accuracy: 68.75%\n",
      "Step:  687, Loss: 1.1130, Accuracy: 71.09%\n",
      "Step:  688, Loss: 1.2567, Accuracy: 67.97%\n",
      "Step:  689, Loss: 1.1479, Accuracy: 71.88%\n",
      "Step:  690, Loss: 1.2251, Accuracy: 65.62%\n",
      "Step:  691, Loss: 1.1837, Accuracy: 67.97%\n",
      "Step:  692, Loss: 1.1490, Accuracy: 68.75%\n",
      "Step:  693, Loss: 1.3178, Accuracy: 60.16%\n",
      "Step:  694, Loss: 1.3044, Accuracy: 61.72%\n",
      "Step:  695, Loss: 1.2502, Accuracy: 65.62%\n",
      "Step:  696, Loss: 1.1407, Accuracy: 71.09%\n",
      "Step:  697, Loss: 1.1915, Accuracy: 67.19%\n",
      "Step:  698, Loss: 1.2701, Accuracy: 62.50%\n",
      "Step:  699, Loss: 1.2261, Accuracy: 66.41%\n",
      "Step:  701, Loss: 1.2162, Accuracy: 69.53%\n",
      "Step:  702, Loss: 1.1943, Accuracy: 68.75%\n",
      "Step:  703, Loss: 1.1737, Accuracy: 65.62%\n",
      "Step:  704, Loss: 1.2837, Accuracy: 60.16%\n",
      "Step:  705, Loss: 1.1315, Accuracy: 74.22%\n",
      "Step:  706, Loss: 1.0594, Accuracy: 76.56%\n",
      "Step:  707, Loss: 1.1895, Accuracy: 72.66%\n",
      "Step:  708, Loss: 1.1346, Accuracy: 63.28%\n",
      "Step:  709, Loss: 1.1627, Accuracy: 67.97%\n",
      "Step:  710, Loss: 1.1888, Accuracy: 76.56%\n",
      "Step:  711, Loss: 1.2192, Accuracy: 65.62%\n",
      "Step:  712, Loss: 1.2560, Accuracy: 64.06%\n",
      "Step:  713, Loss: 1.1020, Accuracy: 70.31%\n",
      "Step:  714, Loss: 1.1608, Accuracy: 68.75%\n",
      "Step:  715, Loss: 1.0402, Accuracy: 77.34%\n",
      "Step:  716, Loss: 1.1540, Accuracy: 71.88%\n",
      "Step:  717, Loss: 1.0985, Accuracy: 73.44%\n",
      "Step:  718, Loss: 1.1418, Accuracy: 73.44%\n",
      "Step:  719, Loss: 1.3380, Accuracy: 60.94%\n",
      "Step:  720, Loss: 1.2219, Accuracy: 64.06%\n",
      "Step:  721, Loss: 1.2444, Accuracy: 67.97%\n",
      "Step:  722, Loss: 1.3227, Accuracy: 64.84%\n",
      "Step:  723, Loss: 1.3327, Accuracy: 59.38%\n",
      "Step:  724, Loss: 1.0943, Accuracy: 75.00%\n",
      "Step:  725, Loss: 1.2130, Accuracy: 62.50%\n",
      "Step:  726, Loss: 1.2474, Accuracy: 62.50%\n",
      "Step:  727, Loss: 1.2746, Accuracy: 64.06%\n",
      "Step:  728, Loss: 1.2802, Accuracy: 63.28%\n",
      "Step:  729, Loss: 1.1685, Accuracy: 68.75%\n",
      "Step:  730, Loss: 1.1396, Accuracy: 72.66%\n",
      "Step:  731, Loss: 1.2375, Accuracy: 64.06%\n",
      "Step:  732, Loss: 1.2602, Accuracy: 67.97%\n",
      "Step:  733, Loss: 1.2048, Accuracy: 66.41%\n",
      "Step:  734, Loss: 1.1231, Accuracy: 72.66%\n",
      "Step:  735, Loss: 1.2304, Accuracy: 64.06%\n",
      "Step:  736, Loss: 1.1696, Accuracy: 69.53%\n",
      "Step:  737, Loss: 1.2039, Accuracy: 65.62%\n",
      "Step:  738, Loss: 1.1527, Accuracy: 70.31%\n",
      "Step:  739, Loss: 1.1638, Accuracy: 65.62%\n",
      "Step:  740, Loss: 1.1876, Accuracy: 66.41%\n",
      "Step:  741, Loss: 1.2036, Accuracy: 66.41%\n",
      "Step:  742, Loss: 1.0974, Accuracy: 70.31%\n",
      "Step:  743, Loss: 1.1130, Accuracy: 74.22%\n",
      "Step:  744, Loss: 1.1242, Accuracy: 73.44%\n",
      "Step:  745, Loss: 1.1374, Accuracy: 67.19%\n",
      "Step:  746, Loss: 0.9791, Accuracy: 84.38%\n",
      "Step:  747, Loss: 1.1694, Accuracy: 66.41%\n",
      "Step:  748, Loss: 1.0766, Accuracy: 73.44%\n",
      "Step:  749, Loss: 1.1921, Accuracy: 66.41%\n",
      "Step:  750, Loss: 1.1159, Accuracy: 72.66%\n",
      "Step:  751, Loss: 1.0760, Accuracy: 73.44%\n",
      "Step:  752, Loss: 1.1626, Accuracy: 67.19%\n",
      "Step:  753, Loss: 1.1956, Accuracy: 67.19%\n",
      "Step:  754, Loss: 1.0266, Accuracy: 79.69%\n",
      "Step:  755, Loss: 1.1481, Accuracy: 71.09%\n",
      "Step:  756, Loss: 1.1428, Accuracy: 67.97%\n",
      "Step:  757, Loss: 1.0071, Accuracy: 77.34%\n",
      "Step:  758, Loss: 1.1692, Accuracy: 70.31%\n",
      "Step:  759, Loss: 1.2284, Accuracy: 62.50%\n",
      "Step:  760, Loss: 1.1715, Accuracy: 71.88%\n",
      "Step:  761, Loss: 1.1955, Accuracy: 64.06%\n",
      "Step:  762, Loss: 1.2015, Accuracy: 65.62%\n",
      "Step:  763, Loss: 1.1302, Accuracy: 69.53%\n",
      "Step:  764, Loss: 0.9921, Accuracy: 75.00%\n",
      "Step:  765, Loss: 1.1098, Accuracy: 69.53%\n",
      "Step:  766, Loss: 1.1525, Accuracy: 71.09%\n",
      "Step:  767, Loss: 1.1169, Accuracy: 71.09%\n",
      "Step:  768, Loss: 1.1828, Accuracy: 68.75%\n",
      "Step:  769, Loss: 1.1736, Accuracy: 70.31%\n",
      "Step:  770, Loss: 1.1335, Accuracy: 70.31%\n",
      "Step:  771, Loss: 1.2469, Accuracy: 64.84%\n",
      "Step:  772, Loss: 1.0955, Accuracy: 71.09%\n",
      "Step:  773, Loss: 1.2498, Accuracy: 58.59%\n",
      "Step:  774, Loss: 1.1405, Accuracy: 72.66%\n",
      "Step:  775, Loss: 1.1982, Accuracy: 65.62%\n",
      "Step:  776, Loss: 1.2239, Accuracy: 63.28%\n",
      "Step:  777, Loss: 1.1305, Accuracy: 73.44%\n",
      "Step:  778, Loss: 1.0810, Accuracy: 75.78%\n",
      "Step:  779, Loss: 1.2202, Accuracy: 66.41%\n",
      "Step:  780, Loss: 1.1326, Accuracy: 69.53%\n",
      "Step:  781, Loss: 1.1717, Accuracy: 67.97%\n",
      "Step:  782, Loss: 1.1972, Accuracy: 67.19%\n",
      "Step:  783, Loss: 1.2969, Accuracy: 59.38%\n",
      "Step:  784, Loss: 1.1527, Accuracy: 69.53%\n",
      "Step:  785, Loss: 1.1624, Accuracy: 70.31%\n",
      "Step:  786, Loss: 1.0652, Accuracy: 75.00%\n",
      "Step:  787, Loss: 1.1268, Accuracy: 71.88%\n",
      "Step:  788, Loss: 1.1211, Accuracy: 74.22%\n",
      "Step:  789, Loss: 1.1752, Accuracy: 64.84%\n",
      "Step:  790, Loss: 1.2538, Accuracy: 64.06%\n",
      "Step:  791, Loss: 1.1537, Accuracy: 69.53%\n",
      "Step:  792, Loss: 1.1024, Accuracy: 67.97%\n",
      "Step:  793, Loss: 1.1748, Accuracy: 68.75%\n",
      "Step:  794, Loss: 1.1166, Accuracy: 68.75%\n",
      "Step:  795, Loss: 1.2082, Accuracy: 64.84%\n",
      "Step:  796, Loss: 1.1785, Accuracy: 66.41%\n",
      "Step:  797, Loss: 1.0811, Accuracy: 73.44%\n",
      "Step:  798, Loss: 1.0527, Accuracy: 71.09%\n",
      "Step:  799, Loss: 1.0902, Accuracy: 75.00%\n",
      "Step:  801, Loss: 1.0912, Accuracy: 69.53%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  802, Loss: 1.1014, Accuracy: 72.66%\n",
      "Step:  803, Loss: 1.1062, Accuracy: 71.88%\n",
      "Step:  804, Loss: 1.1440, Accuracy: 73.44%\n",
      "Step:  805, Loss: 1.1734, Accuracy: 67.97%\n",
      "Step:  806, Loss: 1.0226, Accuracy: 76.56%\n",
      "Step:  807, Loss: 1.1543, Accuracy: 69.53%\n",
      "Step:  808, Loss: 1.1527, Accuracy: 72.66%\n",
      "Step:  809, Loss: 1.2145, Accuracy: 63.28%\n",
      "Step:  810, Loss: 1.1221, Accuracy: 70.31%\n",
      "Step:  811, Loss: 1.0425, Accuracy: 75.00%\n",
      "Step:  812, Loss: 1.1502, Accuracy: 71.88%\n",
      "Step:  813, Loss: 1.1769, Accuracy: 67.19%\n",
      "Step:  814, Loss: 1.1154, Accuracy: 70.31%\n",
      "Step:  815, Loss: 1.2235, Accuracy: 64.06%\n",
      "Step:  816, Loss: 1.0240, Accuracy: 75.78%\n",
      "Step:  817, Loss: 1.1764, Accuracy: 64.84%\n",
      "Step:  818, Loss: 1.1292, Accuracy: 68.75%\n",
      "Step:  819, Loss: 1.1234, Accuracy: 72.66%\n",
      "Step:  820, Loss: 1.1561, Accuracy: 66.41%\n",
      "Step:  821, Loss: 1.0485, Accuracy: 76.56%\n",
      "Step:  822, Loss: 1.2156, Accuracy: 67.19%\n",
      "Step:  823, Loss: 1.1256, Accuracy: 75.00%\n",
      "Step:  824, Loss: 1.1118, Accuracy: 75.00%\n",
      "Step:  825, Loss: 1.1642, Accuracy: 68.75%\n",
      "Step:  826, Loss: 1.1213, Accuracy: 74.22%\n",
      "Step:  827, Loss: 1.0492, Accuracy: 72.66%\n",
      "Step:  828, Loss: 1.1553, Accuracy: 70.31%\n",
      "Step:  829, Loss: 1.0382, Accuracy: 72.66%\n",
      "Step:  830, Loss: 1.0882, Accuracy: 67.97%\n",
      "Step:  831, Loss: 1.0928, Accuracy: 75.78%\n",
      "Step:  832, Loss: 1.0380, Accuracy: 72.66%\n",
      "Step:  833, Loss: 1.1633, Accuracy: 65.62%\n",
      "Step:  834, Loss: 1.1217, Accuracy: 67.97%\n",
      "Step:  835, Loss: 1.0275, Accuracy: 75.00%\n",
      "Step:  836, Loss: 0.9985, Accuracy: 75.78%\n",
      "Step:  837, Loss: 1.0121, Accuracy: 71.88%\n",
      "Step:  838, Loss: 1.0223, Accuracy: 75.00%\n",
      "Step:  839, Loss: 1.1429, Accuracy: 66.41%\n",
      "Step:  840, Loss: 1.1346, Accuracy: 75.00%\n",
      "Step:  841, Loss: 1.0073, Accuracy: 75.78%\n",
      "Step:  842, Loss: 1.1096, Accuracy: 67.19%\n",
      "Step:  843, Loss: 1.1458, Accuracy: 70.31%\n",
      "Step:  844, Loss: 1.1551, Accuracy: 68.75%\n",
      "Step:  845, Loss: 1.1078, Accuracy: 70.31%\n",
      "Step:  846, Loss: 1.1043, Accuracy: 67.97%\n",
      "Step:  847, Loss: 1.0802, Accuracy: 71.09%\n",
      "Step:  848, Loss: 1.1875, Accuracy: 62.50%\n",
      "Step:  849, Loss: 1.0905, Accuracy: 70.31%\n",
      "Step:  850, Loss: 1.0108, Accuracy: 77.34%\n",
      "Step:  851, Loss: 1.0788, Accuracy: 71.88%\n",
      "Step:  852, Loss: 1.2197, Accuracy: 63.28%\n",
      "Step:  853, Loss: 1.0112, Accuracy: 76.56%\n",
      "Step:  854, Loss: 1.0862, Accuracy: 69.53%\n",
      "Step:  855, Loss: 1.1798, Accuracy: 71.09%\n",
      "Step:  856, Loss: 1.0800, Accuracy: 71.88%\n",
      "Step:  857, Loss: 1.1485, Accuracy: 72.66%\n",
      "Step:  858, Loss: 1.1330, Accuracy: 69.53%\n",
      "Step:  859, Loss: 1.1577, Accuracy: 64.06%\n",
      "Step:  860, Loss: 1.1255, Accuracy: 69.53%\n",
      "Step:  861, Loss: 1.1162, Accuracy: 71.88%\n",
      "Step:  862, Loss: 1.0952, Accuracy: 71.88%\n",
      "Step:  863, Loss: 1.0465, Accuracy: 75.00%\n",
      "Step:  864, Loss: 1.0846, Accuracy: 68.75%\n",
      "Step:  865, Loss: 1.0617, Accuracy: 68.75%\n",
      "Step:  866, Loss: 1.1367, Accuracy: 67.19%\n",
      "Step:  867, Loss: 1.1742, Accuracy: 64.06%\n",
      "Step:  868, Loss: 1.0826, Accuracy: 74.22%\n",
      "Step:  869, Loss: 1.2267, Accuracy: 62.50%\n",
      "Step:  870, Loss: 1.0460, Accuracy: 71.88%\n",
      "Step:  871, Loss: 1.1441, Accuracy: 69.53%\n",
      "Step:  872, Loss: 0.9849, Accuracy: 71.88%\n",
      "Step:  873, Loss: 1.0259, Accuracy: 78.12%\n",
      "Step:  874, Loss: 1.0545, Accuracy: 76.56%\n",
      "Step:  875, Loss: 1.0206, Accuracy: 73.44%\n",
      "Step:  876, Loss: 1.1137, Accuracy: 75.78%\n",
      "Step:  877, Loss: 1.0631, Accuracy: 74.22%\n",
      "Step:  878, Loss: 1.0434, Accuracy: 76.56%\n",
      "Step:  879, Loss: 1.1348, Accuracy: 70.31%\n",
      "Step:  880, Loss: 1.1507, Accuracy: 68.75%\n",
      "Step:  881, Loss: 1.0988, Accuracy: 67.19%\n",
      "Step:  882, Loss: 1.1520, Accuracy: 65.62%\n",
      "Step:  883, Loss: 1.1239, Accuracy: 68.75%\n",
      "Step:  884, Loss: 1.0210, Accuracy: 77.34%\n",
      "Step:  885, Loss: 1.0768, Accuracy: 69.53%\n",
      "Step:  886, Loss: 1.0093, Accuracy: 81.25%\n",
      "Step:  887, Loss: 0.9945, Accuracy: 74.22%\n",
      "Step:  888, Loss: 1.1574, Accuracy: 67.19%\n",
      "Step:  889, Loss: 1.0742, Accuracy: 77.34%\n",
      "Step:  890, Loss: 1.0439, Accuracy: 68.75%\n",
      "Step:  891, Loss: 1.0813, Accuracy: 74.22%\n",
      "Step:  892, Loss: 1.1194, Accuracy: 66.41%\n",
      "Step:  893, Loss: 1.1144, Accuracy: 68.75%\n",
      "Step:  894, Loss: 1.0871, Accuracy: 69.53%\n",
      "Step:  895, Loss: 1.0191, Accuracy: 75.78%\n",
      "Step:  896, Loss: 1.1316, Accuracy: 67.97%\n",
      "Step:  897, Loss: 1.1885, Accuracy: 65.62%\n",
      "Step:  898, Loss: 1.0232, Accuracy: 73.44%\n",
      "Step:  899, Loss: 1.0736, Accuracy: 75.00%\n",
      "Step:  901, Loss: 1.0498, Accuracy: 71.88%\n",
      "Step:  902, Loss: 1.1472, Accuracy: 72.66%\n",
      "Step:  903, Loss: 1.0482, Accuracy: 71.09%\n",
      "Step:  904, Loss: 1.1598, Accuracy: 64.06%\n",
      "Step:  905, Loss: 1.0081, Accuracy: 75.00%\n",
      "Step:  906, Loss: 1.0573, Accuracy: 77.34%\n",
      "Step:  907, Loss: 1.0688, Accuracy: 75.00%\n",
      "Step:  908, Loss: 1.0474, Accuracy: 71.88%\n",
      "Step:  909, Loss: 1.0292, Accuracy: 75.00%\n",
      "Step:  910, Loss: 1.1047, Accuracy: 70.31%\n",
      "Step:  911, Loss: 1.0152, Accuracy: 71.88%\n",
      "Step:  912, Loss: 1.0376, Accuracy: 71.88%\n",
      "Step:  913, Loss: 1.0590, Accuracy: 72.66%\n",
      "Step:  914, Loss: 1.1063, Accuracy: 67.19%\n",
      "Step:  915, Loss: 1.0833, Accuracy: 70.31%\n",
      "Step:  916, Loss: 1.1451, Accuracy: 70.31%\n",
      "Step:  917, Loss: 1.1139, Accuracy: 72.66%\n",
      "Step:  918, Loss: 1.1241, Accuracy: 63.28%\n",
      "Step:  919, Loss: 1.2193, Accuracy: 64.06%\n",
      "Step:  920, Loss: 1.1420, Accuracy: 68.75%\n",
      "Step:  921, Loss: 1.1097, Accuracy: 70.31%\n",
      "Step:  922, Loss: 1.0753, Accuracy: 74.22%\n",
      "Step:  923, Loss: 1.1521, Accuracy: 70.31%\n",
      "Step:  924, Loss: 1.0952, Accuracy: 73.44%\n",
      "Step:  925, Loss: 1.0379, Accuracy: 75.00%\n",
      "Step:  926, Loss: 1.1594, Accuracy: 66.41%\n",
      "Step:  927, Loss: 0.9556, Accuracy: 77.34%\n",
      "Step:  928, Loss: 1.1497, Accuracy: 68.75%\n",
      "Step:  929, Loss: 1.0175, Accuracy: 73.44%\n",
      "Step:  930, Loss: 1.1787, Accuracy: 64.84%\n",
      "Step:  931, Loss: 1.0242, Accuracy: 73.44%\n",
      "Step:  932, Loss: 1.0536, Accuracy: 75.00%\n",
      "Step:  933, Loss: 1.0473, Accuracy: 71.09%\n",
      "Step:  934, Loss: 1.1460, Accuracy: 66.41%\n",
      "Step:  935, Loss: 0.9985, Accuracy: 78.91%\n",
      "Step:  936, Loss: 1.0250, Accuracy: 75.78%\n",
      "Step:  937, Loss: 1.1105, Accuracy: 71.88%\n",
      "Step:  938, Loss: 1.1640, Accuracy: 67.97%\n",
      "Step:  939, Loss: 0.9913, Accuracy: 73.44%\n",
      "Step:  940, Loss: 1.0788, Accuracy: 67.97%\n",
      "Step:  941, Loss: 1.1687, Accuracy: 67.97%\n",
      "Step:  942, Loss: 1.0977, Accuracy: 67.97%\n",
      "Step:  943, Loss: 1.0349, Accuracy: 73.44%\n",
      "Step:  944, Loss: 1.1065, Accuracy: 66.41%\n",
      "Step:  945, Loss: 1.1247, Accuracy: 67.97%\n",
      "Step:  946, Loss: 1.1832, Accuracy: 66.41%\n",
      "Step:  947, Loss: 1.0781, Accuracy: 71.09%\n",
      "Step:  948, Loss: 1.0969, Accuracy: 67.19%\n",
      "Step:  949, Loss: 1.0871, Accuracy: 71.88%\n",
      "Step:  950, Loss: 1.1104, Accuracy: 72.66%\n",
      "Step:  951, Loss: 1.0235, Accuracy: 72.66%\n",
      "Step:  952, Loss: 0.9513, Accuracy: 78.12%\n",
      "Step:  953, Loss: 1.0524, Accuracy: 71.09%\n",
      "Step:  954, Loss: 1.1546, Accuracy: 67.19%\n",
      "Step:  955, Loss: 0.9537, Accuracy: 78.91%\n",
      "Step:  956, Loss: 0.9555, Accuracy: 75.00%\n",
      "Step:  957, Loss: 1.1034, Accuracy: 68.75%\n",
      "Step:  958, Loss: 1.0715, Accuracy: 64.84%\n",
      "Step:  959, Loss: 1.0805, Accuracy: 70.31%\n",
      "Step:  960, Loss: 1.0803, Accuracy: 68.75%\n",
      "Step:  961, Loss: 1.0099, Accuracy: 72.66%\n",
      "Step:  962, Loss: 1.0328, Accuracy: 77.34%\n",
      "Step:  963, Loss: 1.0828, Accuracy: 69.53%\n",
      "Step:  964, Loss: 1.0551, Accuracy: 68.75%\n",
      "Step:  965, Loss: 1.1211, Accuracy: 67.19%\n",
      "Step:  966, Loss: 1.0252, Accuracy: 74.22%\n",
      "Step:  967, Loss: 0.9987, Accuracy: 72.66%\n",
      "Step:  968, Loss: 1.0425, Accuracy: 71.09%\n",
      "Step:  969, Loss: 0.9931, Accuracy: 72.66%\n",
      "Step:  970, Loss: 1.0444, Accuracy: 71.09%\n",
      "Step:  971, Loss: 0.9938, Accuracy: 78.12%\n",
      "Step:  972, Loss: 0.9894, Accuracy: 73.44%\n",
      "Step:  973, Loss: 1.0300, Accuracy: 71.88%\n",
      "Step:  974, Loss: 1.0276, Accuracy: 75.78%\n",
      "Step:  975, Loss: 1.0384, Accuracy: 75.78%\n",
      "Step:  976, Loss: 1.0412, Accuracy: 75.00%\n",
      "Step:  977, Loss: 1.0741, Accuracy: 75.78%\n",
      "Step:  978, Loss: 0.9564, Accuracy: 78.12%\n",
      "Step:  979, Loss: 1.0298, Accuracy: 70.31%\n",
      "Step:  980, Loss: 1.0640, Accuracy: 71.09%\n",
      "Step:  981, Loss: 1.0232, Accuracy: 74.22%\n",
      "Step:  982, Loss: 0.9395, Accuracy: 76.56%\n",
      "Step:  983, Loss: 0.9895, Accuracy: 76.56%\n",
      "Step:  984, Loss: 1.0433, Accuracy: 71.88%\n",
      "Step:  985, Loss: 1.0611, Accuracy: 71.09%\n",
      "Step:  986, Loss: 0.8887, Accuracy: 82.03%\n",
      "Step:  987, Loss: 1.0908, Accuracy: 70.31%\n",
      "Step:  988, Loss: 1.0034, Accuracy: 74.22%\n",
      "Step:  989, Loss: 0.9737, Accuracy: 73.44%\n",
      "Step:  990, Loss: 1.0938, Accuracy: 72.66%\n",
      "Step:  991, Loss: 1.0375, Accuracy: 69.53%\n",
      "Step:  992, Loss: 0.9887, Accuracy: 74.22%\n",
      "Step:  993, Loss: 1.0371, Accuracy: 70.31%\n",
      "Step:  994, Loss: 1.0078, Accuracy: 71.88%\n",
      "Step:  995, Loss: 1.0668, Accuracy: 71.88%\n",
      "Step:  996, Loss: 1.0846, Accuracy: 69.53%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  997, Loss: 1.0316, Accuracy: 75.00%\n",
      "Step:  998, Loss: 1.1782, Accuracy: 66.41%\n",
      "Step:  999, Loss: 0.9345, Accuracy: 78.12%\n",
      "Loss: 0.9994, Accuracy: 73.73%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        batch=mnist.train.next_batch(128)\n",
    "        _,l,ac=sess.run([opt,loss,acc],{x:batch[0],y_:batch[1]})\n",
    "        if i%100:\n",
    "            print(\"Step:{:>5}, Loss:{:>7.4f}, Accuracy:{:>7.2%}\".format(i, l, ac))\n",
    "    \n",
    "    t_l,t_ac=sess.run([loss,acc],{x:mnist.test.images,y_:mnist.test.labels})\n",
    "    print(\"Loss:{:>7.4f}, Accuracy:{:>7.2%}\".format( t_l, t_ac))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建单层隐藏层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_h = tf.Variable(tf.truncated_normal([784, 512], mean=0., stddev=0.1))\n",
    "b_h = tf.Variable(tf.zeros([512]))\n",
    "h_linear_output = tf.matmul(x, W_h) + b_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_output = tf.nn.tanh(h_linear_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 连接全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal([512, 10], mean=0., stddev=0.1))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.matmul(h_output, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_result = tf.nn.softmax(y)\n",
    "pred = tf.argmax(softmax_result, axis=1)\n",
    "true = tf.argmax(y_, axis=1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, true), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 Session 并且训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    0, Loss: 2.9742, Accuracy: 12.50%\n",
      "Step:  100, Loss: 0.3540, Accuracy: 89.06%\n",
      "Step:  200, Loss: 0.2728, Accuracy: 92.97%\n",
      "Step:  300, Loss: 0.4171, Accuracy: 86.72%\n",
      "Step:  400, Loss: 0.1956, Accuracy: 94.53%\n",
      "Step:  500, Loss: 0.0493, Accuracy: 99.22%\n",
      "Step:  600, Loss: 0.2376, Accuracy: 92.19%\n",
      "Step:  700, Loss: 0.1439, Accuracy: 96.88%\n",
      "Step:  800, Loss: 0.1855, Accuracy: 95.31%\n",
      "Step:  900, Loss: 0.3610, Accuracy: 89.84%\n",
      "Step: 1000, Loss: 0.0669, Accuracy: 96.88%\n",
      "Step: 1100, Loss: 0.1723, Accuracy: 96.09%\n",
      "Step: 1200, Loss: 0.1010, Accuracy: 96.88%\n",
      "Step: 1300, Loss: 0.0592, Accuracy: 98.44%\n",
      "Step: 1400, Loss: 0.0949, Accuracy: 96.88%\n",
      "Step: 1500, Loss: 0.1134, Accuracy: 97.66%\n",
      "Step: 1600, Loss: 0.1009, Accuracy: 96.88%\n",
      "Step: 1700, Loss: 0.1958, Accuracy: 95.31%\n",
      "Step: 1800, Loss: 0.0835, Accuracy: 96.09%\n",
      "Step: 1900, Loss: 0.0883, Accuracy: 96.88%\n",
      "Step: 2000, Loss: 0.0627, Accuracy: 97.66%\n",
      "Step: 2100, Loss: 0.0867, Accuracy: 96.09%\n",
      "Step: 2200, Loss: 0.0649, Accuracy: 98.44%\n",
      "Step: 2300, Loss: 0.1034, Accuracy: 96.09%\n",
      "Step: 2400, Loss: 0.0542, Accuracy: 98.44%\n",
      "Step: 2500, Loss: 0.0685, Accuracy: 96.88%\n",
      "Step: 2600, Loss: 0.0472, Accuracy: 99.22%\n",
      "Step: 2700, Loss: 0.1327, Accuracy: 94.53%\n",
      "Step: 2800, Loss: 0.1037, Accuracy: 96.88%\n",
      "Step: 2900, Loss: 0.0469, Accuracy: 98.44%\n",
      "Step: 3000, Loss: 0.0278, Accuracy: 98.44%\n",
      "Step: 3100, Loss: 0.0949, Accuracy: 99.22%\n",
      "Step: 3200, Loss: 0.0799, Accuracy: 98.44%\n",
      "Step: 3300, Loss: 0.0270, Accuracy: 99.22%\n",
      "Step: 3400, Loss: 0.0297, Accuracy: 99.22%\n",
      "Step: 3500, Loss: 0.0258, Accuracy: 99.22%\n",
      "Step: 3600, Loss: 0.0177, Accuracy: 99.22%\n",
      "Step: 3700, Loss: 0.0306, Accuracy: 99.22%\n",
      "Step: 3800, Loss: 0.0385, Accuracy: 98.44%\n",
      "Step: 3900, Loss: 0.0154, Accuracy:100.00%\n",
      "Step: 4000, Loss: 0.0583, Accuracy: 96.09%\n",
      "Step: 4100, Loss: 0.0365, Accuracy: 99.22%\n",
      "Step: 4200, Loss: 0.0256, Accuracy:100.00%\n",
      "Step: 4300, Loss: 0.0449, Accuracy: 98.44%\n",
      "Step: 4400, Loss: 0.0246, Accuracy: 99.22%\n",
      "Step: 4500, Loss: 0.0216, Accuracy:100.00%\n",
      "Step: 4600, Loss: 0.0511, Accuracy: 99.22%\n",
      "Step: 4700, Loss: 0.0243, Accuracy: 99.22%\n",
      "Step: 4800, Loss: 0.0034, Accuracy:100.00%\n",
      "Step: 4900, Loss: 0.0117, Accuracy:100.00%\n",
      "Training Finished, Loss: 0.0654, Accuracy: 97.93%\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(5000):\n",
    "    batch = mnist.train.next_batch(128)\n",
    "    _, loss_, acu = sess.run([train_step, loss, accuracy], {x: batch[0], y_: batch[1]})\n",
    "    if i % 100 == 0:\n",
    "        print(\"Step:{:>5}, Loss:{:>7.4f}, Accuracy:{:>7.2%}\".format(i, loss_, acu))\n",
    "loss_, acu = sess.run([loss, accuracy], {x: mnist.test.images, \n",
    "                                                        y_: mnist.test.labels})\n",
    "print(\"Training Finished, Loss:{:>7.4f}, Accuracy:{:>7.2%}\".format(loss_, acu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## tf.layers.dense\n",
    "[用法](https://tensorflow.google.cn/api_docs/python/tf/layers/dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuyanfang/anaconda3/envs/tensorflow1.4/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 构建3层隐藏层 增加validationset(10 分钟):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义图\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    #定义参数\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    \n",
    "    #定义模型结构：\n",
    "    \n",
    "    layer1=tf.layers.dense(inputs=x,\n",
    "                           units=512,\n",
    "                           activation=tf.nn.relu,\n",
    "                           bias_initializer=tf.constant_initializer(0),\n",
    "                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.1)\n",
    "                          )\n",
    "    layer2=tf.layers.dense(inputs=layer1,\n",
    "                           units=128,\n",
    "                           activation=tf.nn.relu,\n",
    "                           bias_initializer=tf.constant_initializer(0),\n",
    "                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.1)\n",
    "                          )\n",
    "    layer3=tf.layers.dense(inputs=layer2,\n",
    "                           units=512,\n",
    "                           activation=tf.nn.relu,\n",
    "                           bias_initializer=tf.constant_initializer(0),\n",
    "                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.1)\n",
    "                          )\n",
    "    # 分类：\n",
    "    out=tf.layers.dense(inputs=layer3,\n",
    "                       units=10,\n",
    "                       activation=None,\n",
    "                       bias_initializer=tf.constant_initializer(0),\n",
    "                       kernel_initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    # 误差与准确率\n",
    "    with tf.name_scope('loss'):\n",
    "        loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=out))\n",
    "    opt=tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "    out_softmax= tf.nn.softmax(out)\n",
    "    with tf.name_scope(\"acc\"):\n",
    "        acc=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_,axis=1),tf.argmax(out_softmax,axis=1)),tf.float32))\n",
    "\n",
    "    tf.summary.scalar('loss',loss)\n",
    "    tf.summary.scalar('acc',acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.8779,accuracy=0.039062\n",
      "loss=0.7595,accuracy=0.812500\n",
      "loss=0.4861,accuracy=0.875000\n",
      "loss=0.3919,accuracy=0.882812\n",
      "loss=0.3963,accuracy=0.906250\n",
      "loss=0.4097,accuracy=0.875000\n",
      "loss=0.4014,accuracy=0.875000\n",
      "loss=0.2159,accuracy=0.937500\n",
      "loss=0.2837,accuracy=0.921875\n",
      "loss=0.2049,accuracy=0.953125\n",
      "loss=0.3587,accuracy=0.890625\n",
      "loss=0.2642,accuracy=0.937500\n",
      "loss=0.4592,accuracy=0.882812\n",
      "loss=0.2054,accuracy=0.953125\n",
      "loss=0.1369,accuracy=0.968750\n",
      "loss=0.2605,accuracy=0.929688\n",
      "loss=0.2592,accuracy=0.937500\n",
      "loss=0.2547,accuracy=0.914062\n",
      "loss=0.1493,accuracy=0.960938\n",
      "loss=0.3157,accuracy=0.906250\n",
      "[0.20959419, 0.9377]\n"
     ]
    }
   ],
   "source": [
    "#运行图\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    merge=tf.summary.merge_all()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "    for i in range(2000):\n",
    "        batch=mnist.train.next_batch(128)\n",
    "        _,lo,ac=sess.run([opt,loss,acc],{x:batch[0],y_:batch[1]})\n",
    "        if i%100==0:\n",
    "            print(\"loss={:>5.4f},accuracy={:>5.6f}\".format(lo,ac))\n",
    "    print(sess.run([loss,acc],{x:mnist.test.images,y_:mnist.test.labels}))\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = tf.layers.dense(inputs=x, \n",
    "                         units=512, \n",
    "                         activation=tf.nn.tanh,                        \n",
    "                         bias_initializer=tf.constant_initializer(0),\n",
    "                         kernel_initializer=tf.truncated_normal_initializer(stddev=0.1)\n",
    "                        )\n",
    "layer2 = tf.layers.dense(inputs=layer1, \n",
    "                         units=256, \n",
    "                         activation=tf.nn.tanh,                        \n",
    "                         bias_initializer=tf.constant_initializer(0),\n",
    "                         kernel_initializer=tf.truncated_normal_initializer(stddev=0.1)\n",
    "                        )\n",
    "layer3 = tf.layers.dense(inputs=layer2, \n",
    "                         units=128, \n",
    "                         activation=tf.nn.tanh,                        \n",
    "                         bias_initializer=tf.constant_initializer(0),\n",
    "                         kernel_initializer=tf.truncated_normal_initializer(stddev=0.1)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.layers.dense(inputs=layer3, \n",
    "                    units=10, \n",
    "                    activation=None, \n",
    "                    bias_initializer=tf.constant_initializer(0), \n",
    "                    kernel_initializer=tf.truncated_normal_initializer(stddev=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_result = tf.nn.softmax(y)\n",
    "pred = tf.argmax(softmax_result, axis=1)\n",
    "true = tf.argmax(y_, axis=1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, true), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义Interactive Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5000):\n",
    "    batch = mnist.train.next_batch(128)\n",
    "    _, loss_, acu = sess.run([train_step, loss, accuracy], {x: batch[0], y_: batch[1]})\n",
    "    if i % 100 == 0:\n",
    "        print(\"Step:{:>5}, Loss:{:>7.4f}, Accuracy:{:>7.2%}\".format(i, loss_, acu))\n",
    "        loss_, acu = sess.run([loss, accuracy], {x: mnist.validation.images, \n",
    "                                                 y_: mnist.validation.labels})\n",
    "        print(\"Step:{:>5}, Vali Loss:{:>7.4f}, Vali Accuracy:{:>7.2%}\".format(i, loss_, acu))\n",
    "loss_, acu = sess.run([loss, accuracy], {x: mnist.test.images, \n",
    "                                        y_: mnist.test.labels})\n",
    "print(\"Training Finished, Loss:{:>7.4f}, Accuracy:{:>7.2%}\".format(loss_, acu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回顾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10], name='y')\n",
    "    with tf.name_scope('hidden_layers'):\n",
    "        layer1 = tf.layers.dense(inputs=x, \n",
    "                                 units=512, \n",
    "                                 activation=tf.nn.tanh,                        \n",
    "                                 bias_initializer=tf.constant_initializer(0),\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                                 name='l1'\n",
    "                                )\n",
    "        layer2 = tf.layers.dense(inputs=layer1, \n",
    "                                 units=256, \n",
    "                                 activation=tf.nn.tanh,                        \n",
    "                                 bias_initializer=tf.constant_initializer(0),\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                                 name='l2'\n",
    "                                )\n",
    "        layer3 = tf.layers.dense(inputs=layer2, \n",
    "                                 units=128, \n",
    "                                 activation=tf.nn.tanh,                        \n",
    "                                 bias_initializer=tf.constant_initializer(0),\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                                 name='l3'\n",
    "                                )\n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        y = tf.layers.dense(inputs=layer3, \n",
    "                            units=10, \n",
    "                            activation=None, \n",
    "                            bias_initializer=tf.constant_initializer(0), \n",
    "                            kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                            name='dense'\n",
    "                           )\n",
    "    with tf.name_scope(\"cal_loss\"):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    with tf.name_scope(\"cal_accuracy\"):\n",
    "        softmax_result = tf.nn.softmax(y)\n",
    "        pred = tf.argmax(softmax_result, axis=1)\n",
    "        true = tf.argmax(y_, axis=1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, true), tf.float32))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    merged = tf.summary.merge_all()\n",
    "    # 定义filewriter\n",
    "    writer_train = tf.summary.FileWriter('./tmp/train', graph=graph)\n",
    "    writer_vali = tf.summary.FileWriter('./tmp/vali')\n",
    "    # 定义saver\n",
    "    saver = tf.train.Saver()\n",
    "    for i in range(5000):\n",
    "        batch = mnist.train.next_batch(128)\n",
    "        _, mgd = sess.run([train_step, merged], {x:batch[0],\n",
    "                                                 y_:batch[1]\n",
    "                                                })\n",
    "        writer_train.add_summary(mgd, global_step=i)\n",
    "        mgd = sess.run(merged, {x: mnist.validation.images,\n",
    "                                              y_: mnist.validation.labels\n",
    "                                                })\n",
    "        writer_vali.add_summary(mgd, global_step=i)\n",
    "    loss_, acu = sess.run([loss, accuracy], {x: mnist.test.images, \n",
    "                                            y_: mnist.test.labels})\n",
    "    print(\"Training Finished, Loss:{:>7.4f}, Accuracy:{:>7.2%}\".format(loss_, acu))\n",
    "    saver.save(sess, './tmp/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "t = mpimg.imread('./test.jpg')\n",
    "test = np.mean(t, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.reshape(test / 255., 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, './tmp/model.ckpt')\n",
    "    p = sess.run(pred, {x: [test]})\n",
    "    print(\"prediction result{:>2}\".format(p[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2]\n",
    "x,y=a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1]\n",
    "a += 2,\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
