{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于DNN的花卉识别练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “skimage” python 图像处理模块，如果用的conda的虚拟环境，需要用conda install scikit-image 进行安装\n",
    "# glob 文件路径查找模块\n",
    "# cv2图像处理包，比skimage要快\n",
    "import cv2\n",
    "from skimage import io,transform\n",
    "import glob \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集路径---根据自己的路径进行改写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/liuyanfang/lyf/Codes/GAN/flower_photos/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们的数据集图片大小不一致，所以需要resize成统一大小 这里resize成100x100x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=100\n",
    "h=100\n",
    "c=3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据集图片并添加标签,最后的形式是data 对应图片, label 是标签，roses 0,daisy 1,sunflowers 2,tulips 3,dandelion 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuyue/anaconda2/envs/tensorflow-gpu/lib/python3.5/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "def read_img(path):\n",
    "    cate=[path+x for x in os.listdir(path) if os.path.isdir(path+x)]#逐个打开文件夹，并提取图片\n",
    "    imgs=[]\n",
    "    labels=[]\n",
    "    for idx,folder in enumerate(cate):#用于在for循环中得到计数\n",
    "        for im in glob.glob(folder+'/*.jpg'):#返回所有匹配的文件路径列表\n",
    "            #print('reading the images:%s'%(im))\n",
    "            img=io.imread(im)\n",
    "            img=transform.resize(img,(w,h))\n",
    "            imgs.append(img)\n",
    "            labels.append(idx)\n",
    "                  \n",
    "    return np.asarray(imgs,np.float32),np.asarray(labels,np.int32)#变成矩阵格式\n",
    "data,label=read_img(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    cate=[path+x for x in os.listdir(path) if os.path.isdir(path+x)]#逐个打开文件夹，并提取图片\n",
    "    imgs=[]\n",
    "    labels=[]\n",
    "    for idx,folder in enumerate(cate):#用于在for循环中得到计数\n",
    "        for im in os.listdir(folder):#返回所有匹配的文件路径列表        \n",
    "       \n",
    "            img=cv2.imread(folder+'/'+im)\n",
    "            #print('reading the images:%s'%(folder+'/'+im))\n",
    "            img=cv2.resize(img,(w,h))\n",
    "            imgs.append(img)\n",
    "            labels.append(idx)\n",
    "                  \n",
    "    return np.asarray(imgs,np.float32),np.asarray(labels,np.int32)#变成矩阵格式\n",
    "data,label=read_img(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据集打乱顺序，下阶段课程会讲其他方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 3 ... 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "num_example=data.shape[0] # data.shape是(3029, 100, 100, 3)\n",
    "arr=np.arange(num_example)# 创建等差数组 0，1，...,3028\n",
    "np.random.shuffle(arr)# 打乱顺序\n",
    "data=data[arr]\n",
    "label=label[arr]\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将标签onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels):\n",
    "    l = len(labels)\n",
    "    res = np.zeros((l, 5), dtype=np.float32)\n",
    "    for i in range(l):\n",
    "        res[i][labels[i]] = 1.\n",
    "    return res\n",
    "label_oh = to_one_hot(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将所有数据集分为训练集80%、验证集10%和测试集10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio=0.8\n",
    "s=np.int(num_example*ratio)\n",
    "x_train=data[:s]\n",
    "y_train=label_oh[:s]\n",
    "x_val=data[s:]\n",
    "y_val=label_oh[s:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(dataset, labelset, batchsize):\n",
    "    for i in range(dataset.shape[0]//batchsize):\n",
    "        pos = i * batchsize\n",
    "        x = dataset[pos:pos + batchsize]\n",
    "        y = labelset[pos:pos + batchsize]\n",
    "        yield x,y\n",
    "    remain = np.shape(dataset)[0] % batchsize\n",
    "    if remain != 0:\n",
    "        x = dataset[-remain:]\n",
    "        y = labelset[-remain:]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    X=tf.placeholder(tf.float32,shape=[None,w,h,c],name='X')\n",
    "    y_=tf.placeholder(tf.int32,shape=[None,5],name='y_')#keep_prob = tf.placeholder(tf.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope('hidden_layers'):\n",
    "        re1 = tf.reshape(X, [-1, w * h* c])\n",
    "        dense1 = tf.layers.dense(inputs=re1,      units=1024,   activation=tf.nn.relu,\n",
    "                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        dense2 = tf.layers.dense(inputs=dense1,      units=1024,   activation=tf.nn.relu,\n",
    "                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        dense3= tf.layers.dense(inputs=dense2,  units=128,  activation=tf.nn.relu,\n",
    "                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        logits= tf.layers.dense(inputs=dense3,  units=5,   activation=None,\n",
    "                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope(\"cal_loss\"):\n",
    "        #loss = tf.losses.sparse_softmax_cross_entropy(labels=y_,logits=logits)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=logits))\n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_op=tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "    with tf.name_scope(\"cal_accuracy\"):\n",
    "        true = tf.argmax(y_, axis=1)\n",
    "        correct_prediction = tf.equal(tf.cast(tf.argmax(logits,1),tf.int64), true)    \n",
    "        acc= tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    summary_loss = tf.summary.scalar('cost', loss)\n",
    "    summary_acc = tf.summary.scalar('accuracy', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    0, Loss: 1.6095, Acc: 20.3125%\n",
      "Step:  500, Loss: 1.1360, Acc: 53.1250%\n",
      "Step: 1000, Loss: 0.6236, Acc: 71.8750%\n",
      "Step: 1500, Loss: 0.3960, Acc: 78.1250%\n",
      "Step: 2000, Loss: 0.5905, Acc: 76.5625%\n",
      "Step: 2500, Loss: 0.5793, Acc: 78.1250%\n",
      "Step: 3000, Loss: 0.3727, Acc: 85.9375%\n",
      "Step: 3500, Loss: 0.3333, Acc: 85.9375%\n",
      "Step: 4000, Loss: 0.4079, Acc: 89.0625%\n",
      "Step: 4500, Loss: 0.0735, Acc: 95.3125%\n",
      "Step: 5000, Loss: 0.1718, Acc: 93.7500%\n",
      "Step: 5500, Loss: 0.2913, Acc: 92.1875%\n",
      "Step: 6000, Loss: 0.0870, Acc: 96.8750%\n",
      "Step: 6500, Loss: 0.0367, Acc: 98.4375%\n",
      "Step: 7000, Loss: 0.0660, Acc: 96.8750%\n",
      "Step: 7500, Loss: 0.0646, Acc: 96.8750%\n",
      "Step: 8000, Loss: 0.5751, Acc: 89.0625%\n",
      "Step: 8500, Loss: 0.0086, Acc: 100.0000%\n",
      "Step: 9000, Loss: 0.2588, Acc: 95.3125%\n",
      "Training finished.\n",
      "Testing Loss: 5.1341, Testing Acc: 47.9564%\n"
     ]
    }
   ],
   "source": [
    "epoch=200\n",
    "batch_size=64\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('./tmp/visualization/',sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    step = 0\n",
    "    for epc in range(epoch):\n",
    "        for x, y in gen_batch(x_train, y_train, batch_size):\n",
    "            l, ac, _,s_m = sess.run([loss, acc, train_op, merged], feed_dict={X:x,y_:y})\n",
    "            if step % 500 == 0:\n",
    "                print(\"Step: {:>4}, Loss: {:.4f}, Acc: {:.4%}\".format(step, l, ac))\n",
    "            step += 1\n",
    "            writer.add_summary(s_m, global_step=epc)\n",
    "    print(\"Training finished.\")\n",
    "    l, ac, _ = sess.run([loss, acc, train_op],\n",
    "                                       {X: x_val, y_: y_val})\n",
    "    print(\"Testing Loss: {:.4f}, Testing Acc: {:.4%}\".format(l, ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用防止过拟合方法 对结果影响不大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 添加正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope('hidden_layers'):\n",
    "        re1 = tf.reshape(X, [-1, w * h* c])\n",
    "        dense1 = tf.layers.dense(inputs=re1,      units=1024,   activation=tf.nn.relu,\n",
    "                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                      kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))\n",
    "        dense2 = tf.layers.dense(inputs=dense1,      units=1024,   activation=tf.nn.relu,\n",
    "                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                                kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))\n",
    "        dense3= tf.layers.dense(inputs=dense2,  units=128,  activation=tf.nn.relu,\n",
    "                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        logits= tf.layers.dense(inputs=dense3,  units=5,   activation=None,\n",
    "                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                         kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    0, Loss: 1.6094, Acc: 23.4375%\n",
      "Step:  500, Loss: 1.0235, Acc: 67.1875%\n",
      "Step: 1000, Loss: 0.7885, Acc: 65.6250%\n",
      "Step: 1500, Loss: 0.5379, Acc: 79.6875%\n",
      "Step: 2000, Loss: 0.3865, Acc: 85.9375%\n",
      "Step: 2500, Loss: 0.3553, Acc: 85.9375%\n",
      "Step: 3000, Loss: 0.2125, Acc: 90.6250%\n",
      "Step: 3500, Loss: 0.0487, Acc: 98.4375%\n",
      "Step: 4000, Loss: 0.1525, Acc: 96.8750%\n",
      "Step: 4500, Loss: 0.0308, Acc: 100.0000%\n",
      "Step: 5000, Loss: 0.1623, Acc: 92.1875%\n",
      "Step: 5500, Loss: 0.0071, Acc: 100.0000%\n",
      "Step: 6000, Loss: 0.0961, Acc: 96.8750%\n",
      "Step: 6500, Loss: 0.1535, Acc: 95.3125%\n",
      "Step: 7000, Loss: 0.0094, Acc: 100.0000%\n",
      "Step: 7500, Loss: 0.0632, Acc: 98.4375%\n",
      "Step: 8000, Loss: 0.0135, Acc: 100.0000%\n",
      "Step: 8500, Loss: 0.5829, Acc: 85.9375%\n",
      "Step: 9000, Loss: 0.0478, Acc: 98.4375%\n",
      "Training finished.\n",
      "Testing Loss: 4.1709, Testing Acc: 46.8665%\n"
     ]
    }
   ],
   "source": [
    "epoch=200\n",
    "batch_size=64\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('./tmp/visualization_l2/',sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    step = 0\n",
    "    for epc in range(epoch):\n",
    "        for x, y in gen_batch(x_train, y_train, batch_size):\n",
    "            l, ac, _,s_m = sess.run([loss, acc, train_op, merged], feed_dict={X:x,y_:y})\n",
    "            if step % 500 == 0:\n",
    "                print(\"Step: {:>4}, Loss: {:.4f}, Acc: {:.4%}\".format(step, l, ac))\n",
    "            step += 1\n",
    "            writer.add_summary(s_m, global_step=epc)\n",
    "    print(\"Training finished.\")\n",
    "    l, ac, _ = sess.run([loss, acc, train_op],\n",
    "                                       {X: x_val, y_: y_val})\n",
    "    print(\"Testing Loss: {:.4f}, Testing Acc: {:.4%}\".format(l, ac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 添加dropout层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    with tf.name_scope('hidden_layers'):\n",
    "        re1 = tf.reshape(X, [-1, w * h* c])\n",
    "        dense1 = tf.layers.dense(inputs=re1,      units=1024,   activation=tf.nn.relu,\n",
    "                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        dense2 = tf.layers.dense(inputs=dense1,      units=1024,   activation=tf.nn.relu,\n",
    "                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        dense3= tf.layers.dense(inputs=dense2,  units=128,  activation=tf.nn.relu,\n",
    "                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        drop_out=tf.nn.dropout(dense3,keep_prob)\n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        logits= tf.layers.dense(inputs=drop_out,  units=5,   activation=None,\n",
    "                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    0, Loss: 1.6076, Acc: 12.5000%\n",
      "Step:  500, Loss: 1.5767, Acc: 35.9375%\n",
      "Step: 1000, Loss: 1.6033, Acc: 23.4375%\n",
      "Step: 1500, Loss: 1.6399, Acc: 14.0625%\n",
      "Step: 2000, Loss: 1.6184, Acc: 26.5625%\n",
      "Step: 2500, Loss: 1.5844, Acc: 28.1250%\n",
      "Step: 3000, Loss: 1.5798, Acc: 26.5625%\n",
      "Step: 3500, Loss: 1.5860, Acc: 26.5625%\n",
      "Step: 4000, Loss: 1.6117, Acc: 18.7500%\n",
      "Step: 4500, Loss: 1.5901, Acc: 32.8125%\n",
      "Step: 5000, Loss: 1.6385, Acc: 17.1875%\n",
      "Step: 5500, Loss: 1.5761, Acc: 31.2500%\n",
      "Step: 6000, Loss: 1.6078, Acc: 25.0000%\n",
      "Step: 6500, Loss: 1.6060, Acc: 23.4375%\n",
      "Step: 7000, Loss: 1.5934, Acc: 26.5625%\n",
      "Step: 7500, Loss: 1.6114, Acc: 18.7500%\n",
      "Step: 8000, Loss: 1.6166, Acc: 18.7500%\n",
      "Step: 8500, Loss: 1.5769, Acc: 29.6875%\n",
      "Step: 9000, Loss: 1.5873, Acc: 28.1250%\n",
      "Training finished.\n",
      "Testing Loss: 1.6103, Testing Acc: 22.0708%\n"
     ]
    }
   ],
   "source": [
    "epoch=200\n",
    "batch_size=64\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('./tmp/visualization_l2/',sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    step = 0\n",
    "    for epc in range(epoch):\n",
    "        for x, y in gen_batch(x_train, y_train, batch_size):\n",
    "            l, ac, _,s_m = sess.run([loss, acc, train_op, merged], feed_dict={X:x,y_:y,keep_prob:1})\n",
    "            if step % 500 == 0:\n",
    "                print(\"Step: {:>4}, Loss: {:.4f}, Acc: {:.4%}\".format(step, l, ac))\n",
    "            step += 1\n",
    "            writer.add_summary(s_m, global_step=epc)\n",
    "    print(\"Training finished.\")\n",
    "    l, ac, _ = sess.run([loss, acc, train_op],\n",
    "                                       {X: x_val, y_: y_val,keep_prob:1})\n",
    "    print(\"Testing Loss: {:.4f}, Testing Acc: {:.4%}\".format(l, ac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "查看tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate,\n",
    "                      staircase=False, name=None) \n",
    "使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    learning_rate=0.001\n",
    "    global_steps = tf.Variable(0, trainable=False)\n",
    "    learn_rate = tf.train.exponential_decay(learning_rate,global_steps,1000,0.9,staircase=True)\n",
    "    with tf.name_scope(\"cal_loss\"):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=logits))\n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_op=tf.train.AdamOptimizer(learning_rate).minimize(loss,global_step=global_steps)\n",
    "    with tf.name_scope(\"cal_accuracy\"):\n",
    "        true = tf.argmax(y_, axis=1)\n",
    "        correct_prediction = tf.equal(tf.cast(tf.argmax(logits,1),tf.int64), true)    \n",
    "        acc= tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    summary_loss = tf.summary.scalar('cost', loss)\n",
    "    summary_acc = tf.summary.scalar('accuracy', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    0, Learn_rate: 0.0010, Loss: 1.6097,  Acc: 1.5625%\n",
      "Step:  500, Learn_rate: 0.0010, Loss: 0.6427,  Acc: 71.8750%\n",
      "Step: 1000, Learn_rate: 0.0009, Loss: 0.3524,  Acc: 85.9375%\n",
      "Step: 1500, Learn_rate: 0.0009, Loss: 0.2983,  Acc: 85.9375%\n",
      "Step: 2000, Learn_rate: 0.0008, Loss: 0.1944,  Acc: 92.1875%\n",
      "Step: 2500, Learn_rate: 0.0008, Loss: 0.4052,  Acc: 85.9375%\n",
      "Step: 3000, Learn_rate: 0.0007, Loss: 0.1683,  Acc: 92.1875%\n",
      "Step: 3500, Learn_rate: 0.0007, Loss: 0.0226,  Acc: 100.0000%\n",
      "Step: 4000, Learn_rate: 0.0007, Loss: 0.2541,  Acc: 96.8750%\n",
      "Step: 4500, Learn_rate: 0.0007, Loss: 0.0855,  Acc: 96.8750%\n",
      "Step: 5000, Learn_rate: 0.0006, Loss: 0.0464,  Acc: 98.4375%\n",
      "Step: 5500, Learn_rate: 0.0006, Loss: 0.0072,  Acc: 100.0000%\n",
      "Step: 6000, Learn_rate: 0.0005, Loss: 0.0359,  Acc: 98.4375%\n",
      "Step: 6500, Learn_rate: 0.0005, Loss: 0.0031,  Acc: 100.0000%\n",
      "Step: 7000, Learn_rate: 0.0005, Loss: 0.0019,  Acc: 100.0000%\n",
      "Step: 7500, Learn_rate: 0.0005, Loss: 0.0099,  Acc: 100.0000%\n",
      "Training finished.\n",
      "Testing Loss: 3.8367, Testing Acc: 56.2706%\n"
     ]
    }
   ],
   "source": [
    "epoch=200\n",
    "batch_size=64\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('./tmp/visualization_l2/',sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    step = 0\n",
    "    for epc in range(epoch):\n",
    "        for x, y in gen_batch(x_train, y_train, batch_size):\n",
    "            l, ac, Learn_rate,_,s_m = sess.run([loss, acc,learn_rate, train_op, merged], feed_dict={X:x,y_:y})\n",
    "            if step % 500 == 0:\n",
    "                print(\"Step: {:>4}, Learn_rate: {:.4f}, Loss: {:.4f},  Acc: {:.4%}\".format(step, Learn_rate , l, ac))\n",
    "            step += 1\n",
    "            writer.add_summary(s_m, global_step=epc)\n",
    "    print(\"Training finished.\")\n",
    "    l, ac, _ = sess.run([loss, acc, train_op],\n",
    "                                       {X: x_val, y_: y_val})\n",
    "    print(\"Testing Loss: {:.4f}, Testing Acc: {:.4%}\".format(l, ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论\n",
    "## 虽然 利用正则化 dropout 减小学习率 能够避免过拟合提高训练准确率，但是由于模型自身的问题，测试集的准确率还是比较低，下周我们将用CNN 和RNN模型结构，来提高识别率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
