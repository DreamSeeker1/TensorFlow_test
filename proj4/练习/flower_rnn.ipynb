{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于RNN的花卉识别练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “skimage” python 图像处理模块，如果用的conda的虚拟环境，需要用conda install scikit-image 进行安装\n",
    "# glob 文件路径查找模块\n",
    "from skimage import io,transform\n",
    "import glob \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集路径---根据自己的路径进行改写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/liuyanfang/lyf/Codes/GAN/flower_photos/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们的数据集图片大小不一致，所以需要resize成统一大小 这里resize成100x100x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=40\n",
    "h=40\n",
    "c=3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据集图片并添加标签,最后的形式是data 对应图片, label 是标签，roses 0,daisy 1,sunflowers 2,tulips 3,dandelion 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuyue/anaconda2/envs/tensorflow-gpu/lib/python3.5/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "def read_img(path):\n",
    "    cate=[path+x for x in os.listdir(path) if os.path.isdir(path+x)]#逐个打开文件夹，并提取图片\n",
    "    imgs=[]\n",
    "    labels=[]\n",
    "    for idx,folder in enumerate(cate):#用于在for循环中得到计数\n",
    "        for im in glob.glob(folder+'/*.jpg'):#返回所有匹配的文件路径列表\n",
    "            #print('reading the images:%s'%(im))\n",
    "            img=io.imread(im)\n",
    "            img=transform.resize(img,(w,h))\n",
    "            imgs.append(img)\n",
    "            labels.append(idx)\n",
    "                  \n",
    "    return np.asarray(imgs,np.float32),np.asarray(labels,np.int32)#变成矩阵格式\n",
    "data,label=read_img(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.reshape(data,(3029,w*h*c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据集打乱顺序，下阶段课程会讲其他方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3 4 ... 2 4 3]\n"
     ]
    }
   ],
   "source": [
    "num_example=data.shape[0] # data.shape是(3029, 100, 100, 3)\n",
    "arr=np.arange(num_example)# 创建等差数组 0，1，...,3028\n",
    "np.random.shuffle(arr)# 打乱顺序\n",
    "data=data[arr]\n",
    "label=label[arr]\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将标签onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels):\n",
    "    l = len(labels)\n",
    "    res = np.zeros((l, 5), dtype=np.float32)\n",
    "    for i in range(l):\n",
    "        res[i][labels[i]] = 1.\n",
    "    return res\n",
    "label_oh = to_one_hot(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将所有数据集分为训练集80%、验证集10%和测试集10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio=0.8\n",
    "s=np.int(num_example*ratio)\n",
    "x_train=data[:s]\n",
    "y_train=label_oh[:s]\n",
    "x_val=data[s:]\n",
    "y_val=label_oh[s:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(dataset, labelset, batchsize):\n",
    "    for i in range(dataset.shape[0]//batchsize):\n",
    "        pos = i * batchsize\n",
    "        x = dataset[pos:pos + batchsize]\n",
    "        y = labelset[pos:pos + batchsize]\n",
    "        yield x,y\n",
    "    remain = np.shape(dataset)[0] % batchsize\n",
    "    if remain != 0:\n",
    "        x = dataset[-remain:]\n",
    "        y = labelset[-remain:]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_=tf.placeholder(tf.float32,shape=[None,w*h*c],name='X')\n",
    "y_=tf.placeholder(tf.float32,shape=[None,5],name='y_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建RNN网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(lstm_size):\n",
    "  return tf.contrib.rnn.BasicLSTMCell(lstm_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_lstm = tf.contrib.rnn.MultiRNNCell(\n",
    "    [lstm_cell(w*h*c)] )# 参数是： cells, state_is_tuple=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = stacked_lstm.zero_state(batch_size=tf.shape(x_)[0], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4800)\n"
     ]
    }
   ],
   "source": [
    "output = x_\n",
    "for i in range(6):\n",
    "    output, state = stacked_lstm(output, state)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rnn = tf.layers.dense(output, 5,\n",
    "                    kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                    bias_initializer=tf.constant_initializer(0.1),\n",
    "                    name='rnn_output_layer'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaa\n",
      "Step:    0, Loss: 1.6479, Acc: 12.5000%\n",
      "Step:   10, Loss: 1.3234, Acc: 39.0625%\n",
      "Step:   20, Loss: 1.2795, Acc: 34.3750%\n",
      "Step:   30, Loss: 1.1465, Acc: 48.4375%\n",
      "Step:   40, Loss: 1.0601, Acc: 50.0000%\n",
      "Step:   50, Loss: 0.9567, Acc: 60.9375%\n",
      "Step:   60, Loss: 1.0568, Acc: 53.1250%\n",
      "Step:   70, Loss: 0.8874, Acc: 70.3125%\n",
      "Step:   80, Loss: 0.9303, Acc: 59.3750%\n",
      "Step:   90, Loss: 0.8806, Acc: 62.5000%\n",
      "Step:  100, Loss: 1.0058, Acc: 51.5625%\n",
      "Step:  110, Loss: 0.9805, Acc: 53.1250%\n",
      "Step:  120, Loss: 0.8658, Acc: 60.9375%\n",
      "Step:  130, Loss: 0.9183, Acc: 53.1250%\n",
      "Step:  140, Loss: 0.9808, Acc: 59.3750%\n",
      "Step:  150, Loss: 1.0006, Acc: 56.2500%\n",
      "Step:  160, Loss: 0.7216, Acc: 76.5625%\n",
      "Step:  170, Loss: 0.8010, Acc: 67.1875%\n",
      "Step:  180, Loss: 1.0488, Acc: 57.8125%\n",
      "Step:  190, Loss: 0.9947, Acc: 53.1250%\n",
      "Step:  200, Loss: 0.8703, Acc: 67.1875%\n",
      "Step:  210, Loss: 0.7879, Acc: 68.7500%\n",
      "Step:  220, Loss: 0.6907, Acc: 75.0000%\n",
      "Step:  230, Loss: 0.7386, Acc: 75.0000%\n",
      "Step:  240, Loss: 0.7304, Acc: 67.1875%\n",
      "Step:  250, Loss: 0.8036, Acc: 65.6250%\n",
      "Step:  260, Loss: 0.6927, Acc: 75.0000%\n",
      "Step:  270, Loss: 0.5418, Acc: 75.0000%\n",
      "Step:  280, Loss: 0.4752, Acc: 82.8125%\n",
      "Step:  290, Loss: 0.7450, Acc: 68.7500%\n",
      "Step:  300, Loss: 0.8774, Acc: 57.8125%\n",
      "Step:  310, Loss: 0.6027, Acc: 78.1250%\n",
      "Step:  320, Loss: 0.5220, Acc: 71.8750%\n",
      "Step:  330, Loss: 0.7727, Acc: 67.1875%\n",
      "Step:  340, Loss: 0.6518, Acc: 73.4375%\n",
      "Step:  350, Loss: 0.5182, Acc: 79.6875%\n",
      "Step:  360, Loss: 0.3534, Acc: 87.5000%\n",
      "Step:  370, Loss: 0.4243, Acc: 84.3750%\n",
      "Step:  380, Loss: 0.6735, Acc: 71.8750%\n",
      "Step:  390, Loss: 0.5415, Acc: 78.1250%\n",
      "Step:  400, Loss: 0.3911, Acc: 84.3750%\n",
      "Step:  410, Loss: 0.4002, Acc: 85.9375%\n",
      "Step:  420, Loss: 0.9739, Acc: 71.8750%\n",
      "Step:  430, Loss: 0.6880, Acc: 73.4375%\n",
      "Step:  440, Loss: 0.6416, Acc: 70.3125%\n",
      "Step:  450, Loss: 0.5069, Acc: 82.8125%\n",
      "Step:  460, Loss: 0.4381, Acc: 78.1250%\n",
      "Step:  470, Loss: 0.3860, Acc: 84.3750%\n",
      "Step:  480, Loss: 0.4481, Acc: 81.2500%\n",
      "Step:  490, Loss: 0.5825, Acc: 75.0000%\n",
      "Step:  500, Loss: 0.2669, Acc: 85.9375%\n",
      "Step:  510, Loss: 0.2059, Acc: 90.6250%\n",
      "Step:  520, Loss: 0.9509, Acc: 73.4375%\n",
      "Step:  530, Loss: 0.7332, Acc: 75.0000%\n",
      "Step:  540, Loss: 0.5259, Acc: 73.4375%\n",
      "Step:  550, Loss: 0.7369, Acc: 73.4375%\n",
      "Step:  560, Loss: 0.4733, Acc: 81.2500%\n",
      "Step:  570, Loss: 0.4014, Acc: 82.8125%\n",
      "Step:  580, Loss: 0.5562, Acc: 71.8750%\n",
      "Step:  590, Loss: 0.3420, Acc: 81.2500%\n",
      "Step:  600, Loss: 0.5496, Acc: 81.2500%\n",
      "Step:  610, Loss: 0.7965, Acc: 71.8750%\n",
      "Step:  620, Loss: 0.2515, Acc: 93.7500%\n",
      "Step:  630, Loss: 0.3972, Acc: 85.9375%\n",
      "Step:  640, Loss: 0.2738, Acc: 92.1875%\n",
      "Step:  650, Loss: 0.1493, Acc: 95.3125%\n",
      "Step:  660, Loss: 0.3200, Acc: 87.5000%\n",
      "Step:  670, Loss: 0.3145, Acc: 85.9375%\n",
      "Step:  680, Loss: 0.2671, Acc: 85.9375%\n",
      "Step:  690, Loss: 0.2867, Acc: 85.9375%\n",
      "Step:  700, Loss: 0.1086, Acc: 96.8750%\n",
      "Step:  710, Loss: 0.5394, Acc: 78.1250%\n",
      "Step:  720, Loss: 0.2962, Acc: 90.6250%\n",
      "Step:  730, Loss: 0.2968, Acc: 85.9375%\n",
      "Step:  740, Loss: 0.1381, Acc: 98.4375%\n",
      "Step:  750, Loss: 0.3085, Acc: 87.5000%\n",
      "Step:  760, Loss: 0.9570, Acc: 68.7500%\n",
      "Step:  770, Loss: 0.3649, Acc: 87.5000%\n",
      "Step:  780, Loss: 0.4175, Acc: 82.8125%\n",
      "Step:  790, Loss: 0.2280, Acc: 89.0625%\n",
      "Step:  800, Loss: 0.3712, Acc: 79.6875%\n",
      "Step:  810, Loss: 0.4567, Acc: 78.1250%\n",
      "Step:  820, Loss: 0.3605, Acc: 84.3750%\n",
      "Step:  830, Loss: 0.3253, Acc: 87.5000%\n",
      "Step:  840, Loss: 0.1907, Acc: 95.3125%\n",
      "Step:  850, Loss: 0.2530, Acc: 90.6250%\n",
      "Step:  860, Loss: 0.4701, Acc: 82.8125%\n",
      "Step:  870, Loss: 0.4415, Acc: 89.0625%\n",
      "Step:  880, Loss: 0.4078, Acc: 78.1250%\n",
      "Step:  890, Loss: 0.2206, Acc: 90.6250%\n",
      "Step:  900, Loss: 0.5505, Acc: 71.8750%\n",
      "Step:  910, Loss: 0.3860, Acc: 84.3750%\n",
      "Step:  920, Loss: 0.4935, Acc: 79.6875%\n",
      "Step:  930, Loss: 0.3735, Acc: 85.9375%\n",
      "Step:  940, Loss: 0.4334, Acc: 81.2500%\n",
      "Step:  950, Loss: 0.2415, Acc: 90.6250%\n",
      "Step:  960, Loss: 0.3698, Acc: 82.8125%\n",
      "Step:  970, Loss: 0.3474, Acc: 89.0625%\n",
      "Step:  980, Loss: 0.1784, Acc: 95.3125%\n",
      "Step:  990, Loss: 0.3815, Acc: 85.9375%\n",
      "Step: 1000, Loss: 0.2416, Acc: 89.0625%\n",
      "Step: 1010, Loss: 0.2482, Acc: 92.1875%\n",
      "Step: 1020, Loss: 0.2051, Acc: 90.6250%\n",
      "Step: 1030, Loss: 0.2134, Acc: 89.0625%\n",
      "Step: 1040, Loss: 0.2177, Acc: 90.6250%\n",
      "Step: 1050, Loss: 0.0626, Acc: 100.0000%\n",
      "Step: 1060, Loss: 0.1073, Acc: 95.3125%\n",
      "Step: 1070, Loss: 0.1893, Acc: 90.6250%\n",
      "Step: 1080, Loss: 0.0886, Acc: 98.4375%\n",
      "Step: 1090, Loss: 0.2050, Acc: 92.1875%\n",
      "Step: 1100, Loss: 0.3537, Acc: 82.8125%\n",
      "Step: 1110, Loss: 0.2270, Acc: 87.5000%\n",
      "Step: 1120, Loss: 0.4064, Acc: 85.9375%\n",
      "Step: 1130, Loss: 0.1524, Acc: 96.8750%\n",
      "Step: 1140, Loss: 0.2000, Acc: 92.1875%\n",
      "Step: 1150, Loss: 0.2476, Acc: 89.0625%\n",
      "Step: 1160, Loss: 0.3744, Acc: 84.3750%\n",
      "Step: 1170, Loss: 0.1501, Acc: 93.7500%\n",
      "Step: 1180, Loss: 0.1610, Acc: 93.7500%\n",
      "Step: 1190, Loss: 0.2004, Acc: 90.6250%\n",
      "Step: 1200, Loss: 0.2576, Acc: 90.6250%\n",
      "Step: 1210, Loss: 0.1073, Acc: 95.3125%\n",
      "Step: 1220, Loss: 0.2308, Acc: 93.7500%\n",
      "Step: 1230, Loss: 0.2263, Acc: 90.6250%\n",
      "Step: 1240, Loss: 0.1719, Acc: 93.7500%\n",
      "Step: 1250, Loss: 0.1301, Acc: 95.3125%\n",
      "Step: 1260, Loss: 0.1380, Acc: 95.3125%\n",
      "Step: 1270, Loss: 0.1097, Acc: 96.8750%\n",
      "Step: 1280, Loss: 0.2579, Acc: 95.3125%\n",
      "Step: 1290, Loss: 0.2903, Acc: 90.6250%\n",
      "Step: 1300, Loss: 0.1289, Acc: 95.3125%\n",
      "Step: 1310, Loss: 0.1491, Acc: 95.3125%\n",
      "Step: 1320, Loss: 0.1929, Acc: 89.0625%\n",
      "Step: 1330, Loss: 0.5992, Acc: 76.5625%\n",
      "Step: 1340, Loss: 0.1510, Acc: 95.3125%\n",
      "Step: 1350, Loss: 0.2267, Acc: 89.0625%\n",
      "Step: 1360, Loss: 0.2380, Acc: 87.5000%\n",
      "Step: 1370, Loss: 0.0988, Acc: 100.0000%\n",
      "Step: 1380, Loss: 0.1384, Acc: 96.8750%\n",
      "Step: 1390, Loss: 0.1369, Acc: 93.7500%\n",
      "Step: 1400, Loss: 0.0628, Acc: 96.8750%\n",
      "Step: 1410, Loss: 0.2182, Acc: 92.1875%\n",
      "Step: 1420, Loss: 0.3244, Acc: 89.0625%\n",
      "Step: 1430, Loss: 0.0321, Acc: 98.4375%\n",
      "Step: 1440, Loss: 0.2040, Acc: 93.7500%\n",
      "Step: 1450, Loss: 0.0764, Acc: 98.4375%\n",
      "Step: 1460, Loss: 0.0606, Acc: 96.8750%\n",
      "Step: 1470, Loss: 0.1225, Acc: 96.8750%\n",
      "Step: 1480, Loss: 0.0790, Acc: 96.8750%\n",
      "Step: 1490, Loss: 0.1204, Acc: 95.3125%\n",
      "Step: 1500, Loss: 0.0648, Acc: 96.8750%\n",
      "Step: 1510, Loss: 0.0651, Acc: 96.8750%\n",
      "Step: 1520, Loss: 0.1052, Acc: 95.3125%\n",
      "Step: 1530, Loss: 0.0221, Acc: 100.0000%\n",
      "Step: 1540, Loss: 0.1294, Acc: 95.3125%\n",
      "Step: 1550, Loss: 0.0143, Acc: 100.0000%\n",
      "Step: 1560, Loss: 0.1010, Acc: 96.8750%\n",
      "Step: 1570, Loss: 0.0360, Acc: 100.0000%\n",
      "Step: 1580, Loss: 0.1343, Acc: 98.4375%\n",
      "Step: 1590, Loss: 0.0857, Acc: 95.3125%\n",
      "Step: 1600, Loss: 0.3493, Acc: 89.0625%\n",
      "Step: 1610, Loss: 0.1014, Acc: 98.4375%\n",
      "Step: 1620, Loss: 0.0606, Acc: 100.0000%\n",
      "Step: 1630, Loss: 0.0887, Acc: 96.8750%\n",
      "Step: 1640, Loss: 0.0324, Acc: 98.4375%\n",
      "Step: 1650, Loss: 0.0403, Acc: 98.4375%\n",
      "Step: 1660, Loss: 0.0608, Acc: 96.8750%\n",
      "Step: 1670, Loss: 0.1935, Acc: 92.1875%\n",
      "Step: 1680, Loss: 0.0700, Acc: 98.4375%\n",
      "Step: 1690, Loss: 0.0842, Acc: 98.4375%\n",
      "Step: 1700, Loss: 0.0813, Acc: 95.3125%\n",
      "Step: 1710, Loss: 0.0376, Acc: 98.4375%\n",
      "Step: 1720, Loss: 0.0217, Acc: 100.0000%\n",
      "Step: 1730, Loss: 0.1015, Acc: 95.3125%\n",
      "Step: 1740, Loss: 0.0852, Acc: 95.3125%\n",
      "Step: 1750, Loss: 0.0569, Acc: 98.4375%\n",
      "Step: 1760, Loss: 0.1366, Acc: 93.7500%\n",
      "Step: 1770, Loss: 0.1418, Acc: 95.3125%\n",
      "Step: 1780, Loss: 0.1659, Acc: 92.1875%\n",
      "Step: 1790, Loss: 0.0653, Acc: 96.8750%\n",
      "Step: 1800, Loss: 0.1674, Acc: 93.7500%\n",
      "Step: 1810, Loss: 0.2097, Acc: 92.1875%\n",
      "Step: 1820, Loss: 0.1841, Acc: 90.6250%\n",
      "Step: 1830, Loss: 0.0358, Acc: 100.0000%\n",
      "Step: 1840, Loss: 0.0443, Acc: 98.4375%\n",
      "Step: 1850, Loss: 0.0514, Acc: 98.4375%\n",
      "Step: 1860, Loss: 0.0585, Acc: 98.4375%\n",
      "Step: 1870, Loss: 0.0469, Acc: 98.4375%\n",
      "Step: 1880, Loss: 0.2409, Acc: 92.1875%\n",
      "Step: 1890, Loss: 0.0948, Acc: 95.3125%\n",
      "Step: 1900, Loss: 0.0568, Acc: 98.4375%\n",
      "Step: 1910, Loss: 0.0426, Acc: 100.0000%\n",
      "Step: 1920, Loss: 0.1995, Acc: 93.7500%\n",
      "Step: 1930, Loss: 0.1326, Acc: 95.3125%\n",
      "Step: 1940, Loss: 0.0592, Acc: 98.4375%\n",
      "Step: 1950, Loss: 0.0907, Acc: 93.7500%\n",
      "Step: 1960, Loss: 0.0348, Acc: 100.0000%\n",
      "Step: 1970, Loss: 0.0617, Acc: 96.8750%\n",
      "Step: 1980, Loss: 0.0418, Acc: 98.4375%\n",
      "Step: 1990, Loss: 0.0837, Acc: 96.8750%\n",
      "Step: 2000, Loss: 0.0393, Acc: 100.0000%\n",
      "Step: 2010, Loss: 0.0392, Acc: 98.4375%\n",
      "Step: 2020, Loss: 0.0327, Acc: 98.4375%\n",
      "Step: 2030, Loss: 0.0200, Acc: 98.4375%\n",
      "Step: 2040, Loss: 0.2293, Acc: 89.0625%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2050, Loss: 0.4930, Acc: 81.2500%\n",
      "Step: 2060, Loss: 0.1753, Acc: 93.7500%\n",
      "Step: 2070, Loss: 0.0648, Acc: 96.8750%\n",
      "Step: 2080, Loss: 0.0625, Acc: 96.8750%\n",
      "Step: 2090, Loss: 0.0637, Acc: 96.8750%\n",
      "Step: 2100, Loss: 0.0308, Acc: 100.0000%\n",
      "Step: 2110, Loss: 0.1617, Acc: 93.7500%\n",
      "Step: 2120, Loss: 0.1176, Acc: 95.3125%\n",
      "Step: 2130, Loss: 0.2289, Acc: 87.5000%\n",
      "Step: 2140, Loss: 0.0723, Acc: 96.8750%\n",
      "Step: 2150, Loss: 0.0403, Acc: 98.4375%\n",
      "Step: 2160, Loss: 0.0189, Acc: 100.0000%\n",
      "Step: 2170, Loss: 0.0209, Acc: 100.0000%\n",
      "Step: 2180, Loss: 0.0995, Acc: 96.8750%\n",
      "Step: 2190, Loss: 0.1986, Acc: 92.1875%\n",
      "Step: 2200, Loss: 0.0269, Acc: 100.0000%\n",
      "Step: 2210, Loss: 0.0507, Acc: 98.4375%\n",
      "Step: 2220, Loss: 0.1094, Acc: 96.8750%\n",
      "Step: 2230, Loss: 0.3065, Acc: 92.1875%\n",
      "Step: 2240, Loss: 0.2187, Acc: 90.6250%\n",
      "Step: 2250, Loss: 0.2156, Acc: 89.0625%\n",
      "Step: 2260, Loss: 0.0367, Acc: 98.4375%\n",
      "Step: 2270, Loss: 0.1888, Acc: 92.1875%\n",
      "Step: 2280, Loss: 0.1431, Acc: 93.7500%\n",
      "Step: 2290, Loss: 0.0968, Acc: 96.8750%\n",
      "Step: 2300, Loss: 0.0848, Acc: 95.3125%\n",
      "Step: 2310, Loss: 0.3203, Acc: 92.1875%\n",
      "Step: 2320, Loss: 0.4393, Acc: 81.2500%\n",
      "Step: 2330, Loss: 0.1041, Acc: 96.8750%\n",
      "Step: 2340, Loss: 0.3558, Acc: 90.6250%\n",
      "Step: 2350, Loss: 0.0634, Acc: 96.8750%\n",
      "Step: 2360, Loss: 0.1192, Acc: 95.3125%\n",
      "Step: 2370, Loss: 0.0632, Acc: 98.4375%\n",
      "Step: 2380, Loss: 0.0367, Acc: 98.4375%\n",
      "Step: 2390, Loss: 0.2672, Acc: 92.1875%\n",
      "Step: 2400, Loss: 0.1452, Acc: 96.8750%\n",
      "Step: 2410, Loss: 0.1823, Acc: 95.3125%\n",
      "Step: 2420, Loss: 0.0762, Acc: 95.3125%\n",
      "Step: 2430, Loss: 0.2586, Acc: 89.0625%\n",
      "Step: 2440, Loss: 0.1189, Acc: 98.4375%\n",
      "Step: 2450, Loss: 0.0553, Acc: 98.4375%\n",
      "Step: 2460, Loss: 0.0797, Acc: 96.8750%\n",
      "Step: 2470, Loss: 0.1895, Acc: 95.3125%\n",
      "Step: 2480, Loss: 0.0450, Acc: 98.4375%\n",
      "Step: 2490, Loss: 0.1212, Acc: 95.3125%\n",
      "Step: 2500, Loss: 0.0491, Acc: 96.8750%\n",
      "Step: 2510, Loss: 0.0175, Acc: 100.0000%\n",
      "Step: 2520, Loss: 0.0440, Acc: 98.4375%\n",
      "Step: 2530, Loss: 0.0654, Acc: 98.4375%\n",
      "Step: 2540, Loss: 0.0670, Acc: 96.8750%\n",
      "Step: 2550, Loss: 0.0450, Acc: 96.8750%\n",
      "Step: 2560, Loss: 0.0856, Acc: 96.8750%\n",
      "Step: 2570, Loss: 0.0201, Acc: 100.0000%\n",
      "Step: 2580, Loss: 0.0470, Acc: 98.4375%\n",
      "Step: 2590, Loss: 0.0295, Acc: 98.4375%\n",
      "Step: 2600, Loss: 0.0746, Acc: 96.8750%\n",
      "Step: 2610, Loss: 0.1763, Acc: 95.3125%\n",
      "Step: 2620, Loss: 0.0624, Acc: 98.4375%\n",
      "Step: 2630, Loss: 0.0783, Acc: 96.8750%\n",
      "Step: 2640, Loss: 0.0303, Acc: 98.4375%\n",
      "Step: 2650, Loss: 0.0437, Acc: 98.4375%\n",
      "Step: 2660, Loss: 0.0528, Acc: 96.8750%\n",
      "Step: 2670, Loss: 0.0436, Acc: 98.4375%\n",
      "Step: 2680, Loss: 0.0538, Acc: 96.8750%\n",
      "Step: 2690, Loss: 0.0625, Acc: 98.4375%\n",
      "Step: 2700, Loss: 0.0399, Acc: 98.4375%\n",
      "Step: 2710, Loss: 0.0181, Acc: 100.0000%\n",
      "Step: 2720, Loss: 0.0218, Acc: 100.0000%\n",
      "Step: 2730, Loss: 0.1188, Acc: 93.7500%\n",
      "Step: 2740, Loss: 0.0725, Acc: 96.8750%\n",
      "Step: 2750, Loss: 0.0053, Acc: 100.0000%\n",
      "Step: 2760, Loss: 0.1913, Acc: 95.3125%\n",
      "Step: 2770, Loss: 0.0326, Acc: 98.4375%\n",
      "Step: 2780, Loss: 0.0440, Acc: 100.0000%\n",
      "Step: 2790, Loss: 0.0753, Acc: 98.4375%\n",
      "Step: 2800, Loss: 0.0161, Acc: 100.0000%\n",
      "Step: 2810, Loss: 0.0147, Acc: 100.0000%\n",
      "Step: 2820, Loss: 0.0137, Acc: 100.0000%\n",
      "Step: 2830, Loss: 0.0450, Acc: 98.4375%\n",
      "Step: 2840, Loss: 0.0552, Acc: 98.4375%\n",
      "Step: 2850, Loss: 0.0078, Acc: 100.0000%\n",
      "Step: 2860, Loss: 0.0269, Acc: 98.4375%\n",
      "Step: 2870, Loss: 0.0801, Acc: 96.8750%\n",
      "Step: 2880, Loss: 0.0077, Acc: 100.0000%\n",
      "Step: 2890, Loss: 0.1052, Acc: 95.3125%\n",
      "Step: 2900, Loss: 0.1540, Acc: 95.3125%\n",
      "Step: 2910, Loss: 0.0394, Acc: 98.4375%\n",
      "Step: 2920, Loss: 0.0420, Acc: 98.4375%\n",
      "Step: 2930, Loss: 0.1601, Acc: 95.3125%\n",
      "Step: 2940, Loss: 0.0304, Acc: 96.8750%\n",
      "Step: 2950, Loss: 0.0108, Acc: 100.0000%\n",
      "Step: 2960, Loss: 0.0214, Acc: 100.0000%\n",
      "Step: 2970, Loss: 0.0031, Acc: 100.0000%\n",
      "Step: 2980, Loss: 0.0483, Acc: 98.4375%\n",
      "Step: 2990, Loss: 0.0044, Acc: 100.0000%\n",
      "Step: 3000, Loss: 0.1456, Acc: 93.7500%\n",
      "Step: 3010, Loss: 0.0597, Acc: 98.4375%\n",
      "Step: 3020, Loss: 0.0542, Acc: 96.8750%\n",
      "Step: 3030, Loss: 0.0057, Acc: 100.0000%\n",
      "Step: 3040, Loss: 0.0049, Acc: 100.0000%\n",
      "Step: 3050, Loss: 0.0596, Acc: 98.4375%\n",
      "Step: 3060, Loss: 0.1153, Acc: 95.3125%\n",
      "Step: 3070, Loss: 0.2311, Acc: 92.1875%\n",
      "Step: 3080, Loss: 0.1861, Acc: 96.8750%\n",
      "Step: 3090, Loss: 0.0995, Acc: 98.4375%\n",
      "Step: 3100, Loss: 0.0183, Acc: 100.0000%\n",
      "Step: 3110, Loss: 0.0042, Acc: 100.0000%\n",
      "Step: 3120, Loss: 0.0166, Acc: 98.4375%\n",
      "Step: 3130, Loss: 0.0302, Acc: 100.0000%\n",
      "Step: 3140, Loss: 0.0320, Acc: 98.4375%\n",
      "Step: 3150, Loss: 0.1111, Acc: 95.3125%\n",
      "Step: 3160, Loss: 0.0296, Acc: 98.4375%\n",
      "Step: 3170, Loss: 0.0089, Acc: 100.0000%\n",
      "Step: 3180, Loss: 0.0085, Acc: 100.0000%\n",
      "Step: 3190, Loss: 0.0359, Acc: 98.4375%\n",
      "Step: 3200, Loss: 0.0912, Acc: 96.8750%\n",
      "Step: 3210, Loss: 0.0485, Acc: 98.4375%\n",
      "Step: 3220, Loss: 0.0059, Acc: 100.0000%\n",
      "Step: 3230, Loss: 0.0335, Acc: 98.4375%\n",
      "Step: 3240, Loss: 0.0113, Acc: 100.0000%\n",
      "Step: 3250, Loss: 0.0536, Acc: 96.8750%\n",
      "Step: 3260, Loss: 0.0024, Acc: 100.0000%\n",
      "Step: 3270, Loss: 0.0949, Acc: 96.8750%\n",
      "Step: 3280, Loss: 0.2326, Acc: 92.1875%\n",
      "Step: 3290, Loss: 0.0171, Acc: 100.0000%\n",
      "Step: 3300, Loss: 0.0435, Acc: 96.8750%\n",
      "Step: 3310, Loss: 0.0401, Acc: 96.8750%\n",
      "Step: 3320, Loss: 0.0770, Acc: 96.8750%\n",
      "Step: 3330, Loss: 0.0107, Acc: 100.0000%\n",
      "Step: 3340, Loss: 0.0591, Acc: 98.4375%\n",
      "Step: 3350, Loss: 0.0864, Acc: 95.3125%\n",
      "Step: 3360, Loss: 0.0030, Acc: 100.0000%\n",
      "Step: 3370, Loss: 0.0031, Acc: 100.0000%\n",
      "Step: 3380, Loss: 0.2948, Acc: 89.0625%\n",
      "Step: 3390, Loss: 0.0150, Acc: 100.0000%\n",
      "Step: 3400, Loss: 0.1528, Acc: 95.3125%\n",
      "Step: 3410, Loss: 0.0248, Acc: 98.4375%\n",
      "Step: 3420, Loss: 0.0696, Acc: 96.8750%\n",
      "Step: 3430, Loss: 0.0104, Acc: 100.0000%\n",
      "Step: 3440, Loss: 0.1217, Acc: 95.3125%\n",
      "Step: 3450, Loss: 0.0115, Acc: 100.0000%\n",
      "Step: 3460, Loss: 0.0280, Acc: 98.4375%\n",
      "Step: 3470, Loss: 0.0058, Acc: 100.0000%\n",
      "Step: 3480, Loss: 0.0518, Acc: 96.8750%\n",
      "Step: 3490, Loss: 0.0007, Acc: 100.0000%\n",
      "Step: 3500, Loss: 0.0133, Acc: 100.0000%\n",
      "Step: 3510, Loss: 0.0613, Acc: 98.4375%\n",
      "Step: 3520, Loss: 0.0280, Acc: 100.0000%\n",
      "Step: 3530, Loss: 0.0079, Acc: 100.0000%\n",
      "Step: 3540, Loss: 0.0403, Acc: 98.4375%\n",
      "Step: 3550, Loss: 0.2720, Acc: 90.6250%\n",
      "Step: 3560, Loss: 0.0097, Acc: 100.0000%\n",
      "Step: 3570, Loss: 0.0240, Acc: 100.0000%\n",
      "Step: 3580, Loss: 0.0254, Acc: 100.0000%\n",
      "Step: 3590, Loss: 0.0128, Acc: 100.0000%\n",
      "Step: 3600, Loss: 0.0164, Acc: 100.0000%\n",
      "Step: 3610, Loss: 0.1141, Acc: 93.7500%\n",
      "Step: 3620, Loss: 0.0118, Acc: 100.0000%\n",
      "Step: 3630, Loss: 0.0902, Acc: 93.7500%\n",
      "Step: 3640, Loss: 0.2653, Acc: 95.3125%\n",
      "Step: 3650, Loss: 0.1318, Acc: 93.7500%\n",
      "Step: 3660, Loss: 0.0437, Acc: 98.4375%\n",
      "Step: 3670, Loss: 0.0374, Acc: 100.0000%\n",
      "Step: 3680, Loss: 0.0051, Acc: 100.0000%\n",
      "Step: 3690, Loss: 0.0319, Acc: 98.4375%\n",
      "Step: 3700, Loss: 0.0105, Acc: 100.0000%\n",
      "Step: 3710, Loss: 0.0634, Acc: 98.4375%\n",
      "Step: 3720, Loss: 0.0028, Acc: 100.0000%\n",
      "Step: 3730, Loss: 0.0026, Acc: 100.0000%\n",
      "Step: 3740, Loss: 0.0139, Acc: 100.0000%\n",
      "Step: 3750, Loss: 0.0552, Acc: 98.4375%\n",
      "Step: 3760, Loss: 0.0439, Acc: 96.8750%\n",
      "Step: 3770, Loss: 0.0307, Acc: 98.4375%\n",
      "Step: 3780, Loss: 0.0853, Acc: 96.8750%\n",
      "Step: 3790, Loss: 0.0079, Acc: 100.0000%\n",
      "Training finished.\n",
      "Testing Loss: 3.5994, Testing Acc: 57.4257%\n"
     ]
    }
   ],
   "source": [
    "epoch=100\n",
    "batch_size=64\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_rnn))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_rnn, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"aaaaa\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step=0\n",
    "    for epc in range(epoch):      \n",
    "        for x, y in gen_batch(x_train, y_train, batch_size):\n",
    "            l,ac,_ = sess.run([cross_entropy, accuracy, train_step], feed_dict={x_:x,y_:y})\n",
    "            if step % 10 == 0:\n",
    "                print(\"Step: {:>4}, Loss: {:.4f}, Acc: {:.4%}\".format(step, l, ac))\n",
    "            step += 1\n",
    "    print(\"Training finished.\")\n",
    "    l, ac= sess.run([cross_entropy, accuracy],\n",
    "                                       {x_: x_val, y_: y_val})\n",
    "    print(\"Testing Loss: {:.4f}, Testing Acc: {:.4%}\".format(l, ac))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论 \n",
    "## RNN可以用来做图像的识别，但毕竟图像是静态的，RNN是序列性的，所以一般不用RNN做图像识别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
