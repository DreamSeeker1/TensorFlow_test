{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于DNN的花卉识别练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “skimage” python 图像处理模块，如果用的conda的虚拟环境，需要用conda install scikit-image 进行安装\n",
    "# glob 文件路径查找模块\n",
    "# cv2图像处理包，比skimage要快\n",
    "import cv2\n",
    "from skimage import io,transform\n",
    "import glob \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集路径---根据自己的路径进行改写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/liuyanfang/lyf/Codes/GAN/flower_photos/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们的数据集图片大小不一致，所以需要resize成统一大小 这里resize成100x100x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=100\n",
    "h=100\n",
    "c=3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据集图片并添加标签,最后的形式是data 对应图片, label 是标签，roses 0,daisy 1,sunflowers 2,tulips 3,dandelion 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    imgs=[]\n",
    "    labels=[]\n",
    "    cate=[path+x for x in os.listdir(path) if os.path.isdir(path+x)]\n",
    "    for idx,i in enumerate(cate):\n",
    "        for j in os.listdir(i):\n",
    "            im = cv2.imread(i+'/'+j)\n",
    "            img = cv2.resize(im, (w, h))\n",
    "            #print('reading the images:%s'%(i+'/'+j))\n",
    "            imgs.append(img)\n",
    "            labels.append(idx)\n",
    "    return np.asarray(imgs,np.float32),np.asarray(labels,np.int32)\n",
    "data,label=read_img(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuyue/anaconda2/envs/tensorflow-gpu/lib/python3.5/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "def read_img(path):\n",
    "    cate=[path+x for x in os.listdir(path) if os.path.isdir(path+x)]#逐个打开文件夹，并提取图片\n",
    "    imgs=[]\n",
    "    labels=[]\n",
    "    for idx,folder in enumerate(cate):#用于在for循环中得到计数\n",
    "        for im in glob.glob(folder+'/*.jpg'):#返回所有匹配的文件路径列表\n",
    "            #print('reading the images:%s'%(im))\n",
    "            img=io.imread(im)\n",
    "            img=transform.resize(img,(w,h))\n",
    "            imgs.append(img)\n",
    "            labels.append(idx)\n",
    "                  \n",
    "    return np.asarray(imgs,np.float32),np.asarray(labels,np.int32)#变成矩阵格式\n",
    "data,label=read_img(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据集打乱顺序，下阶段课程会讲其他方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 3 ... 3 4 1]\n"
     ]
    }
   ],
   "source": [
    "num_example=data.shape[0] # data.shape是(3029, 100, 100, 3)\n",
    "arr=np.arange(num_example)# 创建等差数组 0，1，...,3028\n",
    "np.random.shuffle(arr)# 打乱顺序\n",
    "data=data[arr]\n",
    "label=label[arr]\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将标签onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels):\n",
    "    l = len(labels)\n",
    "    res = np.zeros((l, 5), dtype=np.float32)\n",
    "    for i in range(l):\n",
    "        res[i][labels[i]] = 1.\n",
    "    return res\n",
    "label_oh = to_one_hot(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将所有数据集分为训练集80%、测试集20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio=0.8\n",
    "s=np.int(num_example*ratio)\n",
    "x_train=data[:s]\n",
    "y_train=label_oh[:s]\n",
    "x_val=data[s:]\n",
    "y_val=label_oh[s:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(dataset, labelset, batchsize):\n",
    "    for i in range(dataset.shape[0]//batchsize):\n",
    "        pos = i * batchsize\n",
    "        x = dataset[pos:pos + batchsize]\n",
    "        y = labelset[pos:pos + batchsize]\n",
    "        yield x,y\n",
    "    remain = np.shape(dataset)[0] % batchsize\n",
    "    if remain != 0:\n",
    "        x = dataset[-remain:]\n",
    "        y = labelset[-remain:]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    X=tf.placeholder(tf.float32,shape=[None,w,h,c],name='X')\n",
    "    y_=tf.placeholder(tf.int32,shape=[None,5],name='y_')\n",
    "    keep_prob=tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 6, 6, 128)\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope('hidden_layers'):\n",
    "        conv1=tf.layers.conv2d(inputs=X,  filters=32,  kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu,\n",
    "          kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        pool1=tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    #第二个卷积层(50->25)\n",
    "        conv2=tf.layers.conv2d( inputs=pool1, filters=64,  kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu,\n",
    "          kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        pool2=tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    #第三个卷积层(25->12)\n",
    "        conv3=tf.layers.conv2d(inputs=pool2, filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu,\n",
    "          kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        pool3=tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    #第四个卷积层(12->6)\n",
    "        conv4=tf.layers.conv2d(  inputs=pool3, filters=128, kernel_size=[3, 3], padding=\"same\",activation=tf.nn.relu,\n",
    "          kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        pool4=tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    \n",
    "\n",
    "    #全连接层\n",
    "    with tf.name_scope('hidden_layers'):\n",
    "        print(pool4.get_shape())\n",
    "        re1 = tf.reshape(pool4,shape=[-1, np.prod(pool4.get_shape()[1:])])\n",
    "        dense1 = tf.layers.dense(inputs=re1,      units=1024,   activation=tf.nn.relu,\n",
    "                          kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                          kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))\n",
    "        h_fc_drop1 = tf.nn.dropout(dense1, keep_prob)\n",
    "        dense2= tf.layers.dense(inputs=h_fc_drop1,  units=512,  activation=tf.nn.relu,\n",
    "                          kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                          kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))\n",
    "        h_fc_drop2 = tf.nn.dropout(dense2, keep_prob)\n",
    "        logits= tf.layers.dense(inputs=h_fc_drop2,  units=5,   activation=None,\n",
    "                            kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))\n",
    "    #---------------------------网络结束-----\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope(\"cal_loss\"):\n",
    "        #loss = tf.losses.sparse_softmax_cross_entropy(labels=y_,logits=logits)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=logits))\n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_op=tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n",
    "    with tf.name_scope(\"cal_accuracy\"):\n",
    "        true = tf.argmax(y_, axis=1)\n",
    "        correct_prediction = tf.equal(tf.cast(tf.argmax(logits,1),tf.int64), true)    \n",
    "        acc= tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    summary_loss = tf.summary.scalar('cost', loss)\n",
    "    summary_acc = tf.summary.scalar('accuracy', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    0, Loss: 1.6091, Acc: 23.4375%\n",
      "Step:  500, Loss: 0.7481, Acc: 71.8750%\n",
      "Step: 1000, Loss: 0.3975, Acc: 81.2500%\n",
      "Step: 1500, Loss: 0.2343, Acc: 93.7500%\n",
      "Step: 2000, Loss: 0.0552, Acc: 96.8750%\n",
      "Step: 2500, Loss: 0.0370, Acc: 98.4375%\n",
      "Step: 3000, Loss: 0.0064, Acc: 100.0000%\n",
      "Step: 3500, Loss: 0.0317, Acc: 98.4375%\n",
      "Step: 4000, Loss: 0.0246, Acc: 100.0000%\n",
      "Step: 4500, Loss: 0.0536, Acc: 98.4375%\n",
      "Step: 5000, Loss: 0.0085, Acc: 100.0000%\n",
      "Step: 5500, Loss: 0.0012, Acc: 100.0000%\n",
      "Step: 6000, Loss: 0.0004, Acc: 100.0000%\n",
      "Step: 6500, Loss: 0.0002, Acc: 100.0000%\n",
      "Step: 7000, Loss: 0.0001, Acc: 100.0000%\n",
      "Step: 7500, Loss: 0.0002, Acc: 100.0000%\n",
      "Step: 8000, Loss: 0.0028, Acc: 100.0000%\n",
      "Step: 8500, Loss: 0.0014, Acc: 100.0000%\n",
      "Step: 9000, Loss: 0.0016, Acc: 100.0000%\n",
      "Training finished.\n",
      "Testing Loss: 2.9787, Testing Acc: 69.7548%\n"
     ]
    }
   ],
   "source": [
    "epoch=200\n",
    "batch_size=64\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('./tmp/visualization/',sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    step = 0\n",
    "    for epc in range(epoch):\n",
    "        for x, y in gen_batch(x_train, y_train, batch_size):\n",
    "            l, ac, _,s_m = sess.run([loss, acc, train_op, merged], feed_dict={X:x,y_:y,keep_prob:0.5})\n",
    "            if step % 500 == 0:\n",
    "                print(\"Step: {:>4}, Loss: {:.4f}, Acc: {:.4%}\".format(step, l, ac))\n",
    "            step += 1\n",
    "            writer.add_summary(s_m, global_step=epc)\n",
    "    print(\"Training finished.\")\n",
    "    l, ac, _ = sess.run([loss, acc, train_op],\n",
    "                                       {X: x_val, y_: y_val,keep_prob:1})\n",
    "    print(\"Testing Loss: {:.4f}, Testing Acc: {:.4%}\".format(l, ac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
